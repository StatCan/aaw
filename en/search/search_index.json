{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to the Advanced Analytics Workspace","text":""},{"location":"#the-advanced-analytics-workspace-documentation","title":"The Advanced Analytics Workspace Documentation","text":"<p>Welcome to the world of data science and machine learning!</p> <p>What is the AAW?</p> <p>Advanced Analytics Workspace is an open source platform designed for data scientists, data stewards, analysts and researchers familiar with open source tools and coding. Developed by data scientists for data scientists, AAW provides a flexible environment that enables advanced practitioners to get their work done with ease.</p> <p>The AAW is a comprehensive solution for data science and data analytics.  With the AAW, you can customize notebook server deployments to suit your specific data science needs. We have a small number of custom Docker images made by our team.</p> <p>What is Kubeflow?</p> <p>The AAW is based on Kubeflow, an open source comprehensive solution for deploying and managing end-to-end ML workflows.</p> <p>Whether you're just getting started or already knee-deep in data analysis, the Advanced Analytics Workspace has everything you need to take your work to the next level. From powerful tools for data pipelines to cloud storage for your datasets, our platform has it all. Need to collaborate with colleagues or publish your results? No problem. We offer seamless collaboration features that make it easy to work together and share your work with others.</p> <p>No matter what stage of your data science journey you're at, the Advanced Analytics Workspace has the resources you need to succeed.</p>"},{"location":"#getting-started-with-the-aaw","title":"Getting Started with the AAW","text":""},{"location":"#the-aaw-portal","title":"The AAW Portal","text":"<p>The AAW portal homepage is available for internal users only. However, external users with a cloud account granted access by the business sponsor can access the platform through the analytics-platform URL.</p> <p>AAW Portal Homepage</p> <ul> <li>Portal Homepage for Statistics Canada Employees</li> <li>Portal Homepage for External Users</li> </ul>"},{"location":"#kubeflow-account","title":"Kubeflow Account\ud83d\udc49 Click here to setup your Kubeflow account! \ud83d\udc48","text":"<p>Attention External Users!</p> <p>Users external to Statistics Canada will require a cloud account granted access by the business sponsor.</p> <p>Attention Statistics Canada Employees!</p> <p>Users internal to Statistics Canada can get started right away without any additional sign up procedures, just head to  https://kubeflow.aaw.cloud.statcan.ca/.</p> <p> </p> <p>Kubeflow is a powerful and flexible open source platform that allows for dynamic leverage of cloud compute, with users having the ability to control compute, memory, and storage resources used. </p> <p>Kubeflow simplifies the following tasks:</p> <ul> <li>Creating customizable environments to work with data with user-controlled resource provisioning (custom CPU, GPU, RAM and storage).</li> <li>Managing notebook servers including Ubuntu Desktop (via noVNC), R Studio, JupyterLab with Python, R, Julia and SAS for Statistics Canada employees.</li> </ul> <p>Kubeflow Dashboard</p> <ul> <li>Kubeflow Dashboard Use this link once you have your cloud account!</li> </ul> <p>Getting started with the Advanced Analytics Workspace (AAW) is easy and quick. First, you'll want to login to Kubeflow to create your first notebook server running JupyterLab, RStudio or Ubuntu Desktop. We encourage you to join our Slack channel to connect with other data scientists and analysts, ask questions, and share your experiences with the AAW platform.</p>"},{"location":"#slack","title":"SlackUse your @statcan.gc.ca email address so that you will be automatically approved.","text":"<ul> <li>Click here sign in to our Slack Support Workspace</li> </ul> <ul> <li>Use the General Channel!</li> </ul> <p>At StatCan, we understand that embarking on a new project can be overwhelming, and you're likely to have many platform-related questions along the way. That's why we've created a dedicated  Slack channel to provide you with the support you need. Our team of experts is standing by to answer your questions, address any concerns, and guide you through every step of the process.</p> <p>To join our  Slack channel, simply click on the link provided and follow the instructions. You'll be prompted to create an account in the upper right-hand corner of the page. If you have an <code>@statcan.gc.ca</code> email address, use it when signing up as this will ensure that you are automatically approved and can start engaging with our community right away.</p> <p>Once you've created your account, you'll have access to a wealth of resources and information, as well as the opportunity to connect with other users who are working on similar projects. Our  Slack channel is the perfect place to ask questions, share insights, and collaborate with your peers in real-time. Whether you're just getting started with a new project or you're looking for expert advice on a complex issue, our team is here to help.</p> <p>So don't hesitate - join our Slack channel today and start getting the answers you need to succeed. We look forward to welcoming you to our community!</p> <p>Click on the link, then choose \"Create an account\" in the upper right-hand corner.</p> <p> </p>"},{"location":"#getting-started","title":"\ud83e\udded Getting Started","text":"<p>To access AAW services, you need to log in to Kubeflow with your StatCan guest cloud account. Once logged in, select Notebook Servers and click the \"New Server\" button to get started.</p> <ol> <li>Login to Kubeflow with your StatCan guest cloud account. You will be prompted to authenticate the account.</li> <li>Select Notebook Servers.</li> <li>Click the \"\u2795 New Server\" button.</li> </ol>"},{"location":"#tools-offered","title":"\ud83e\uddf0 Tools Offered","text":"<p>AAW is a flexible platform for data analysis and machine learning. It offers a range of languages, including Python, R, and Julia. AAW also supports development environments such as VS Code, R Studio, and Jupyter Notebooks. Additionally, Linux virtual desktops are available for users who require additional tools such as OpenM++ and QGIS.</p> <p>Here's a list of tools we offer:</p> <ul> <li>\ud83d\udcdc Languages:<ul> <li>\ud83d\udc0d Python</li> <li>\ud83d\udcc8 R</li> <li>\ud83d\udc69\u200d\ud83d\udd2c Julia</li> </ul> </li> <li>\ud83e\uddee Development environments:<ul> <li>VS Code</li> <li>R Studio</li> <li>Jupyter Notebooks</li> </ul> </li> <li>\ud83d\udc27 Linux virtual desktops for additional tools (\ud83e\uddeb OpenM++, \ud83c\udf0f QGIS etc.)</li> </ul> <p>Sharing code, disks, and workspaces (e.g.: two people sharing the same virtual machine) is described in more detail in the Collaboration section. Sharing data through buckets is described in more detail in the Azure Blob Storage section.</p>"},{"location":"#help","title":"\ud83d\udca1 Help","text":"<ul> <li>Disk (also called Volumes on the Notebook Server creation screen)</li> <li>Containers (Blob Storage)</li> <li>Data Lakes (coming soon)</li> </ul> <ul> <li>\ud83d\udcd7 AAW Portal Documentation<ul> <li>https://statcan.github.io/daaas/</li> </ul> </li> <li>\ud83d\udcd8 Kubeflow Documentation<ul> <li>https://www.kubeflow.org/docs/ </li> </ul> </li> <li>\ud83e\udd1d Slack Support Channel<ul> <li>https://statcan-aaw.slack.com</li> </ul> </li> </ul>"},{"location":"#demos","title":"\ud83d\udc31 Demos","text":"<p>If you require a quick onboarding demo session, need help, or have any questions, please reach out to us through our \ud83e\udd1d Slack Support Channel.</p>"},{"location":"#contributing","title":"Contributing","text":"<p>If you have any bugs to report or features to request please do so via https://github.com/Statcan/daaas.</p>"},{"location":"Help/","title":"Have questions? Or feedback?","text":"<p>Come join us on the Advanced Analytics Workspace Slack channel! You will find a bunch of other users of the platform there who may be able to answer your questions, and some of the engineers will usually be present on the channel. You can ask questions and provide feedback there.</p> <ul> <li>Slack (en)</li> </ul> <p>We will also post notices there if there are updates or downtime.</p>"},{"location":"Help/#video-tutorials","title":"Video tutorials","text":"<p>After you have joined our Slack community, go and check out the following tutorials:</p> <ul> <li>Platform official</li> <li>Community driven content</li> </ul>"},{"location":"Help/#github","title":"GitHub","text":"<p>Want to know even more about our platform? Find everything about it on our GitHub page.</p> <ul> <li>Advanced Analytics Workspace on GitHub</li> </ul>"},{"location":"welcome-message/","title":"Welcome message","text":""},{"location":"welcome-message/#welcome-to-advanced-analytics-workspace-aaw","title":"\ud83e\uddd9\ud83d\udd2e Welcome to Advanced Analytics Workspace (AAW)","text":"<p>Please find below additional information, videos and links to help better understand how to get started with Advanced Analytics Workspace (AAW). </p> <p>Advanced Analytics Workspace (AAW) is our open source platform for data science and machine learning (ML) for advanced practitioners to get their work done in an unrestricted environment made by data scientists for data scientists. With AAW, you can customize your notebook deployments to suit your data science needs. We also have a small number of expertly crafted images made by our expert data science team.</p> <p>AAW is based on the Kubeflow project which is an open source comprehensive solution for deploying and managing end-to-end ML workflows. Kubeflow is designed to make deployments of ML workflows on Kubernetes simple, portable and scalable.</p> <p>\ud83d\udd14 Important! Users external to Statistics Canada will require a cloud account granted access by the business sponsor.</p> <p>\ud83d\udd14 Important! Users internal to Statistics Canada can get started right away without any additional sign up procedures, just head to  https://kubeflow.aaw.cloud.statcan.ca/.</p>"},{"location":"welcome-message/#helpful-links","title":"\ud83d\udd17 Helpful Links","text":""},{"location":"welcome-message/#aaw-services","title":"\ud83d\udece\ufe0f AAW Services","text":"<ul> <li>\ud83c\udf00 AAW Portal Homepage<ul> <li>Internal Only https://www.statcan.gc.ca/data-analytics-service/aaw</li> <li>Internal/External https://analytics-platform.statcan.gc.ca/covid19</li> </ul> </li> </ul> <ul> <li>\ud83e\udd16 Kubeflow Dashboard<ul> <li>https://kubeflow.aaw.cloud.statcan.ca/ </li> </ul> </li> </ul>"},{"location":"welcome-message/#help","title":"\ud83d\udca1 Help","text":"<ul> <li>\ud83d\udcd7 AAW Portal Documentation<ul> <li>https://statcan.github.io/daaas/</li> </ul> </li> <li>\ud83d\udcd8 Kubeflow Documentation<ul> <li>https://www.kubeflow.org/docs/ </li> </ul> </li> <li>\ud83e\udd1d Slack Support Channel<ul> <li>https://statcan-aaw.slack.com</li> </ul> </li> </ul>"},{"location":"welcome-message/#getting-started","title":"\ud83e\udded Getting Started","text":"<p>In order to access the AAW services, you will need to:</p> <ol> <li>Login to Kubeflow with your StatCan guest cloud account. You will be prompted to authenticate the account.</li> <li>Select Notebook Servers.</li> <li>Click the \"\u2795 New Server\" button.</li> </ol>"},{"location":"welcome-message/#tools-offered","title":"\ud83e\uddf0 Tools Offered","text":"<p>AAW is a flexible platform for data analysis and machine learning, featuring:</p> <p>- \ud83d\udcdc Languages     - \ud83d\udc0d Python     - \ud83d\udcc8 R     - \ud83d\udc69\u200d\ud83d\udd2c Julia   - \ud83e\uddee Development environments     - VS Code     - R Studio     - Jupyter Notebooks   - \ud83d\udc27 Linux virtual desktops for additional tools (\ud83e\uddeb OpenM++, \ud83c\udf0f QGIS etc.)</p>"},{"location":"welcome-message/#demos","title":"\ud83d\udc31 Demos","text":"<p>If you would like a quick Onboarding Demo session or require any help or have any questions, please do not hesitate to reach out through our \ud83e\udd1d Slack Support Channel.</p>"},{"location":"welcome-message/#faq","title":"FAQ","text":"<ul> <li>Frequently Asked Questions are located here.</li> </ul> <p>Thank you!</p>"},{"location":"welcome/","title":"Welcome","text":""},{"location":"welcome/#welcome-to-advanced-analytics-workspace-aaw","title":"\ud83e\uddd9\ud83d\udd2e Welcome to Advanced Analytics Workspace (AAW)","text":"<p>\"What we want to do is make a leapfrog product that is way smarter than any mobile device has ever been, and super-easy to use. This is what iPhone is. OK? So, we're going to reinvent the phone.\" -- Steve Jobs</p> <p>Please find below additional information, videos and links to help better understand how to get started with Advanced Analytics Workspace (AAW).</p> <p>Advanced Analytics Workspace (AAW) is an open source platform designed for data science and machine learning (ML) practitioners. Developed by data scientists for data scientists, AAW provides an unrestricted environment that enables advanced practitioners to get their work done with ease.</p> <p>Built on the Kubeflow project, AAW is a comprehensive solution for deploying and managing end-to-end ML workflows. It simplifies the deployment of ML workflows on Kubernetes, making it simple, portable, and scalable. With AAW, you can customize your notebook deployments to suit your specific data science needs. Additionally, we have a small number of expertly crafted images made by our team of data science experts.</p> <p>Advanced Analytics Workspace (AAW) is our open source platform for data science and machine learning (ML) for advanced practitioners to get their work done in an unrestricted environment made by data scientists for data scientists. With AAW, you can customize your notebook deployments to suit your data science needs. We also have a small number of expertly crafted images made by our expert data science team.</p> <p>AAW is based on the Kubeflow project which is an open source comprehensive solution for deploying and managing end-to-end ML workflows. Kubeflow is designed to make deployments of ML workflows on Kubernetes simple, portable and scalable.</p> <p>\ud83d\udd14 Important! Users external to Statistics Canada will require a cloud account granted access by the business sponsor.</p> <p>\ud83d\udd14 Important! Users internal to Statistics Canada can get started right away without any additional sign up procedures, just head to  https://kubeflow.aaw.cloud.statcan.ca/.</p>"},{"location":"welcome/#helpful-links","title":"\ud83d\udd17 Helpful Links","text":""},{"location":"welcome/#aaw-services","title":"\ud83d\udece\ufe0f AAW Services","text":"<p>The AAW portal homepage is available for internal users only. However, external users with a cloud account granted access by the business sponsor can access the platform through the analytics-platform URL.</p> <ul> <li>\ud83c\udf00 AAW Portal Homepage<ul> <li>Internal Only https://www.statcan.gc.ca/data-analytics-service/aaw</li> <li>Internal/External https://analytics-platform.statcan.gc.ca/covid19</li> </ul> </li> </ul> <ul> <li>\ud83e\udd16 Kubeflow Dashboard<ul> <li>https://kubeflow.aaw.cloud.statcan.ca/ </li> </ul> </li> </ul>"},{"location":"welcome/#help","title":"\ud83d\udca1 Help","text":"<p>The AAW Portal Documentation and Kubeflow Documentation provide helpful resources to get started with AAW. If you need further assistance, our Slack Support Channel is available for support.</p> <ul> <li>\ud83d\udcd7 AAW Portal Documentation<ul> <li>https://statcan.github.io/daaas/</li> </ul> </li> <li>\ud83d\udcd8 Kubeflow Documentation<ul> <li>https://www.kubeflow.org/docs/ </li> </ul> </li> <li>\ud83e\udd1d Slack Support Channel<ul> <li>https://statcan-aaw.slack.com</li> </ul> </li> </ul>"},{"location":"welcome/#getting-started","title":"\ud83e\udded Getting Started","text":"<p>To access AAW services, you need to log in to Kubeflow with your StatCan guest cloud account. Once logged in, select Notebook Servers and click the \"New Server\" button to get started.</p> <ol> <li>Login to Kubeflow with your StatCan guest cloud account. You will be prompted to authenticate the account.</li> <li>Select Notebook Servers.</li> <li>Click the \"\u2795 New Server\" button.</li> </ol>"},{"location":"welcome/#tools-offered","title":"\ud83e\uddf0 Tools Offered","text":"<p>AAW is a flexible platform for data analysis and machine learning. It offers a range of languages, including Python, R, and Julia. AAW also supports development environments such as VS Code, R Studio, and Jupyter Notebooks. Additionally, Linux virtual desktops are available for users who require additional tools such as OpenM++ and QGIS.</p> <p>Here's a list of tools we offer:</p> <p>- \ud83d\udcdc Languages:     - \ud83d\udc0d Python     - \ud83d\udcc8 R     - \ud83d\udc69\u200d\ud83d\udd2c Julia   - \ud83e\uddee Development environments:     - VS Code     - R Studio     - Jupyter Notebooks   - \ud83d\udc27 Linux virtual desktops for additional tools (\ud83e\uddeb OpenM++, \ud83c\udf0f QGIS etc.)</p>"},{"location":"welcome/#demos","title":"\ud83d\udc31 Demos","text":"<p>If you require a quick onboarding demo session, need help, or have any questions, please reach out to us through our \ud83e\udd1d Slack Support Channel.</p>"},{"location":"welcome/#frequently-asked-questions","title":"Frequently Asked Questions","text":"<p>For frequently asked questions, please refer to the FAQ section in our Github repository, located here.</p> <p>Thank you for choosing Advanced Analytics Workspace!</p>"},{"location":"1-Experiments/Jupyter/","title":"Overview","text":""},{"location":"1-Experiments/Jupyter/#jupyter-friendly-r-and-python-experience","title":"Jupyter: friendly R and Python experience","text":"<p>Jupyter gives you notebooks to write your code and make visualizations. You can quickly iterate, visualize, and share your analyses. Because it's running on a server (that you set up in the Kubeflow section) you can do really big analyses on centralized hardware, adding as much horsepower as you need! And because it's on the cloud, you can share it with your colleagues too.</p>"},{"location":"1-Experiments/Jupyter/#explore-your-data","title":"Explore your data","text":"<p>Jupyter comes with a number of features (and we can add more)</p> <ul> <li>Integrated visuals within your notebook</li> <li>Data volume for storing your data</li> <li>You can share your workspace with colleagues.</li> </ul> <p></p>"},{"location":"1-Experiments/Jupyter/#explore-your-data-with-an-api","title":"Explore Your Data with an API","text":"<p>Use Datasette , an instant JSON API for your SQLite databases. Run SQL queries in a more interactive way!</p>"},{"location":"1-Experiments/Jupyter/#ide-in-the-browser","title":"IDE in the browser","text":"<p>Create for exploring, and also great for writing code</p> <ul> <li>Linting and a debugger</li> <li>Git integration</li> <li>Built in Terminal</li> <li>Light/Dark theme (change settings at the top)</li> </ul> <p></p> <p>More information on Jupyter here</p>"},{"location":"1-Experiments/Jupyter/#setup","title":"Setup","text":""},{"location":"1-Experiments/Jupyter/#get-started-with-the-examples","title":"Get started with the examples","text":"<p>When you started your server, it got loaded with a bunch of example notebooks. Double click to open the jupyter-notebooks folder. Great notebooks to start with are <code>R/01-R-Notebook-Demo.ipynb</code>, or the notebooks in <code>scikitlearn</code>. <code>pytorch</code> and <code>tensorflow</code> are great if you are familiar with machine learning. The <code>mapreduce-pipeline</code> and <code>ai-pipeline</code> are more advanced.</p> <p>Some notebooks only work in certain server versions</p> <p>For instance, <code>gdal</code> is only in the geomatics image. So if you use another image then a notebook using <code>gdal</code> might not work.</p>"},{"location":"1-Experiments/Jupyter/#adding-software","title":"Adding software","text":"<p>You do not have <code>sudo</code> in Jupyter, but you can use</p> <pre><code>conda install --use-local your_package_name\n</code></pre> <p>or</p> <pre><code>pip install --user your_package_name\n</code></pre> <p>Don't forget to restart your Jupyter kernel afterwards, to make new packages available.</p> Make sure to restart the Jupyter kernel after installing new software <p>If you install software in a terminal, but your Jupyter kernel was already running, then it will not be updated.</p> Is there something that you can't install? <p>If you need something installed, reach us or open a GitHub issue. We can add it to the default software.</p>"},{"location":"1-Experiments/Jupyter/#once-youve-got-the-basics","title":"Once you've got the basics ...","text":""},{"location":"1-Experiments/Jupyter/#getting-data-in-and-out-of-jupyter","title":"Getting Data in and out of Jupyter","text":"<p>You can upload and download data to/from JupyterHub directly in the menu. There is an upload button at the top, and you can right-click most files or folders to download them.</p>"},{"location":"1-Experiments/Jupyter/#shareable-bucket-storage","title":"Shareable \"Bucket\" storage","text":"<p>There is also a mounted <code>buckets</code> folder in your home directory, which holds files in MinIO.</p> <p>Refer to the Storage section for details.</p>"},{"location":"1-Experiments/Jupyter/#data-analysis","title":"Data Analysis","text":"<p>Data analysis is an underappreciated art.</p> <p>Data analysis is the process of examining and interpreting large amounts of data to extract useful insights and draw meaningful conclusions. This can be done using various techniques and tools, such as statistical analysis, machine learning, and visualization. The goal of data analysis is to uncover patterns, trends, and relationships in the data, which can then be used to inform decisions and solve problems. Data analysis is used in a wide range of fields, from business and finance to healthcare and science, to help organizations make more informed decisions based on evidence and data-driven insights.</p>"},{"location":"1-Experiments/Jupyter/#jupyterlab","title":"JupyterLab","text":"<p>Process data using R, Python, or Julia in JupyterLab</p> <p> </p> <p>Processing data using R, Python, or Julia is made easy with the Advanced Analytics Workspace. Whether you're new to data analysis or an experienced data scientist, our platform supports a range of programming languages to fit your needs. You can install and run packages for R or Python to perform data processing tasks such as data cleaning, transformation, and modeling. If you prefer Julia, our platform also offers support for this programming language.</p>"},{"location":"1-Experiments/Kubeflow/","title":"Overview","text":""},{"location":"1-Experiments/Kubeflow/#what-does-kubeflow-do","title":"What does Kubeflow do?","text":"<p>Kubeflow runs your workspaces. You can have notebook servers (called Jupyter Servers), and in them you can create analyses in R and Python with interactive visuals. You can save and upload data, download it, and create shared workspaces for your team.</p> <p></p> <p>Let's get started!</p>"},{"location":"1-Experiments/Kubeflow/#video-tutorial","title":"Video Tutorial","text":"<p>This video is not up to date, some things have changed since.</p> <p></p>"},{"location":"1-Experiments/Kubeflow/#setup","title":"Setup","text":""},{"location":"1-Experiments/Kubeflow/#log-into-kubeflow","title":"Log into Kubeflow","text":"Log into the Azure Portal using your Cloud Credentials <p>You have to login to the Azure Portal using your StatCan cloud credentials. <code>first.lastname@cloud.statcan.ca</code> or StatCan credentials <code>first.lastname@statcan.gc.ca</code>. You can do that using the Azure Portal. </p> <ul> <li>Log into Kubeflow</li> </ul> <ul> <li>Navigate to the Notebook Servers tab</li> </ul> <p></p> <ul> <li>Then click + New Server</li> </ul>"},{"location":"1-Experiments/Kubeflow/#server-name-and-namespace","title":"Server Name and Namespace","text":"<ul> <li>You will get a template to create your notebook server. Note: the name of   your server can consist of only lower-case letters, numbers, and hyphens. No spaces, and no   underscores.</li> </ul> <ul> <li>You will need to specify a namespace. By default you will have a default   namespace for your account, but for projects you may need to select the   namespace created specifically for that project. Otherwise the notebook server   you create may not have access rights to resources required for the project.</li> </ul>"},{"location":"1-Experiments/Kubeflow/#image","title":"Image","text":"<p>You will need to choose an image. There are JupyterLab, RStudio, Ubuntu remote desktop, and SAS images available. The SAS image is only available for StatCan employees (due to license limitations), the others are available for everyone. Select the drop down menu to select additional options within these (for instance, CPU, PyTorch, and TensorFlow images for JupyterLab).</p> <p>Check the name of the images and choose one that matches what you want to do. Don't know which one to choose? Check out your options here.</p> <p></p>"},{"location":"1-Experiments/Kubeflow/#cpu-and-memory","title":"CPU and Memory","text":"<p>At the time of writing (December 23, 2021) there are two types of computers in the cluster</p> <ul> <li>CPU: <code>D16s v3</code> (16 CPU cores, 64 GiB memory; for user use 15 CPU cores    and 48 GiB memory are available; 1 CPU core and 16 GiB memory reserved for    system use).</li> <li>GPU: <code>NC6s_v3</code> (6 CPU cores, 112 GiB memory, 1 GPU; for user use 96 GiB    memory are available; 16 GiB memory reserved for system use). The available    GPU is the NVIDIA Tesla V100 GPU with specifications    here.</li> </ul> <p>When creating a notebook server, the system will limit you to the maximum specifications above. For CPU notebook servers, you can specify the exact amount of CPU and memory that you require. This allows you to meet your compute needs while minimising cost. For a GPU notebook server, you will always get the full server (6 CPU cores, 96 GiB accessible memory, and 1 GPU). See below section on GPUs for information on how to select a GPU server.</p> <p>In the advanced options, you can select a higher limit than the number of CPU cores and RAM requested. The amount requested is the amount guaranteed to be available for your notebook server and you will always pay for at least this much. If the limit is higher than the amount requested, if additional RAM and CPU cores are available on that shared server in the cluster your notebook server can use them as needed. One use case for this is jobs that usually need only one CPU core but can benefit from multithreading to speed up certain operations. By requesting one CPU core but a higher limit, you can pay much less for the notebook server while allowing it to use spare unused CPU cores as needed to speed up computations.</p> <p></p>"},{"location":"1-Experiments/Kubeflow/#gpus","title":"GPUs","text":"<p>If you want a GPU server, select <code>1</code> as the number of GPUs and <code>NVIDIA</code> as the GPU vendor (the create button will be greyed out until the GPU vendor is selected if you have a GPU specified). Multi-GPU servers are currently supported on the AAW system only on a special on-request basis, please contact the AAW maintainers if you would like a multi-GPU server.</p> <p></p> <p>As mentioned before, if you select a GPU server you will automatically get 6 CPU cores and 112 GiB of memory.</p> <p>Use GPU machines responsibly</p> <p>GPU machines are significantly more expensive than CPU machines, so use them responsibly.</p>"},{"location":"1-Experiments/Kubeflow/#workspace-volume","title":"Workspace Volume","text":"<p>You will need a workspace volume, which is where the home folder will be mounted. There are various configuration options available:</p> <ul> <li>You can either reuse an existing workspace volume from before, or create a new one.</li> </ul> <ul> <li>You can specify the size of the workspace volume, from 4 GiB to 32 GiB.</li> </ul> <p></p> <p>Check for old volumes by looking at the Existing option</p> <p>When you create your server you have the option of reusing an old volume or creating a new one. You probably want to reuse your old volume.</p>"},{"location":"1-Experiments/Kubeflow/#data-volumes","title":"Data Volumes","text":"<p>You can also create data volumes that can be used to store additional data. Multiple data volumes can be created. Click the add new volume button to create a new volume and specify its configuration. Click the attach existing volume button to mount an existing data volume to the notebook server. There are the following configuration parameters for data volumes:</p> <ul> <li>Name: Name of the volume.</li> </ul> <ul> <li>Size in GiB: From 4 GiB to 512 GiB.</li> </ul> <ul> <li>Mount path: Path where the data volume can be accessed on the notebook server, by   default <code>/home/jovyan/vol-1</code>, <code>/home/jovyan/vol-2</code>, etc. (incrementing counter per data   volume mounted).</li> </ul> <p>When mounting an existing data volume, the name option becomes a drop-down list of the existing data volumes. Only a volume not currently mounted to an existing notebook server can be used. The mount path option remains user-configurable with the same defaults as creating a new volume.</p> <p>The garbage can icon on the right can be used to delete an existing or accidentally created data volume.</p> <p></p>"},{"location":"1-Experiments/Kubeflow/#configurations","title":"Configurations","text":"<p>There are currently three checkbox options available here:</p> <ul> <li>Mount MinIO storage to ~/minio (experimental): This should make MinIO   repositories accessible as subfolders / files of the <code>minio/</code> folder. This is   still experimental and may not work properly currently.</li> <li>Run a Protected B notebook: Enable this if the server you create needs   access to any Protected B resources. Protected B notebook servers run with many   security restrictions and have access to separate MinIO instances specifically   designed for Protected B data.</li> </ul>"},{"location":"1-Experiments/Kubeflow/#miscellaneous-settings","title":"Miscellaneous Settings","text":"<p>The following can be customized here:</p> <ul> <li>Enable Shared Memory: This is required if you use PyTorch with multiple data   loaders, which otherwise will generate an error. If using PyTorch make sure this   is enabled, otherwise it does not matter unless you have another application   that requires shared memory.</li> <li>System Language: Can specify English or French here.</li> </ul> <p></p>"},{"location":"1-Experiments/Kubeflow/#and-create","title":"And... Create!!!","text":"<ul> <li>If you're satisfied with the settings, you can now create the server! It may   take a few minutes to spin up depending on the resources you asked for. GPUs   take longer.</li> </ul> <p>Your server is running</p> <p>If all goes well, your server should be running!!! You will now have the option to connect, and try out Jupyter!</p>"},{"location":"1-Experiments/Kubeflow/#once-youve-got-the-basics","title":"Once you've got the basics ...","text":""},{"location":"1-Experiments/Kubeflow/#share-your-workspace","title":"Share your workspace","text":"<p>In Kubeflow every user has a namespace that contains their work (their notebook servers, pipelines, disks, etc.). Your namespace belongs to you, but can be shared if you want to collaborate with others. For more details on collaboration on the platform, see Collaboration.</p>"},{"location":"1-Experiments/MLflow/","title":"Overview","text":"<p>!!! danger \"MLflow has been removed from the AAW.     If you need it, contact the development team\"</p> <p>MLflow is an open source platform for managing the Machine Learning lifecycle. It is a \"Model Registry\" for storing your machine learning models and associated metrics. You can use the web interface to examine your models, and you can use its REST API to register your models from Python, using the mlflow pip package.</p> <p></p>"},{"location":"1-Experiments/Overview/","title":"Overview","text":""},{"location":"1-Experiments/Overview/#data-science-experimentation","title":"Data Science Experimentation","text":"<p>Process data using R, Python, or Julia with Kubeflow, a machine learning platform that provides a simple, unified, and scalable infrastructure for machine learning workloads.</p> <p>With Kubeflow, you can process data in a scalable and efficient way using the programming language of your choice. Once you have Kubeflow set up, use Jupyter Notebooks to create and share documents that contain live code, equations, or visualizations.</p> <p>You can also run Ubuntu as a virtual desktop with Kubeflow, giving you access to a powerful development environment that can be customized to your needs. With R Shiny, a web application framework for R, you can easily create and publish static and interactive dashboards to communicate your analysis results to stakeholders.</p> <p>Kubeflow also provides integration with external platforms as a service, such as Google Cloud Platform (GCP) and Amazon Web Services (AWS), allowing you to easily move data and workloads between different cloud services. Additionally, with Kubeflow's collaboration features, you can work on your projects with your team in real-time, sharing your analysis, code, and results seamlessly.</p> <p>Data science experimentation refers to the process of designing, conducting, and analyzing experiments in order to test hypotheses and gain insights from data. This process typically involves several steps:</p> <ol> <li> <p>Formulating a hypothesis: Before conducting an experiment, it is important to have a clear idea of what you are trying to test or learn. This may involve formulating a hypothesis about a relationship between variables, or trying to answer a specific research question.</p> </li> <li> <p>Designing the experiment: Once you have a hypothesis, you need to design an experiment that will allow you to test it. This may involve selecting a sample of data, choosing variables to manipulate or measure, and deciding on the experimental conditions.</p> </li> <li> <p>Collecting and cleaning the data: With the experiment designed, you need to collect the data necessary to test your hypothesis. This may involve gathering data from existing sources or conducting your own experiments. Once the data is collected, you need to clean it to remove any errors or anomalies.</p> </li> <li> <p>Analyzing the data: Once the data is clean, you can begin to analyze it to test your hypothesis. This may involve running statistical tests or machine learning algorithms, visualizing the data to identify patterns or trends, or using other analytical techniques to gain insights.</p> </li> <li> <p>Drawing conclusions: Based on the results of your analysis, you can draw conclusions about whether your hypothesis is supported or not. You may also be able to identify areas for further research or experimentation.</p> </li> </ol> <p>Data analysis is a key component of data science experimentation, and involves using various techniques and tools to make sense of large amounts of data. This may involve exploratory data analysis, where you use visualizations and summary statistics to gain an initial understanding of the data, or more advanced techniques such as machine learning or statistical modeling. Data analysis can be used to answer a wide range of questions, from simple descriptive questions about the data to more complex predictive or prescriptive questions.</p> <p>In summary, data science experimentation and data analysis are important components of the broader field of data science, and involve using data to test hypotheses, gain insights, and make informed decisions.</p>"},{"location":"1-Experiments/RStudio/","title":"Overview","text":"<p>RStudio is an integrated development environment (IDE) for R. It includes a console, editor, and tools for plotting, history, debugging and workspace management.</p>"},{"location":"1-Experiments/RStudio/#video-tutorial","title":"Video Tutorial","text":""},{"location":"1-Experiments/RStudio/#setup","title":"Setup","text":"<p>You can use the <code>rstudio</code> image to get an RStudio environment! When you create your notebook, choose RStudio from the list of available images. </p> <p></p> <p>You can install <code>R</code> or <code>python</code> packages with <code>conda</code> or <code>install.packages()</code>.</p>"},{"location":"1-Experiments/RStudio/#once-youve-got-the-basics","title":"Once you've got the basics ...","text":""},{"location":"1-Experiments/RStudio/#r-shiny","title":"R-Shiny","text":"<p>You can use <code>Shiny</code>, too! Shiny is an open source R package that provides a web framework for building web applications using R. Shiny helps you turn your analyses into interactive web applications.</p> <p></p>"},{"location":"1-Experiments/RStudio/#r-studio","title":"R Studio","text":"<p>Process data using R or Python in R Studio</p> <p> </p> <p>R Studio is a powerful integrated development environment (IDE) that supports both the R and Python programming languages, making it an ideal choice for data analysts and scientists. With R Studio, you can perform a wide range of data processing tasks, from data cleaning and transformation to statistical analysis and machine learning. The software provides a user-friendly interface and a variety of tools and libraries that simplify complex data analysis tasks. In addition, R Studio makes it easy to share your work with others by creating dynamic, interactive reports and visualizations that can be published online or shared as standalone documents. Overall, R Studio is a versatile and powerful tool that is essential for anyone working with data in R or Python.</p> <p>R Studio gives you an integrated development environment for R and Python. Use the r-studio-cpu image to get an R Studio environment.</p>"},{"location":"1-Experiments/Remote-Desktop/","title":"Overview","text":""},{"location":"1-Experiments/Remote-Desktop/#ubuntu-virtual-desktop","title":"Ubuntu Virtual Desktop","text":"<p>You can run a full Ubuntu Desktop, with typical applications, right inside your browser, using Kubeflow!</p> <p> </p> <p>The Ubuntu Virtual Desktop is a powerful tool for data scientists and machine learning engineers who need to run resource-intensive workloads in the cloud. Ubuntu is a popular Linux distribution that is widely used in the data science and machine learning communities due to its strong support for open source tools such as R and Python. With the Ubuntu Virtual Desktop, you can quickly spin up a virtual machine with Ubuntu pre-installed and access it from anywhere with an internet connection. This means you can perform data analysis and machine learning tasks from your laptop, tablet, or phone without having to worry about hardware limitations.</p>"},{"location":"1-Experiments/Remote-Desktop/#what-is-remote-desktop","title":"What is Remote Desktop?","text":"<p>Remote Desktop provides an in-browser GUI Ubuntu desktop experience as well as quick access to supporting tools. The operating system is Ubuntu 18.04 with the XFCE desktop environment.</p> <p></p>"},{"location":"1-Experiments/Remote-Desktop/#geomatics","title":"Geomatics","text":"<p>Our version of Remote Desktop is built on an R Geospatial image.</p>"},{"location":"1-Experiments/Remote-Desktop/#customization","title":"Customization","text":"<p>pip, conda, npm and yarn are available to install various packages.</p>"},{"location":"1-Experiments/Remote-Desktop/#setup","title":"Setup","text":""},{"location":"1-Experiments/Remote-Desktop/#accessing-the-remote-desktop","title":"Accessing the Remote Desktop","text":"<p>To launch the Remote Desktop or any of its supporting tools, create a Notebook Server in Kubeflow and select the remote desktop option.</p> <p></p> <p>Once it has been created, click <code>Connect</code> to be redirected to the Remote Desktop.</p> <p>Remote Desktop brings you to the Desktop GUI through a noVNC session. Click on the &gt; on the left side of the screen to expand a panel with options such as fullscreen and clipboard access.</p> <p></p>"},{"location":"1-Experiments/Remote-Desktop/#accessing-the-clipboard","title":"Accessing the Clipboard","text":"<p>This is done via the second button from the top of the panel on the left. It brings up a text box which we can modify to change the contents of the clipboard or copy stuff from the clipboard of the remote desktop.</p> <p>For example, suppose we want to execute the command <code>head -c 20 /dev/urandom | md5sum</code> and copy-paste the result into a text file on our computer used to connect to the remote desktop.</p> <p>We first open the clipboard from the panel on the left and paste in that command into the text box:</p> <p></p> <p>To close the clipboard window over the remote desktop, simply click the clipboard button again.</p> <p>We then right click on a terminal window to paste in that command and press enter to execute the command. At that point we select the MD5 result, right click, and click copy:</p> <p></p> <p>If we open the clipboard from the panel on the left again, it will now have the new contents:</p> <p></p> <p>The clipboard window will even update in-place if we leave it open the whole time and we simply select new material on the remote desktop and press copy again. We can simply copy what we have in that text box and paste it into any other software running on the computer used to connect.</p>"},{"location":"1-Experiments/Remote-Desktop/#in-browser-tools","title":"In-browser Tools","text":""},{"location":"1-Experiments/Remote-Desktop/#vs-code","title":"VS Code","text":"<p>Visual Studio Code is a lightweight but powerful source code editor. It comes with built-in support for JavaScript, TypeScript and Node.js and has a rich ecosystem of extensions for several languages (such as C++, C#, Java, Python, PHP, Go).</p> <p></p>"},{"location":"1-Experiments/Remote-Desktop/#footnotes","title":"Footnotes","text":"<p>Remote Desktop is based on ml-tooling/ml-workspace.</p>"},{"location":"1-Experiments/Selecting-an-Image/","title":"Selecting an Image for your Notebook Server","text":"<p>Depending on your project or use case of the Notebook Server, some images may be more suitable than others. The following will go through the main features of each to help you pick the most appropriate image for you.</p> <p>When selecting an image, you have 3 main options:</p> <ul> <li>Jupyter Notebook (CPU, TensorFlow, PyTorch)</li> <li>RStudio</li> <li>Remote Desktop (r, geomatics)</li> </ul>"},{"location":"1-Experiments/Selecting-an-Image/#jupyter-notebooks","title":"Jupyter Notebooks","text":"<p>Jupyter Notebooks are used to create and share interactive documents that contain a mix of live code, visualizations, and text. These can be written in <code>Python</code>, <code>Julia</code>, or <code>R</code>.</p> <p></p> Common uses include: <p>data transformation, numerical simulation, statistical modelling, machine learning and more.</p> <p>The jupyter notebooks are great launchpads for analytics including machine learning. The <code>jupyterlab-cpu</code> image gives a good core experience for python, including common packages such as <code>numpy</code>, <code>pandas</code> and <code>scikit-learn</code>. If you're interested specifically in using TensorFlow or PyTorch, we also have <code>jupyterlab-tensorflow</code> and <code>jupyterlab-pytorch</code> which come with those tools pre-installed.</p> <p>For the <code>jupyterlab-pytorch</code> image, the PyTorch packages (torch, torchvision, and torchaudio) are installed in the <code>torch</code> conda environment. You must activate this environment to use PyTorch.</p> <p>For the <code>jupyterlab-cpu</code>, <code>jupyterlab-tensorflow</code>, and <code>jupyterlab-pytorch</code> images, in the default shell the <code>conda activate</code> command may not work. This is due to the environment not being initialized properly. In this case run <code>bash</code>, you should see the AAW logo and a few instructions appear. After this <code>conda activate</code> should work properly. If you see the AAW logo on startup it means the environment is correctly initialized and <code>conda activate</code> should work properly. A fix for this bug is in the works, once this is fixed this paragraph will be removed.</p> <p>Each image comes pre-loaded with VS Code in the browser if you prefer a full IDE experience.</p>"},{"location":"1-Experiments/Selecting-an-Image/#rstudio","title":"RStudio","text":"<p>RStudio gives you an integrated development environment specifically for <code>R</code>. If you're coding in <code>R</code>, this is typically the Notebook Server to use. Use the <code>rstudio</code> image to get an RStudio environment.</p> <p></p>"},{"location":"1-Experiments/Selecting-an-Image/#remote-desktop","title":"Remote-Desktop","text":"<p>For a full Ubuntu desktop experience, use the remote desktop image. It comes pre-loaded with Python, R and Geomatics tooling, but are delivered in a typical desktop experience that also comes with Firefox, VS Code, and open office tools. The operating system is Ubuntu 18.04 with the XFCE desktop environment.</p> <p></p>"},{"location":"2-Publishing/Custom/","title":"Overview","text":""},{"location":"2-Publishing/Custom/#custom-web-apps","title":"Custom Web Apps","text":"<p>We can deploy anything as long as it's open source and we can put it in a Docker container. For instance, Node.js apps, Flask or Dash apps. Etc.</p> <p></p> <p>See the source code for this app</p> <p>We just push these kinds of applications through GitHub into the server. The source for the above app is <code>StatCan/covid19</code></p>"},{"location":"2-Publishing/Custom/#setup","title":"Setup","text":""},{"location":"2-Publishing/Custom/#how-to-get-your-app-hosted","title":"How to get your app hosted","text":"<p>If you already have a web app in a git repository then, as soon as it's containerized, we can fork the Git repository into the StatCan GitHub repository and point a URL to it. To update it, you'll just interact with the StatCan GitHub repository with Pull Requests.</p> <p>Contact us if you have questions.</p>"},{"location":"2-Publishing/Dash/","title":"Overview","text":"<p>Dash is a great tool used by many for data analysis, data exploration, visualization, modelling, instrument control, and reporting.</p> <p>The following example demonstrates a highly reactive and customised Dash app with little code.</p> <p>Running your Notebook Server and accessing the port</p> <p>When running any tool from your Jupyter Notebook that posts a website to a port, you will not be able to simply access it from <code>http://localhost:5000/</code> as normally suggested in the output upon running the web-app.</p> <p>To access the web server you will need to use the base URL. In your notebook terminal run:</p> <pre><code>echo https://kubeflow.covid.cloud.statcan.ca${JUPYTER_SERVER_URL:19}proxy/5000/\n</code></pre>"},{"location":"2-Publishing/Dash/#data-visualization-with-dash","title":"Data Visualization with Dash","text":"<p>Dash makes it simple to build an interactive GUI around your data analysis code. This is an example of a Layout With Figure and Slider from Dash.</p> <p></p>"},{"location":"2-Publishing/Dash/#plotly-dash","title":"Plotly Dash","text":"<p>Publish with Canadian-made software.</p> <p>Plotly Dash is a popular Python library that allows you to create interactive web-based visualizations and dashboards with ease. Developed by the Montreal-based company Plotly, Dash has gained a reputation for being a powerful and flexible tool for building custom data science graphics. With Dash, you can create everything from simple line charts to complex, multi-page dashboards with interactive widgets and controls. Because it's built on open source technologies like Flask, React, and Plotly.js, Dash is highly customizable and can be easily integrated with other data science tools and workflows. Whether you're a data scientist, analyst, or developer, Dash can help you create engaging and informative visualizations that bring your data to life.</p>"},{"location":"2-Publishing/Dash/#getting-started","title":"Getting Started","text":"<p>Open a terminal window in your Jupyter notebook and run the following commands:</p> <pre><code># required installations if not already installed\npip3 install dash==1.16.3\npip3 install pandas\n</code></pre> <p>Create a file called app.py with the following content:</p> <pre><code># app.py\n#!/usr/bin/env python3\nimport dash\nimport dash_core_components as dcc\nimport dash_html_components as html\nfrom dash.dependencies import Input, Output\nimport plotly.express as px\nimport pandas as pd\ndf = pd.read_csv('https://raw.githubusercontent.com/plotly/datasets/master/gapminderDataFiveYear.csv')\nexternal_stylesheets = ['https://codepen.io/chriddyp/pen/bWLwgP.css']\napp = dash.Dash(__name__, external_stylesheets=external_stylesheets)\napp.layout = html.Div([\ndcc.Graph(id='graph-with-slider'),\ndcc.Slider(\nid='year-slider',\nmin=df['year'].min(),\nmax=df['year'].max(),\nvalue=df['year'].min(),\nmarks={str(year): str(year) for year in df['year'].unique()},\nstep=None\n)\n])\n@app.callback(\nOutput('graph-with-slider', 'figure'),\n[Input('year-slider', 'value')])\ndef update_figure(selected_year):\nfiltered_df = df[df.year == selected_year]\nfig = px.scatter(filtered_df, x=\"gdpPercap\", y=\"lifeExp\",\nsize=\"pop\", color=\"continent\", hover_name=\"country\",\nlog_x=True, size_max=55)\nfig.update_layout(transition_duration=500)\nreturn fig\nif __name__ == '__main__':\napp.run_server(debug=True)\n</code></pre>"},{"location":"2-Publishing/Dash/#run-your-app","title":"Run your app","text":"<pre><code>python app.py\n# or you can use:\nexport FLASK_APP=app.py\nflask run\n</code></pre>"},{"location":"2-Publishing/Datasette/","title":"Overview","text":"<p>Datasette is an instant JSON API for your SQLite databases allowing you to explore the DB and run SQL queries in a more interactive way.</p> <p>You can find a list of example datasettes here.</p> <p>The Datasette Ecosystem</p> <p>There are all sorts of tools for converting data to and from sqlite here. For example, you can load shapefiles into sqlite, or create Vega plots from a sqlite database. SQLite works well with <code>R</code>, <code>Python</code>, and many other tools.</p>"},{"location":"2-Publishing/Datasette/#example-datasette","title":"Example Datasette","text":"<p>Below are some screenshots from the global-power-plants Datasette, you can preview and explore the data in the browser, either with clicks or SQL queries.</p> <p></p> <p>You can even explore maps within the tool!</p> <p></p>"},{"location":"2-Publishing/Datasette/#video-tutorial","title":"Video Tutorial","text":""},{"location":"2-Publishing/Datasette/#getting-started","title":"Getting Started","text":""},{"location":"2-Publishing/Datasette/#installing-datasette","title":"Installing Datasette","text":"<p>In your Jupyter Notebook, open a terminal window and run the command <code>pip3 install datasette</code>. </p>"},{"location":"2-Publishing/Datasette/#starting-datasette","title":"Starting Datasette","text":"<p>To view your own database in your Jupyter Notebook, create a file called start.sh in your project directory and copy the below code into it. Make the file executable using <code>chmod +x start.sh</code>. Run the file with <code>./start.sh</code>. Access the web server using the base URL with the port number you are using in the below file.</p> <p>start.sh</p> <pre><code>#!/bin/bash\n# This script just starts Datasette with the correct URL, so\n# that you can use it within kubeflow.\n# Get an example database\nwget https://github.com/StatCan/aaw-contrib-r-notebooks/raw/master/database-connections/latin_phrases.db\n\n# If you have your own database, you can change this line!\nDATABASE=latin_phrases.db\n\nexport BASE_URL=\"https://kubeflow.covid.cloud.statcan.ca${JUPYTER_SERVER_URL:19}proxy/8001/\"\necho \"Base url: ${BASE_URL}\"\ndatasette $DATABASE --cors --config max_returned_rows:100000 --config sql_time_limit_ms:5500 --config base_url:${BASE_URL}\n</code></pre> <p>Check out this video tutorial</p> <p>One user of the platform used Datasette along with a javascript dashboard. See this video for a demo.</p> <p>Running your Notebook Server and accessing the port</p> <p>When running any tool from your Jupyter Notebook that posts a website to a port, you will not be able to simply access it from <code>http://localhost:5000/</code> as normally suggested in the output upon running the web-app.</p> <p>To access the web server you will need to use the base URL. In your notebook terminal, run:</p> <pre><code>echo https://kubeflow.covid.cloud.statcan.ca${JUPYTER_SERVER_URL:19}proxy/5000/\n</code></pre>"},{"location":"2-Publishing/Overview/","title":"Overview","text":""},{"location":"2-Publishing/Overview/#statistical-publishing","title":"Statistical Publishing","text":"<p>Beautiful graphics is important in statistical publishing because it makes the data more accessible and understandable to a wider audience.</p> <p>Publishing is an essential aspect of data science and statistics. It allows researchers to share their findings with the broader scientific community, enabling others to build upon their work and push the field forward. By sharing their data and methods openly, researchers can receive feedback on their work and ensure that their findings are accurate and reliable.</p> <p>Publishing allows researchers to establish their reputation and gain recognition for their work, which can help secure funding and future research opportunities. In addition, publishing research findings can have important implications for public policy and decision-making, as policymakers often rely on scientific evidence to make informed decisions. Overall, publishing is an integral part of the scientific process and plays a critical role in advancing knowledge and solving real-world problems.</p> <p>Statistical publishing involves communicating statistical information to a broader audience using various forms of media, such as charts, graphs, and infographics. Having beautiful graphics is important in statistical publishing because it makes the data more accessible and understandable to a wider audience. Well-designed visualizations can help communicate complex statistical concepts and patterns in a clear and compelling way, allowing the audience to quickly grasp the main insights and conclusions.</p> <p>Beautiful graphics can enhance the overall impact of statistical publications, making them more engaging and memorable. This can be particularly important when communicating important information to decision-makers, stakeholders, or the general public, where the ability to clearly and effectively communicate data-driven insights can be critical to achieving success.</p> <p>In summary, data science and statistical publishing are essential for turning complex data into meaningful insights, and having beautiful graphics is a crucial aspect of effectively communicating those insights to a broader audience.</p>"},{"location":"2-Publishing/PowerBI/","title":"Overview","text":""},{"location":"2-Publishing/PowerBI/#loading-data-into-power-bi","title":"Loading data into Power BI","text":"<p>We do not offer a Power BI server, but you can pull your data into Power BI from our Storage system, and use the data as a <code>pandas</code> data frame.</p> <p></p>"},{"location":"2-Publishing/PowerBI/#setup","title":"Setup","text":""},{"location":"2-Publishing/PowerBI/#what-youll-need","title":"What you'll need","text":"<ol> <li>A computer with Power BI, and Python 3.6</li> <li>Your MinIO <code>ACCESS_KEY</code> and <code>SECRET_KEY</code> on hand. (See    Storage)</li> </ol>"},{"location":"2-Publishing/PowerBI/#set-up-power-bi","title":"Set up Power BI","text":"<p>Open up your Power BI system, and open up this Power BI quick start in your favourite text editor.</p> <p>You'll have to make sure that <code>pandas</code>, <code>boto3</code>, and <code>numpy</code> are installed, and that you're using the right Conda virtual environment (if applicable).</p> <p></p> <p>You'll then need to make sure that Power BI is using the correct Python environment. This is modified from the options menu, and the exact path is specified in the quick start guide.</p>"},{"location":"2-Publishing/PowerBI/#edit-your-python-script","title":"Edit your python script","text":"<p>Then, edit your Python script to use your MinIO <code>ACCESS_KEY</code> and <code>SECRET_KEY</code>, and then click \"Get Data\" and copy it in as a Python Script.</p> <p></p>"},{"location":"2-Publishing/R-Shiny/","title":"Overview","text":"<p>R-Shiny is an R package that makes it easy to build interactive web apps in R. </p> <p>R Shiny App Hosting</p> <p>We currently do not support hosting R Shiny apps but you are able to create them. We want to enable R Shiny app hosting in the future.</p> <p></p>"},{"location":"2-Publishing/R-Shiny/#r-shiny","title":"R Shiny","text":"<p>Publish Professional Quality Graphics</p> <p></p> <p>R Shiny is an open source web application framework that allows data scientists and analysts to create interactive, web-based dashboards and data visualizations using the R programming language. One of the main advantages of R Shiny is that it offers a straightforward way to create high-quality, interactive dashboards without the need for extensive web development expertise. With R Shiny, data scientists can leverage their R coding skills to create dynamic, data-driven web applications that can be shared easily with stakeholders.</p> <p>Another advantage of R Shiny is that it supports a variety of data visualizations that can be easily customized to meet the needs of the project. Users can create a wide range of charts and graphs, from simple bar charts and scatter plots to more complex heatmaps and network graphs. Additionally, R Shiny supports a variety of interactive widgets that allow users to manipulate and explore data in real-time.</p> <p></p> <p>R Shiny is also highly extensible and can be integrated with other open source tools and platforms to build end-to-end data science workflows. With its powerful and flexible features, R Shiny is a popular choice for building data visualization dashboards for a wide range of applications, from scientific research to business analytics. Overall, R Shiny offers a powerful, customizable, and cost-effective solution for creating interactive dashboards and data visualizations.</p> <p>Use R-Shiny to build interactive web apps straight from R. You can deploy your R Shiny dashboard by submitting a pull request to our R-Dashboards GitHub repository.</p>"},{"location":"2-Publishing/R-Shiny/#r-shiny-ui-editor","title":"R Shiny UI Editor","text":"<p>The following Rscript installs the required packages to run <code>shinyuieditor</code> on the AAW. It starts with installing the necessary R packages and uses <code>conda</code> to install <code>yarn</code>.</p> <p>Once the installation has finished you can access your app's code in <code>./my-app</code></p> <p>Run this script from inside <code>rstudio</code>. RStudio may ask for permission to open a new window if you have a popup blocker.</p> setup-shinyuieditor.R<pre><code>#!/usr/bin/env Rscript\n#' Install necessary packages\ninstall.packages(c(\n\"shiny\",\n\"plotly\",\n\"gridlayout\",\n\"bslib\",\n\"remotes\",\n\"rstudioapi\"\n))\n#' Was not installing when installing in the above\ninstall.packages(\"DT\") #' This installs shinyuieditor from Github\nremotes::install_github(\"rstudio/shinyuieditor\", upgrade = F)\n#' We need yarn so we'll install it with conda\nsystem(\"conda install yarn\", wait = T)\n#' This clones shinyuieditor and a sample app from Github\nsystem(\"git clone https://github.com/rstudio/shinyuieditor\", wait = T)\n#' Copy the app from vignettes to our current working directory\nsystem(\"cp -R ./shinyuieditor/vignettes/demo-app/ ./my-app\")\n#' Set the current working directory to the app's root directory\nsetwd(\"./my-app\")\n#' Yarn will set up our project\nsystem(\"yarn install\", wait = T)\n#' Load and launch shinyuieditor\nlibrary(shinyuieditor)\nshinyuieditor::launch_editor(app_loc = \"./\")\n</code></pre>"},{"location":"2-Publishing/R-Shiny/#choose-an-app-template","title":"Choose an App Template","text":"<p>The first thing you'll see is the template chooser. There are three options as of this writing (<code>shinyuieditor</code> is currently in alpha).</p> <p></p>"},{"location":"2-Publishing/R-Shiny/#single-or-multi-file-mode","title":"Single or Multi File Mode","text":"<p>I recommend Multi file mode, this will put the back-end code in a file called <code>server.R</code> and front-end in a file called <code>ui.R</code>.</p> <p></p>"},{"location":"2-Publishing/R-Shiny/#design-your-app","title":"Design Your App","text":"<p>You can design your app with either code or the graphical user interface. Try designing the layout with the GUI and designing the plots with code.</p> <p></p> <p>Any changes you make in <code>shinyuieditor</code> will appear immediately in the code. </p> <p></p> <p>Any change you make in the code will immediately appear in the <code>shinyuieditor</code>.</p> <p></p>"},{"location":"2-Publishing/R-Shiny/#publishing-on-the-aaw","title":"Publishing on the AAW","text":""},{"location":"2-Publishing/R-Shiny/#just-send-a-pull-request","title":"Just send a pull request!","text":"<p>All you have to do is send a pull request to our R-Dashboards repository. Include your repository in a folder with the name you want (for example, \"air-quality-dashboard\"). Then we will approve it and it will come online.</p> <p>If you need extra R libraries to be installed, send your list to the R-Shiny repository by creating a GitHub Issue and we will add the dependencies.</p> <p></p> <p>See the above dashboard here</p> <p>The above dashboard is in GitHub. Take a look at the source, and see the dashboard live.</p>"},{"location":"2-Publishing/R-Shiny/#once-youve-got-the-basics","title":"Once you've got the basics ...","text":""},{"location":"2-Publishing/R-Shiny/#embedding-dashboards-into-your-websites","title":"Embedding dashboards into your websites","text":"<p>Embedding dashboards in other sites</p> <p>We have not had a chance to look at this or prototype it yet, but if you have a use-case, feel free to reach out to engineering. We will work with you to figure something out.</p>"},{"location":"3-Pipelines/Kubeflow-Pipelines/","title":"Overview","text":"<p>Kubeflow pipelines are in the process of being removed from AAW.</p> <p>No new development should use Kubeflow pipelines. If you have questions about this removal, please speak with the AAW maintainers.</p> <p>Kubeflow Pipelines is a platform for building machine learning workflows for deployment in a Kubernetes environment. It enables authoring pipelines that encapsulate analytical workflows (transforming data, training models, building visuals, etc.). These pipelines can be shared, reused, and scheduled, and are built to run on compute provided via Kubernetes. Here is an example of a pipeline with many <code>sample</code> steps feeding into a single <code>average</code> step. This image comes from the Kubeflow Pipelines UI.</p> <p></p> <p>In the context of the Advanced Analytics Workspace, Kubeflow Pipelines are interacted with through:</p> <ul> <li>The Kubeflow UI, where from the Pipelines menu   you can upload pipelines, view the pipelines you have and their results, etc.</li> <li>The Kubeflow Pipelines python   SDK, accessible   through the   Jupyter Notebook Servers,   where you can define your components and pipelines, submit them to run now, or   even save them for later.</li> </ul> More examples in the notebooks <p>More comprehensive pipeline examples specifically made for this platform are available on GitHub (and in every Notebook Server at <code>/jupyter-notebooks</code>). You can also check out public sources.</p> <p>See the official Kubeflow docs for a more detailed explanation of Kubeflow Pipelines.</p>"},{"location":"3-Pipelines/Kubeflow-Pipelines/#what-are-pipelines-and-how-do-they-work","title":"What are pipelines and how do they work?","text":"<p>A pipeline in Kubeflow Pipelines consists of one or more pipeline components chained together to form a workflow. The components are like functions, describing the individual steps in your workflow (such as pulling columns from a data store, transforming data, or training a model). The pipeline is the logic that glues components together, such as:</p> <ol> <li>Run Component-A</li> <li>Pass the output from Component-A to Component-B and Component-C</li> <li>...</li> </ol> <p>In the above image, the logic would be running many <code>sample</code> steps followed by a single <code>average</code> step.</p> <p>At their core, each component has:</p> <ul> <li>A standalone application, packaged as a   Docker image, for doing the actual   work. The code in the Docker image could be a shell script, Python script, or   anything else you can run from a Linux terminal, and generally will have a   command line interface for data exchange (accessible through <code>docker run</code>)</li> <li>A YAML file that describes how Kubeflow Pipelines runs this code (what Docker   image should be run, what command line arguments does it accept, what output   does it generate)</li> </ul> <p>Each component should be single purpose, modular, and reusable.</p>"},{"location":"3-Pipelines/Kubeflow-Pipelines/#setup","title":"Setup","text":""},{"location":"3-Pipelines/Kubeflow-Pipelines/#define-and-run-your-first-pipeline-using-the-python-sdk","title":"Define and run your first pipeline using the Python SDK","text":"<p>While pipelines and components are defined in Kubeflow Pipelines by YAML files that use Docker images, that does not mean we have to work directly with either YAML files or Docker images. The Kubeflow Pipelines SDK provides a way for us to define our pipeline and components directly in Python code, where the SDK then translates our Python code to YAML files for us.</p> <p>For our first example, let's define a simple pipeline using only the Python SDK. The purpose of this section is to give a high level view of component and pipeline authoring, not a deep dive. More detailed looks into defining your own components, passing data between components, and returning data from your pipeline are explained in more detail in further sections.</p> <p>The demo pipeline we define will do the following:</p> <ol> <li>Accept five numbers as arguments</li> <li>Average of the first three numbers</li> <li>Average of the last two numbers</li> <li>Average of the results of (2) and (3)</li> </ol> <p>To do this, we will first define our component. Our <code>average</code> component will call a Docker image that does the following:</p> <ul> <li>Accepts one or more numbers as command line arguments</li> <li>Returns the average of these numbers by writing them to an output file in the   container (by default, to <code>out.txt</code>)</li> </ul> <p>This Docker image is already built for us and stored in our container registry here: <code>k8scc01covidacr.azurecr.io/kfp-components/average:v1</code>. Don't worry if you don't know Docker - since the image is built already, we only have to tell Kubeflow Pipelines where it is.</p> <p>??? info \"Full details of the <code>average</code> component's Docker image are in     GitHub\"     This image effectively runs the following code (slightly cleaned up for     brevity).  By making <code>average.py</code> accept an arbitrary set of numbers as     inputs, we can use the same <code>average</code> component for all steps in our     pipeline:</p> <pre><code>    import argparse\n\n    def parse_args():\n        parser = argparse.ArgumentParser(description=\"Returns the average of one or \"\n                                         \"more numbers as a JSON file\")\n        parser.add_argument(\"numbers\", type=float, nargs=\"+\", help=\"One or more numbers\")\n        parser.add_argument(\"--output_file\", type=str, default=\"out.txt\", help=\"Filename \"\n                            \"to write output number to\")\n        return parser.parse_args()\n\n    if __name__ == '__main__':\n        args = parse_args()\n        numbers = args.numbers\n        output_file = args.output_file\n\n        print(f\"Averaging numbers: {numbers}\")\n        avg = sum(numbers) / len(numbers)\n        print(f\"Result = {avg}\")\n\n        print(f\"Writing output to {output_file}\")\n        with open(output_file, 'w') as fout:\n            fout.write(str(avg))\n\n        print(\"Done\")\n</code></pre> <p>To make our <code>average</code> image into a Kubeflow Pipelines component, we make a <code>kfp.dsl.ContainerOp</code> in Python that defines how Kubeflow Pipelines interacts with our container, specifying:</p> <ul> <li>The Docker image location to use</li> <li>How to pass arguments to the running container</li> <li>What outputs to expect from the container</li> </ul> <p>We could use <code>ContainerOp</code> directly, but since we'll use <code>average</code> a few times we instead create a factory function we can reuse:</p> <pre><code>from kfp import dsl\ndef average_op(*numbers):\n\"\"\"\n    Factory for average ContainerOps\n    Accepts an arbitrary number of input numbers, returning a ContainerOp that\n    passes those numbers to the underlying Docker image for averaging\n    Returns output collected from ./out.txt from inside the container\n    \"\"\"\n# Input validation\nif len(numbers) &lt; 1:\nraise ValueError(\"Must specify at least one number to take the average of\")\nreturn dsl.ContainerOp(\nname=\"average\",  # What will show up on the pipeline viewer\nimage=\"k8scc01covidacr.azurecr.io/kfp-components/average:v1\",  # The image that KFP runs to do the work\narguments=numbers,  # Passes each number as a separate command line argument\n# Note that these arguments get serialized to strings\nfile_outputs={'data': './out.txt'},  # Expect an output file called out.txt to be generated\n# KFP can read this file and bring it back automatically\n)\n</code></pre> <p>To define our pipeline, we create a Python function decorated by the <code>@dsl.pipeline</code> decorator. We invoke our <code>average_op</code> factory to use our average container. We pass each <code>average</code> some inputs, and even use their outputs by accessing <code>avg_*.output</code>.</p> <pre><code>@dsl.pipeline(\nname=\"my pipeline's name\"\n)\ndef my_pipeline(a, b, c, d, e):\n\"\"\"\n    Averaging pipeline which accepts five numbers and does some averaging\n    operations on them\n    \"\"\"\n# Compute averages for two groups\navg_1 = average_op(a, b, c)\navg_2 = average_op(d, e)\n# Use the results from _1 and _2 to compute an overall average\naverage_result_overall = average_op(avg_1.output, avg_2.output)\n</code></pre> <p>Finally, while we've defined our pipeline in Python, Kubeflow Pipelines itself needs everything defined as a YAML file. This final step uses the Kubeflow Pipelines Python SDK to translate our pipeline function into a YAML file that describes exactly how Kubeflow Pipelines can interact with our component. Unzip it and take a look for yourself!</p> <pre><code>from kfp import compiler\npipeline_yaml = 'pipeline.yaml.zip'\ncompiler.Compiler().compile(\nmy_pipeline,\npipeline_yaml\n)\nprint(f\"Exported pipeline definition to {pipeline_yaml}\")\n</code></pre> Kubeflow Pipelines is a lazy beast <p>It is useful to keep in mind what computation is happening when you run this python code versus what happens when you submit the pipeline to Kubeflow Pipelines. Although it seems like everything is happening in the moment, try adding <code>print(avg_1.output)</code> to the above pipeline and see what happens when you compile your pipeline. The Python SDK we're using is for authoring pipelines, not for running them, so results from components will never be available when you run this Python code. The is discussed more below in Understanding what computation occurs when.</p> <p>To actually run our pipeline, we define an experiment:</p> <pre><code>experiment_name = \"averaging-pipeline\"\nimport kfp\nclient = kfp.Client()\nexp = client.create_experiment(name=experiment_name)\npl_params = {\n'a': 5,\n'b': 5,\n'c': 8,\n'd': 10,\n'e': 18,\n}\n</code></pre> <p>And then run an instance of our pipeline with the arguments we want:</p> <pre><code>import time\nrun = client.run_pipeline(\nexp.id,  # Run inside the above experiment\nexperiment_name + '-' + time.strftime(\"%Y%m%d-%H%M%S\"),  # Give our job a name with a timestamp so its unique\npipeline_yaml,  # Pass the .yaml.zip we created above.  This defines the pipeline\nparams=pl_params  # Pass our parameters we want to run the pipeline with\n)\n</code></pre> <p>This can all be seen in the Kubeflow Pipelines UI:</p> <p></p> <p></p> <p>Later when we want to reuse the pipeline, we can pass different arguments and do it all again.</p> <p>!!! info \"We create our experiment, upload our pipeline, and run from Python in     this example, but we could also do all this through the Kubeflow Pipelines     UI above.</p>"},{"location":"3-Pipelines/Kubeflow-Pipelines/#understanding-what-computation-occurs-when","title":"Understanding what computation occurs when","text":"<p>The above example uses Python code to define:</p> <ul> <li>The interface between Kubeflow Pipelines and our Docker containers doing the   work (by defining <code>ContainerOp</code>'s)</li> <li>The logic of our pipeline (by defining <code>my_pipeline</code>).</li> </ul> <p>But when we run <code>compiler.Compiler().compile()</code> and <code>client.run_pipeline()</code>, what actually happens?</p> <p>It is important to remember that everything we run in Python here is specifying the pipeline and its components in order to write YAML definitions, it is not doing the work of the pipeline. When running <code>compiler.Compiler().compile()</code>, we are not running our pipeline in the typical sense. Instead, KFP uses <code>my_pipeline</code> to build a YAML version of it. When we <code>compile</code>, the KFP SDK is passing placeholder arguments to <code>my_pipeline</code> and tracing where they (and any other runtime data) go, such as any output a component produces. When <code>compile</code> encounters a <code>ContainerOp</code>, nothing runs now - instead it takes note that a container will be there in future and remembers what data it will consume/generate. This can be seen by modifying and recompiling our pipeline:</p> <pre><code>@dsl.pipeline(\nname=\"my pipeline's name\"\n)\ndef my_pipeline(a, b, c, d, e):\n\"\"\"\n    Averaging pipeline which accepts five numbers and does some averaging operations on them\n    \"\"\"\n# NEW CODE\nx = 1 + 1\nprint(f\"The value of x is {x}\")\nprint(f\"The value of a is {a}\")\n# Compute averages for two groups\navg_1 = average_op(a, b, c)\navg_2 = average_op(d, e)\n# NEW CODE\nprint(f\"The value of avg_1.output is {avg_1.output}\")\n# Use the results from _1 and _2 to compute an overall average\naverage_result_overall = average_op(avg_1.output, avg_2.output)\n</code></pre> <p>And when we <code>compile</code>, we see print statements:</p> <pre><code>The value of x is 2\nThe value of a is {{pipelineparam:op=;name=a}}\nThe value of avg_1.output is {{pipelineparam:op=averge;name=data}}\n</code></pre> <p>In the first print statement everything is normal. In the second and third print statements, however, we see string placeholders rather than actual output. So while <code>compile</code> does \"execute\" <code>my_pipeline</code>, the KFP-specific parts of the code don't actually generate results. This can also be seen in the YAML file that <code>compile</code> generates, for example looking at the portion defining our <code>average_result_overall</code> component:</p> <pre><code>- name: average-3\ncontainer:\nargs:\n[\n\"{{inputs.parameters.average-data}}\",\n\"{{inputs.parameters.average-2-data}}\",\n]\nimage: k8scc01covidacr.azurecr.io/kfp-components/average:v1\ninputs:\nparameters:\n- { name: average-2-data }\n- { name: average-data }\noutputs:\nartifacts:\n- { name: average-3-data, path: ./out.txt }\nmetadata:\nlabels: { pipelines.kubeflow.org/pipeline-sdk-type: kfp }\n</code></pre> <p>In this YAML we see the input parameters passed are placeholders for data from previous components rather than their actual value. This is because while KFP knows a result from <code>average-data</code> and <code>average-2-data</code> will be passed to average, but the value of that result is not available until the pipeline is actually run.</p> Component naming within the YAML file <p>Because we made an <code>average_op</code> factory function with <code>name='average'</code> above, our YAML file has component names that automatically increment to avoid recreating the same name twice. We could have been fancier with our factory functions to more directly control our names, giving an argument like <code>name='average_first_input_args'</code>, or could even have explicitly defined the name in our pipeline by using <code>avg_1 = average_op(a, b, c).set_display_name(\"Average 1\")</code>.</p> <p>As one more example, let's try two more pipelines. One has a for loop inside which prints \"Woohoo!\" a fixed number of times. whereas the other does the same but loops <code>n</code> times (where <code>n</code> is a pipeline parameter):</p> <p>!!! info \"Pipeline parameters are described more below, but they work like     parameters for functions. Pipelines can accept data (numbers, string URL's     to large files in MinIO, etc.) as arguments, allowing a single generic     pipeline to work in many situations.\"</p> <pre><code>@dsl.pipeline(\nname=\"my pipeline's name\"\n)\ndef another_pipeline():\n\"\"\"\n    Prints to the screen 10 times\n    \"\"\"\nfor i in range(10):\nprint(\"Woohoo!\")\n# And just so we've got some component going too...\navg = average_op(n)\ncompiler.Compiler().compile(\nanother_pipeline,\n\"another.yaml.zip\"\n)\n</code></pre> <pre><code>@dsl.pipeline(\nname=\"my pipeline's name\"\n)\ndef another_another_pipeline(n):\n\"\"\"\n    Prints to the screen n times\n    \"\"\"\nfor i in range(n):\nprint(\"Woohoo!\")\n# And just so we've got some component going too...\navg = average_op(n)\ncompiler.Compiler().compile(\nanother_another_pipeline,\n\"another.yaml.zip\"\n)\n</code></pre> <p>The first works as you'd expect, but the second raises the exception:</p> <p><code>TypeError: 'PipelineParam' object cannot be interpreted as an integer</code></p> <p>Why? Because when authoring the pipeline <code>n</code> is a placeholder and has no value. KFP cannot define a pipeline from this because it does not know how many times to loop. We'd hit similar problems if using <code>if</code> statements. There are some ways around this, but they're left to the reader to explore through the Kubeflow Pipelines docs.</p> <p>Why does pipeline authoring behave this way? Because pipelines (and components) are meant to be reusable definitions of logic that are defined in static YAML files, with all dynamic decision making done inside components. This can make them a little awkward to define, but also helps them be more reusable.</p>"},{"location":"3-Pipelines/Kubeflow-Pipelines/#once-youve-got-the-basics","title":"Once you've got the basics ...","text":""},{"location":"3-Pipelines/Kubeflow-Pipelines/#data-exchange","title":"Data Exchange","text":""},{"location":"3-Pipelines/Kubeflow-Pipelines/#passing-data-into-within-and-from-a-pipeline","title":"Passing data into, within, and from a pipeline","text":"<p>In the first example above, we pass:</p> <ul> <li>Numbers into our pipeline</li> <li>Numbers between components within our pipeline</li> <li>A number back to the user at the end</li> </ul> <p>But as discussed above, pipeline arguments and component results are just placeholder objects \u2013 so how does KFP know our values are numeric? The answer is: it doesn't. In fact, it didn't even treat them as numbers above. Instead it treated them as strings. It is just that our pipeline components worked just as well with <code>\"5\"</code> as they would have with <code>5</code>.</p> <p>A safe default assumption is that all data exchange happens as a string. When we passed <code>a, b, ...</code> into the pipeline, those numbers were implicitly stringified because they eventually become command line arguments for our Docker container. When we read the result of <code>avg_1</code> from its <code>out.txt</code>, that result was read as a string. By calling <code>average_op(avg_1.output, avg_2.output)</code>, we ask KFP to pass the string output from <code>avg_1</code> and <code>avg_2</code> to a new <code>average_op</code>. It just so happened that, since <code>average_op</code> passes each string as a command line argument to our Docker image, it didn't really matter they were strings.</p> <p>You can still use non-string data types, but you need to pass them as serialized versions. So if we wanted our <code>avg_1</code> component to return both the numbers passed to it and the average returned as a dictionary, for example:</p> <pre><code>{\n'numbers': [5, 5, 8],\n'result': 6.0,\n}\n</code></pre> <p>We could modify our <code>average.py</code> in the Docker image write our dictionary of numbers and result to <code>out.txt</code> as JSON. But then when we pass the result to make <code>average_result_overall</code>, that component needs to deserialize the above JSON and pull the data from it that it needs. And because these results are not available when authoring the pipeline, something like this does not work:</p> <pre><code>def my_pipeline(a, b, c, d, e):\n\"\"\"\n    Averaging pipeline which accepts five numbers and does some averaging operations on them\n    \"\"\"\n# Compute averages for two groups\navg_1 = average_op_that_returns_json(a, b, c)\navg_2 = average_op_that_returns_json(d, e)\n# THIS DOES NOT WORK!\nimport json\navg_1_result = json.loads(avg_1.output)['result']\navg_2_result = json.loads(avg_2.output)['result']\n# Use the results from _1 and _2 to compute an overall average\naverage_result_overall = average_op(avg_1.output, avg_2.output)\n</code></pre> <p>At <code>compile</code> time, <code>avg_1.output</code> is just a placeholder and can't be treated like the JSON it will eventually become. To do something like this, we need to interpret the JSON string within a container.</p>"},{"location":"3-Pipelines/Kubeflow-Pipelines/#passing-secrets","title":"Passing Secrets","text":"<p>Pipelines often need sensitive information (passwords, API keys, etc.) to operate. To keep these secure, these cannot be passed to a Kubeflow Pipeline using a pipeline argument, environment variable, or as a hard coded value in python code, as they will be exposed as plain text for others to read.</p> <p>To address this issue, we use Kubernetes secrets as a way to securely store and pass sensitive information. Each secret is a key-value store containing some number of key-value pairs. The secrets can be passed by reference to the pipeline.</p> Secrets are only accessible within their own namespace <p>Secrets can only be accessed within the namespace they are created in. If you need to use a secret in another namespace, you need to add it there manually.</p>"},{"location":"3-Pipelines/Kubeflow-Pipelines/#create-a-key-value-store","title":"Create a key-value store","text":"<p>The example below creates a key-value store called elastic-credentials which contains two key-value pairs:</p> <pre><code>\"username\": \"USERNAME\",\n\"password\": \"PASSWORD\"\n</code></pre> <pre><code>kubectl create secret generic elastic-credentials \\\n--from-literal=username=\"YOUR_USERNAME\" \\\n--from-literal=password=\"YOUR_PASSWORD\"\n</code></pre>"},{"location":"3-Pipelines/Kubeflow-Pipelines/#get-an-existing-key-value-store","title":"Get an existing key-value store","text":"<pre><code># The secrets will be base64 encoded.\nkubectl get secret elastic-credentials\n</code></pre>"},{"location":"3-Pipelines/Kubeflow-Pipelines/#mounting-kubernetes-secrets-to-environment-variables-in-container-operations","title":"Mounting Kubernetes Secrets to Environment Variables in Container Operations","text":"<p>Once the secrets are defined in the project namespace, you can mount specific secrets as environment variables in your container using the Kubeflow SDK.</p> <p>Example</p> <p>This example is based off of a snippet from the Python KFP source code.</p> <p>This example shows how (1) an Elasticsearch username(2) an Elasticsearch password, and (3) a GitLab deploy token are passed to the container operation as environment variables.</p> <pre><code># Names of k8s secret key-value stores\nES_CREDENTIALS_STORE = \"elastic-credentials\"\nGITLAB_CREDENTIALS_STORE = \"gitlab-credentials\"\n# k8s secrets key names\nES_USER_KEY = \"username\"\nES_PASSWORD_KEY = \"password\"\nGITLAB_DEPLOY_TOKEN_KEY = 'token'\n# Names of environment variables that secrets should be mounted to in the\n# container\nES_USER_ENV = \"ES_USER\"\nES_PASS_ENV = \"ES_PASS\"\nGITLAB_DEPLOY_TOKEN_ENV = \"GITLAB_DEPLOY_TOKEN\"\n# ...\ncontainer_operation = dsl.ContainerOp(\nname='some-op',\nimage='some-image',\n).add_env_variable(\nk8s_client.V1EnvVar(\nname=ES_USER_ENV,\nvalue_from=k8s_client.V1EnvVarSource(\nsecret_key_ref=k8s_client.V1SecretKeySelector(\nname=ES_CREDENTIALS_STORE,\nkey=ES_USER_KEY\n)\n)\n)\n) \\\n    .add_env_variable(\nk8s_client.V1EnvVar(\nname=ES_PASS_ENV,\nvalue_from=k8s_client.V1EnvVarSource(\nsecret_key_ref=k8s_client.V1SecretKeySelector(\nname=ES_CREDENTIALS_STORE,\nkey=ES_PASSWORD_KEY\n)\n)\n)\n) \\\n    .add_env_variable(\nk8s_client.V1EnvVar(\nname=GITLAB_DEPLOY_TOKEN_ENV,\nvalue_from=k8s_client.V1EnvVarSource(\nsecret_key_ref=k8s_client.V1SecretKeySelector(\nname=GITLAB_CREDENTIALS_STORE,\nkey=GITLAB_DEPLOY_TOKEN_KEY\n)\n)\n)\n)\n</code></pre>"},{"location":"3-Pipelines/Kubeflow-Pipelines/#parameterizing-pipelines","title":"Parameterizing pipelines","text":"<p>Whenever possible, create pipelines in a generic way: define parameters that might change as pipeline inputs instead of writing values directly in your Python code. For example, if you want a pipeline to process data from <code>minimal-tenant/john-smith/data1.csv</code>, don't hard code that path - instead, accept it as a pipeline parameter. This way you can call the same pipeline repeatedly by passing it the data location as an argument. You can see this approach in our example notebooks, where we accept MinIO credentials and the location to store our results as pipeline parameters.</p>"},{"location":"3-Pipelines/Kubeflow-Pipelines/#passing-complexlarge-data-tofrom-a-pipeline","title":"Passing complex/large data to/from a pipeline","text":"<p>Although small data can often be stringified, passing by string is not suitable for complex data (large parquet files, images, etc.). It is common to use blob storage (for example: MinIO) or other outside storage methods to persist data between components or even for later use. A typical pattern would be:</p> <ul> <li>Upload large/complex input data to blob storage (e.g. training data, a saved   model, etc.)</li> <li>Pass the location of this data into the pipeline as parameters, and make your   pipeline/components fetch the data as required</li> <li>For each component in a pipeline, specify where they place outputs in the same   way</li> <li>For each component also <code>return</code> the path where it has stored its data (in   this case, the string we passed it in the above bullet). This feels redundant,   but it is a common pattern that lets you chain operations together</li> </ul> <p>Here is a schematic example of this pattern:</p> <pre><code>def my_blobby_pipeline(path_to_numbers_1, path_to_numbers_2, path_for_output):\n\"\"\"\n    Averaging pipeline which accepts two groups of numbers and does some averaging operations on them\n    \"\"\"\n# Compute averages for two groups\navg_1 = average_op_that_takes_path_to_blob(path_to_numbers=path_to_numbers_1,\noutput_location=path_for_output + \"/avg_1\"\n)\navg_2 = average_op_that_takes_path_to_blob(numbers=path_to_numbers_2,\noutput_location=path_for_output + \"/avg_2\"\n)\n# Note that this assumes the average_op can take multiple paths to numbers.  You could also have an\n# aggregation component that combines avg_1 and avg_2 into a single file of numbers\npaths_to_numbers = [\navg_1.output,\navg_2.output\n]\naverage_result_overall = average_op(path_to_numbers=paths_to_numbers,\noutput_location=path_for_output + \"/average_result_overall\"\n)\n</code></pre> <p>Within this platform, the primary method for persisting large files is through MinIO as described in our Storage documentation. Examples of this are also described in our example notebooks (also found in <code>jupyter-notebooks/self-serve-storage/</code> on any notebook server).</p>"},{"location":"3-Pipelines/Kubeflow-Pipelines/#typical-development-patterns","title":"Typical development patterns","text":""},{"location":"3-Pipelines/Kubeflow-Pipelines/#end-to-end-pipeline-development","title":"End-to-end pipeline development","text":"<p>A typical pattern for building pipelines in Kubeflow Pipelines is:</p> <ol> <li>Define components for each of your tasks</li> <li>Compose your components in a <code>@dsl.pipeline</code> decorated function</li> <li><code>compile()</code> your pipeline, upload your YAML files, and run</li> </ol> <p>This pattern lets you define portable components that can be individually tested before combining them into a full pipeline. Depending on the type and complexity of task, there are different methods for building the components.</p>"},{"location":"3-Pipelines/Kubeflow-Pipelines/#methods-for-authoring-components","title":"Methods for authoring components","text":"<p>Fundamentally, every component in Kubeflow Pipelines runs a container. Kubeflow Pipelines offers several methods to define these components with different levels of flexibility and complexity.</p>"},{"location":"3-Pipelines/Kubeflow-Pipelines/#user-defined-container-components","title":"User-defined container components","text":"<p>You can define tasks through custom Docker images. The design pattern for this is:</p> <ul> <li>Define (update) code for your task and commit to Git</li> <li>Build an image from your task (through manual command or CI pipeline)</li> <li>Test running this Docker image locally (and iterate if needed)</li> <li>Push the image to a container registry (usually Docker hub, but it will be   Azure Container Registry in our case on the Advanced Analytics Workspace)</li> <li>Update the Kubeflow Pipeline to point to the new image (via <code>dsl.ContainerOp</code>   like above) and test the pipeline</li> </ul> <p>This lets you run anything you can put into a Docker image as a task in Kubeflow Pipelines. You can manage and test your images and have complete control over how they run and what dependencies use. The <code>docker run</code> interface for each container becomes the API that Kubeflow Pipelines <code>dsl.ContainerOp</code> interacts with \u2013 running the containers is effectively like running them locally using a terminal. Anything you can make into a container with that interface can be run in Kubeflow Pipelines.</p> <p>!!! danger \"...however, for security reasons the platform currently does not     allow users to build/run custom Docker images. This is planned for the     future, but in interim see Lightweight components for a way to develop     pipelines without custom images\"</p>"},{"location":"3-Pipelines/Kubeflow-Pipelines/#lightweight-python-components","title":"Lightweight Python components","text":"<p>While full custom containers offer great flexibility, sometimes they're heavier than needed. The Kubeflow Pipelines SDK also allows for Lightweight Python Components, which are components that can be built straight from Python without building new container images for each change. These components are great for fast iteration during development, as well as for simple tasks that can be written and managed easily.</p> <p>This is an example of a lightweight pipeline with a single component that concatenates strings:</p> <pre><code>import kfp\nfrom kfp import dsl\nfrom kfp.components import func_to_container_op\ndef concat_string(a, b) -&gt; str:\nreturn f\"({a} | {b})\"\nconcat_string_component = func_to_container_op(concat_string,\nbase_image=\"python:3.8.3-buster\"\n)\n@dsl.pipeline(\nname=\"My lightweight pipeline\",\n)\ndef pipeline(str1, str2, str3):\n# Note that we use the concat_string_component, not the\n# original concat_string() function\nconcat_result_1 = concat_string_component(str1, str2)\n# By using cancat_result_1's output, we define the dependency of\n# concat_result_2 on concat_result_1\nconcat_result_2 = concat_string_component(concat_result_1.output, str3)\n</code></pre> <p>We see that our <code>concat_string</code> component is defined directly in Python rather than from a Docker image. In the end, our function still runs in a container, but we don't have to built it ourselves: <code>func_to_container_op()</code> runs our Python code inside the provided base image (<code>python:3.8.3-buster</code>). This lets use avoid building every time we change our code. The base image can be anything accessible by Kubeflow, which includes all images in the Azure Container Registry and any whitelisted images from Docker hub.</p> Lightweight components have a number of advantages but also some drawbacks <p>See this description of their basic characteristics, as well as this example which uses them in a more complex pipeline</p> A convenient base image to use is the the image your notebook server is running <p>By using the same image as your notebook server, you ensure Kubeflow Pipelines has the same packages available to it as the notebook where you do your analysis.  This can help avoid errors from importing packages specific to your environment. You can find that link from the notebook server page as shown below, but make sure you prepend the registry URL (so the below image would have <code>base_image=k8scc01covidacr.azurecr.io/machine-learning-notebook-cpu:562fa4a2899eeb9ae345c51c2491447ec31a87d7</code> ).  Note that while using a fully featured base image for iteration is fine, it's good practice to keep production pipelines lean and only supply the necessary software.  That way you reduce the startup time for each step in your pipeline. </p>"},{"location":"3-Pipelines/Kubeflow-Pipelines/#defining-components-directly-in-yaml","title":"Defining components directly in YAML","text":"<p>Components can be defined directly with a YAML file, where the designer can run terminal commands from a given Docker image. This can be a great way to make non-Python pipeline components from existing containers. As with all components, we can pass both arguments and data files into/out of the component. For example:</p> <pre><code>name: Concat Strings\ninputs:\n- {\n      name: Input text 1,\n      type: String,\n      description: \"Some text to echo into a terminal and tee to a file\",\n}\n- {\n      name: Input text 2,\n      type: String,\n      description: \"Some text to echo into a terminal and tee to a file\",\n}\noutputs:\n- { name: Output filename, type: String }\nimplementation:\ncontainer:\nimage: bash:5\ncommand:\n- bash\n- -ex\n- -c\n- |\necho \"$0 | $1\" | tee $2\n- { inputValue: Input text 1 }\n- { inputValue: Input text 2 }\n- { outputPath: Output filename }\n</code></pre> <p>This example concatenates two strings like our lightweight example above. We then define a component in python from this YAML:</p> <pre><code>from kfp.components import load_component_from_file\necho_and_tee = load_component_from_file('path/to/echo_and_tee.yaml')\n@dsl.pipeline\ndef my_pipeline():\necho_and_tee_task_1 = echo_and_tee(\"My text to echo\")\n# A second use that consumes the return of the first one\necho_and_tee_task_2 = echo_and_tee(echo_and_tee_task_1.output)\n</code></pre> <p>See this example for more details on using existing components.</p>"},{"location":"3-Pipelines/Kubeflow-Pipelines/#reusing-existing-components","title":"Reusing existing components","text":"<p>Similar to well abstracted functions, well abstracted components can reduce the amount of code you have to write for any given project. For example, rather than teaching your machine learning <code>train_model</code> component to also save the resulting model to MinIO, you can instead have <code>train_model</code> return the model and then Kubeflow Pipelines can pass the model to a reusable <code>copy_to_minio</code> component. This reuse pattern applies to components defined through any means (containers, lightweight, or YAML). Take a look at our example notebook, which reuses provided components for simple file IO tasks.</p>"},{"location":"3-Pipelines/Machine-Learning-Model-Cloud-Storage/","title":"Statcan Protected B Data","text":"<p>Protected B</p> <p>The AAW is certified for hosting Protected B data!</p> <p>In order to upload Protected B data to the AAW, users will need to request access through the Customer Success Unit (CSU). AAW users will also need to provide a namespace, get a sponsor and get approval from OPMIC. Once the process has been approved, our Fair Data Infrastructure (FDI) team will then create a folder on Net A which in turn will give access to user(s) through the active directory. The data will then be able to transfer from Net A to AAW Cloud</p> <p>Storing machine learning models in a protected cloud storage environment is essential for ensuring the security and privacy of sensitive data. The Advanced Analytics Workspace (AAW) provides a secure and robust cloud storage environment that can be used to store machine learning models and other data assets.</p> <p>The AAW platform provides a protected cloud storage environment that is designed to meet the most stringent data security and privacy requirements. The storage environment is protected by industry-standard encryption and access controls, which ensures that only authorized personnel can access sensitive data. This protects against unauthorized access, data breaches, and other security threats.</p> <p>In addition to its robust security features, the AAW cloud storage environment is also highly scalable and flexible. This means that data scientists and machine learning engineers can easily scale their storage needs as their datasets and model sizes grow. This enables them to store and manage large volumes of data and models without having to worry about storage limitations or performance bottlenecks.</p> <p>Storing machine learning models in a protected cloud storage environment on the Advanced Analytics Workspace provides a secure, scalable, and flexible solution for managing and protecting sensitive data. By leveraging the cloud storage capabilities provided by the AAW platform, data scientists and machine learning engineers can focus on building and deploying their models with confidence, knowing that their data is protected and secure.</p>"},{"location":"3-Pipelines/Machine-Learning-Model-Cloud-Storage/#cloud-storage","title":"Cloud Storage","text":"<p>Cloud Storage Advantages</p> <p>Cloud storage offers several advantages for data science and machine learning, particularly in terms of scalability, accessibility, and cost-effectiveness.</p> <p>Firstly, cloud storage enables data scientists to store and process large amounts of data without having to worry about the limitations of local storage. This is particularly important in the context of machine learning, where large datasets are required for training and testing models. Cloud storage allows data scientists to scale up their storage capacity as needed, without having to invest in expensive hardware.</p> <p>Secondly, cloud storage allows data scientists to access data from anywhere, using any device with an internet connection. This enables collaboration across geographically distributed teams and allows data scientists to work remotely. Additionally, cloud storage makes it easier to share data with other stakeholders, such as business partners or customers. Finally, cloud storage is typically more cost-effective than on-premises storage, particularly for smaller organizations or those with limited IT resources.</p> <p>Overall, cloud storage is a reliable and convenient solution for storing and managing your data. Whether you need to store large amounts of data or just a few files, cloud storage makes it easy to manage your storage needs without the hassle of traditional storage solutions.</p> <p>The AAW platform provides several types of storage:</p> <ul> <li>Disks (also called Volumes on the Kubeflow Notebook Server creation screen)</li> <li>Buckets (\"Blob\" or S3 storage, provided through MinIO)</li> <li>Data Lakes (coming soon)</li> </ul> <p>Depending on your use case, either disk or bucket may be most suitable. Our storage overview will help you compare them.</p>"},{"location":"3-Pipelines/Machine-Learning-Model-Cloud-Storage/#disks","title":"Disks","text":"<p>Disks are added to your notebook server by adding Data Volumes.</p>"},{"location":"3-Pipelines/Machine-Learning-Model-Cloud-Storage/#buckets","title":"Buckets","text":"<p>MinIO is an S3-API compatible object storage system that provides an open source alternative to proprietary cloud storage services. While we currently use MinIO as our cloud storage solution, we plan on replacing it with s3-proxy in the near future. S3-proxy is a lightweight, open source reverse proxy server that allows you to access Amazon S3-compatible storage services with your existing applications. By switching to s3-proxy, we will be able to improve our cloud storage performance, security, and scalability, while maintaining compatibility with the S3 API.</p> <p></p> <p>MinIO is a cloud-native scalable object store. We use it for buckets (blob or S3 storage).</p>"},{"location":"3-Pipelines/Machine-Learning-Model-Cloud-Storage/#data-lakes-coming-soon","title":"Data Lakes (Coming Soon)","text":"<p>A data lake is a central repository that allows you to store all your structured and unstructured data at any scale. It's a cost-effective way to store and manage all types of data, from raw data to processed data, and it's an essential tool for data scientists.</p> <p>One of the primary advantages of a data lake is its flexibility. It allows you to store all types of data without the need to define a schema in advance, which is especially useful when dealing with unstructured data. This flexibility allows data scientists to easily explore, experiment, and extract insights from their data without being constrained by the limitations of a traditional relational database.</p> <p>Data lakes also enable data scientists to centralize their data, making it easier to manage and analyze large volumes of data from various sources. With a data lake, data scientists can easily ingest and store data from a variety of sources, such as databases, cloud storage, and third-party APIs. Additionally, data lakes often provide features for data governance, metadata management, and access control, which helps ensure the data is of high quality and compliant with relevant regulations.</p> <p>Furthermore, cloud-based data lakes provide scalable, cost-effective storage solutions that can be easily expanded at the click of a button. As a data scientist's data storage needs grow, they can add additional storage capacity to their data lake with minimal effort, without worrying about the underlying infrastructure or maintenance.</p> <p>Overall, data lakes are a critical tool for data scientists, as they provide the flexibility, scalability, and ease of use needed to store and manage large volumes of data, enabling data scientists to focus on extracting insights and value from the data.</p>"},{"location":"3-Pipelines/Machine-Learning-Model-Serving/","title":"Model Serving","text":"<p>In the context of governments, serving machine learning models means making trained models available for use by other applications and systems. This could include making predictions or classifications based on input data, or providing insights and recommendations based on the results of data analysis.</p> <p>Serving machine learning models in a government context raises important issues related to data privacy. Government agencies are often responsible for collecting and managing sensitive personal data, such as health records, financial data, and criminal records. When serving machine learning models, it's critical to ensure that this data is protected and that access to it is strictly controlled.</p> <p>To address these concerns, government agencies must implement robust data privacy and security measures when serving machine learning models. This could include encrypting data both at rest and in transit, implementing access controls and user authentication, and regularly monitoring for data breaches and vulnerabilities.</p> <p>In addition to data privacy and security, it's also important to consider the ethical implications of serving machine learning models in a government context. Machine learning models can be biased or discriminatory, leading to unfair treatment of certain groups of people. To mitigate these risks, government agencies must carefully evaluate and monitor their machine learning models, and take steps to address any biases or discrimination that may arise.</p> <p>Overall, serving machine learning models in a government context requires careful consideration of data privacy, security, and ethical concerns. By implementing robust measures to protect personal data and prevent bias, government agencies can leverage the power of machine learning to make better decisions and improve outcomes for citizens while maintaining trust and transparency.</p>"},{"location":"3-Pipelines/Machine-Learning-Model-Serving/#why-serve-with-us","title":"Why serve with us?","text":"<p>Serving machine learning models with the Advanced Analytics Workspace (AAW) has several advantages. First, the AAW is an open-source data analytics platform that provides access to a variety of advanced analytics tools, including Python, R, and SAS. This makes it easy to deploy machine learning models and integrate them into existing workflows.</p> <p>Second, the AAW supports multiple MLOps frameworks, including Couler, Seldon, and Argo Workflows. These frameworks provide a range of features, including model versioning, model serving, and model monitoring, that simplify the process of deploying and managing machine learning models in production.</p> <p>Third, the AAW provides a secure and scalable platform for serving machine learning models with Protected B status. Models can be served using containerized environments, such as Docker, which provide a high level of isolation and security. The AAW also provides access to cloud computing resources, allowing users to scale up their compute power as needed to handle high volumes of requests.</p> <p>Finally, the AAW is a collaborative platform that allows users to share code and data with other researchers and analysts. This fosters a community of users who can learn from each other's work and collaborate on projects that require advanced analytics capabilities.</p> <p>In summary, serving machine learning models with the Advanced Analytics Workspace provides access to advanced analytics tools, multiple MLOps frameworks, a secure and scalable Proteced B platform, and a collaborative community of users, making it an ideal platform for data scientists and analysts who want to deploy and manage machine learning models in production.</p>"},{"location":"3-Pipelines/Machine-Learning-Model-Serving/#seldon-core","title":"Seldon Core","text":"<p>Seldon Core is an open-source platform for deploying, scaling, and monitoring machine learning models on Kubernetes. It provides a simple and efficient way to deploy machine learning models as microservices in a production environment.</p> <p>Serving machine learning models using Seldon Core involves the following steps:</p> <ol> <li> <p>Model packaging: The first step is to package the trained machine learning model in a container image with all the required dependencies. Seldon Core supports various machine learning frameworks, including TensorFlow, PyTorch, and Scikit-learn.</p> </li> <li> <p>Model deployment: Once the container image is created, the next step is to deploy the model on Kubernetes using Seldon Core. This involves defining the deployment configuration file, which specifies the resources required for the deployment, such as the number of replicas and the compute resources.</p> </li> <li> <p>Model serving: Once the model is deployed, Seldon Core exposes a REST API endpoint that can be used to make predictions. Clients can send requests to the endpoint with input data, and the model will return the corresponding output. Seldon Core also supports various deployment patterns, such as canary deployment and A/B testing, to enable easy experimentation and testing of different models.</p> </li> <li> <p>Model monitoring: Seldon Core provides various monitoring capabilities to track the performance of deployed models. This includes real-time monitoring of model metrics, such as latency and throughput, as well as logging of request and response data for debugging purposes.</p> </li> </ol> <p>Seldon Core makes it easy to serve machine learning models at scale, with support for high availability, scalability, and fault tolerance. It also provides integration with various Kubernetes-native tools, such as Istio and Prometheus, to enable advanced monitoring and observability.</p>"},{"location":"3-Pipelines/Machine-Learning-Training-Pipelines/","title":"Training Machine Learning Models on the AAW","text":"<p>Info</p> <p>Training machine learning models involves using algorithms to learn patterns and relationships in data. This process involves identifying features or variables that are relevant to the problem at hand and using these features to make predictions or classifications. </p>"},{"location":"3-Pipelines/Machine-Learning-Training-Pipelines/#why-train-with-us","title":"Why train with us?","text":"<p>Training machine learning models on the Advanced Analytics Workspace (AAW) has several advantages.</p> <ol> <li> <p>Open Source: The AAW is an open source data platform hosted by Statistics Canada that provides secure (Protected B) access to a variety of data sources, including census data, surveys, and administrative records. This data can be used to train machine learning models and generate insights that can inform policy decisions and improve business processes.</p> </li> <li> <p>Versatile: The AAW is designed to handle large and complex datasets. It provides access to a range of advanced analytics tools, in any language you like, including Python, R, and SAS, which can be used to preprocess data, train machine learning models, and generate visualizations. Because the AAW leverages cloud technologies, users can scale up their computing power as needed*. *</p> </li> <li>Secure: The AAW is a secure platform (Protected B) that adheres to the highest standards of data privacy and security. Data can be stored and processed on the platform without risk of unauthorized access or data breaches.</li> </ol>"},{"location":"3-Pipelines/Machine-Learning-Training-Pipelines/#mlops-and-data-pipelines","title":"MLOps and Data Pipelines","text":"<p>Optimize Data Workflows</p> <p>MLOps and data pipelines are important tools used in the field of data science to manage and optimize data workflows.</p>"},{"location":"3-Pipelines/Machine-Learning-Training-Pipelines/#mlops","title":"MLOps","text":"<p>MLOps refers to the set of practices and tools used to manage the entire lifecycle of a machine learning model. This includes everything from developing and training the model to deploying it in production and maintaining it over time. MLOps ensures that machine learning models are reliable, accurate, and scalable, and that they can be updated and improved as needed.</p>"},{"location":"3-Pipelines/Machine-Learning-Training-Pipelines/#data-pipelines","title":"Data Pipelines","text":"<p>Data pipelines are a series of steps that help move data from one system or application to another. This includes collecting, cleaning, transforming, and storing data, as well as retrieving it when needed. Data pipelines are important for ensuring that data is accurate, reliable, and accessible to those who need it. </p> <p>Automation and Reliability</p> <p>MLOps and data pipelines help organizations manage the complex process of working with large amounts of data and developing machine learning models. By automating these processes and ensuring that data is accurate and reliable, organizations can save time and resources while making better decisions based on data-driven insights.</p>"},{"location":"3-Pipelines/Machine-Learning-Training-Pipelines/#why-containerized-mlops","title":"Why Containerized MLOps?","text":"<p>The advantages of using a containerized approach for training machine learning models with Argo Workflows include:</p> <ol> <li> <p>Reproducibility: Containerizing the machine learning model and its dependencies ensures that the environment remains consistent across different runs, making it easy to reproduce results.</p> </li> <li> <p>Scalability: Argo Workflows can orchestrate parallel jobs and complex workflows, making it easy to scale up the training process as needed.</p> </li> <li> <p>Portability: Containers can be run on any platform that supports containerization, making it easy to move the training process to different environments or cloud providers.</p> </li> <li> <p>Collaboration: By pushing the container to a container registry, other users can easily download and use the container for their own purposes, making it easy to collaborate on machine learning projects.</p> </li> </ol> <p>Argo Workflows and containerization provide a powerful and flexible approach for training machine learning models. By leveraging these tools, data scientists and machine learning engineers can build, deploy, and manage machine learning workflows with ease and reproducibility.</p>"},{"location":"3-Pipelines/Machine-Learning-Training-Pipelines/#how-to-train-models","title":"How to Train Models","text":"<p>There are multiple ways to train machine learning models and it is not our place to tell anyone how to do it. That being said we have provided below a couple of guides on how to train machine learning models using the tools available on the AAW. The first tutorial is about training a simple model directly in a JupyterLab notebook. The second tutorial assumes the user is more advanced and is interested in defining an MLOps pipeline for training models using Argo Workflows.</p>"},{"location":"3-Pipelines/Machine-Learning-Training-Pipelines/#create-a-notebook-server-on-the-aaw","title":"Create a Notebook Server on the AAW","text":"<p>Notebook Servers</p> <p>Regardless of whether you plan on working in JupyterLab, R Studio or something more advanced with Argo Workflows, you'll need the appropriate notebook server. Follow the instructions found here to get started. </p>"},{"location":"3-Pipelines/Machine-Learning-Training-Pipelines/#using-jupyterlab","title":"Using JupyterLab","text":"<p>JupyterLab is Popular</p> <p>Training machine learning models with JupyterLab is a popular approach among data scientists and machine learning engineers.</p> <p>Here you will find the steps required to train a machine learning model with JupyterLab on the AAW. Because we are a multi-lingual environment, we've done our best to provide code examples in our most popular languages, <code>Python</code>, <code>R</code> and <code>SAS</code>.</p>"},{"location":"3-Pipelines/Machine-Learning-Training-Pipelines/#1-import-the-required-libraries","title":"1. Import the required libraries","text":"<p>Once you have a JupyterLab session running, you need to import the required libraries for your machine learning model. This could include libraries such as <code>NumPy</code>, <code>Pandas</code>, <code>Scikit-learn</code>, <code>Tensorflow</code>, or <code>PyTorch</code>. If you are using <code>R</code>, you'll want <code>tidyverse</code>, <code>caret</code> and <code>janitor</code>.</p> PythonRSASPySAS libraries.py<pre><code>#!/usr/bin/env python\n# tensorflow and keras for building and training deep learning models  \nimport tensorflow as tf\nfrom tensorflow import keras\n# numpy for numerical computations  \nimport numpy as np\n# pandas for data manipulation and analysis  \nimport pandas as pd\n# matplotlib for data visualization  \nimport matplotlib.pyplot as plt\n</code></pre> libraries.R<pre><code>#!/usr/bin/env Rscript\n# tidyverse for awesome data analysis and munging tools\nlibrary(tidyverse)\n# janitor to clean your data\nlibrary(janitor)\n# caret for easy machine learning\nlibrary(caret)\n</code></pre> libraries.py<pre><code>#!/usr/bin/env python\n# the only library you'll need to access SAS from Python\nimport saspy\n</code></pre> libraries.sas<pre><code>\n</code></pre> <p>About the Code</p> <p>The code examples you see in this document and throughout the documentation are for illustrative purposes to get you started on your projects. Depending on the specific task or project, other libraries and steps may be required.</p>"},{"location":"3-Pipelines/Machine-Learning-Training-Pipelines/#2-load-and-preprocess-the-data","title":"2. Load and preprocess the data","text":"<p>Next, you need to load and preprocess the data you'll be using to train your machine learning model. This could include data cleaning, feature extraction, and normalization. The exact preprocessing steps you'll need to perform will depend on the specific dataset you're working with, the requirements of your machine learning model and the job to be done.</p> PythonRSASPySAS load_data.py<pre><code>#!/usr/bin/env python\n# Import necessary packages\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n# Load data from a CSV file\ndata = pd.read_csv('data.csv')\n# Data cleaning! A lot more can be done, this is basic\ndata = data.dropna()  # Drop rows with missing values\ndata = data.drop_duplicates()  # Drop duplicate rows\n# Feature extraction\nX = data[['feature1', 'feature2', 'feature3']]  # Select relevant features\n# Normalization\nscaler = StandardScaler()  # Create a scaler object\nX_norm = scaler.fit_transform(X)  # Normalize the feature values\n</code></pre> load_data.R<pre><code>#!/usr/bin/env Rscript\n# Import necessary packages\nlibrary(tidyverse)\nlibrary(janitor)\n# Load data from a CSV file\ndata &lt;- read_csv(\"data.csv\")\n# Clean data using janitor\ndata_cleaned &lt;- data %&gt;%\n# Remove leading/trailing whitespace in column names\nclean_names() %&gt;%\n# Remove rows with missing values\nremove_empty() %&gt;%\n# Convert date column to Date format\nmutate(date = as.Date(date, format = \"%m/%d/%Y\")) %&gt;%\n# Remove duplicate rows\ndistinct() %&gt;%\n# Reorder columns\nselect(date, column2, column1, column3)\n</code></pre> load_data.py<pre><code>#!/usr/bin/env python\n# Import necessary packages\nimport saspy\n# Start a SAS session and check configuration information\nsas = saspy.SASsession(cfgname='default')\n# Load data from a CSV file\ndata = sas.read_csv(\"./data.csv\")\n</code></pre> load_data.sas<pre><code>/* Reading a comma delimited file with a .csv extension                       */\n/*                                                                            */\n/* Since the DBMS= value is CSV, you do not have to use the DELIMITER=        */\n/* statement.  By default, it is assumed the variable names are on the first  */\n/* row, so the GETNAMES= statement is not required.                           */\n/*                                                                            */\n/* Create comma delimited test file to read using PROC IMPORT below.          */\n/* Load data from a CSV file */\nproc import\n    datafile='data.csv'\nout=data\n    dbms=csv\n    replace; \nrun;\n/* Display data */\nproc print;\nrun;\n</code></pre>"},{"location":"3-Pipelines/Machine-Learning-Training-Pipelines/#3-split-the-data-into-training-and-testing-sets","title":"3. Split the data into training and testing sets","text":"<p>Once the data is preprocessed, you need to split it into training and testing sets. The training set will be used to train the machine learning model, while the testing set will be used to evaluate its performance.</p> PythonRSASPySAS train_test.py<pre><code>#!/usr/bin/env python\n# Import necessary packages\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X_norm,\ndata['target'], test_size=0.2, random_state=42)\n</code></pre> train_test.R<pre><code>#!/usr/bin/env Rscript\n# Import necessary packages\nlibrary(caret)\n# Load the dataset\ndata &lt;- read.csv(\"my-dataset.csv\")\n# Set the seed for reproducibility\nset.seed(123)\n# Split the dataset into train and test using caret's createDataPartition function\ntrain_index &lt;- createDataPartition(data$target_variable, p = 0.7, list = FALSE)\ntrain_data &lt;- data[train_index, ]\ntest_data &lt;- data[-train_index, ]\n</code></pre> train_test.py<pre><code>#!/usr/bin/env python\n</code></pre> train_test.sas<pre><code>\n</code></pre> <p>Note</p> <p>We split the data into training and testing sets using the <code>train_test_split</code> function from <code>scikit-learn</code>, which randomly splits the data into two sets based on the specified test size and random seed.</p>"},{"location":"3-Pipelines/Machine-Learning-Training-Pipelines/#4-define-and-train-the-machine-learning-model","title":"4. Define and train the machine learning model","text":"<p>With the data split, you can now define and train your machine learning model using the training set. This could involve selecting the appropriate algorithm, hyperparameter tuning, and cross-validation.</p> PythonRSASPySAS train.py<pre><code>#!/usr/bin/env python\n# Import necessary packages\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\n# Load the dataset\ndata = pd.read_csv(\"my-dataset.csv\")\n# Split the dataset into train and test\nX_train, X_test, y_train, y_test = train_test_split(data.iloc[:, :-1], data.iloc[:, -1], test_size=0.3, random_state=123)\n# Train the model\nmodel = RandomForestClassifier(n_estimators=100, random_state=123)\nmodel.fit(X_train, y_train)\n# Print the accuracy score on the test data\nprint(\"Accuracy on test set: {:.3f}\".format(model.score(X_test, y_test)))\n</code></pre> train.R<pre><code>#!/usr/bin/env Rscript\n# Import necessary packages\nlibrary(caret)\n# Load the dataset\ndata &lt;- read.csv(\"my-dataset.csv\")\n# Set the seed for reproducibility\nset.seed(123)\n# Split the dataset into train and test using caret's createDataPartition function\ntrain_index &lt;- createDataPartition(data$target_variable, p = 0.7, list = FALSE)\ntrain_data &lt;- data[train_index, ]\ntest_data &lt;- data[-train_index, ]\n# Define the training control\ntrain_control &lt;- trainControl(method = \"cv\", number = 5)\n# Train the model using caret's train function, (method = \"rf\" is for random forest)\nmodel &lt;- train(target_variable ~ ., data = train_data, method = \"rf\", trControl = train_control)\n# Print the model object to view the results\nprint(model)\n</code></pre> train.py<pre><code>#!/usr/bin/env python\n# Import necessary packages\nimport saspy\nimport pandas as pd\n# Establish a connection to a SAS session\nsas = saspy.SASsession()\n# Load the dataset\ndata = pd.read_csv(\"my-dataset.csv\")\n# Upload the dataset to SAS\nsas_df = sas.df2sd(data, \"mydata\")\n# Split the dataset into train and test\ntrain, test = sas.surveyselect(data=sas_df,\nmethod=\"SRS\",\nseed=123,\nsamprate=0.7,\noutall=True,\nstrata=\"target_variable\",\npartind=True)\n# Train the model using the HPFOREST procedure\nmodel = sas.hpforest(data=train,\ntarget=\"target_variable\",\ninput=\"input_variable_1-input_variable_n\",\npartition=\"rolevar\",\nrolevars={\"test\": \"0\", \"train\": \"1\"},\nnominals=[\"input_variable_1-input_variable_n\"],\nforestopts={\"ntree\": 100, \"seed\": 123})\n# Score the model on the test data\npredictions = model.predict(newdata=test, out=pred_out)\n# Compute the accuracy score on the test data\naccuracy = sas.freq(data=predictions, tables=\"target_variable*p_target_variable\", nocum=True, nocol=True)\n# Print the accuracy score\nprint(\"Accuracy on test set: {:.3f}\".format(accuracy.Frequency.iloc[0, 1] / accuracy.Frequency.iloc[:, 1].sum()))\n# Disconnect from the SAS session\nsas.disconnect()\n</code></pre> train.sas<pre><code>/* Load the dataset */\nproc import datafile=\"my-dataset.csv\" out=mydata dbms=csv replace;\nrun;\n/* Split the dataset into train and test */\nproc surveyselect data=mydata method=srs seed=123 out=selected outall\nsamprate=0.7;\nstrata target_variable;\nrun;\n/* Train the model */\nproc hpforest data=selected;\nclass _all_;\ntarget target_variable / level=nominal;\npartition rolevar=target_variable(test=\"0\" train=\"1\");\ninput _all_;\nforest ntree=100 seed=123;\nrun;\n/* Score the model on the test data */\nproc hpforest predict testdata=selected out=testout;\nrun;\n/* Print the accuracy score on the test data */\nproc freq data=testout;\ntables target_variable*p_target_variable / nocum nocol;\nrun;\n</code></pre>"},{"location":"3-Pipelines/Machine-Learning-Training-Pipelines/#5-evaluate-the-model","title":"5. Evaluate the model","text":"<p>After training the model, you need to evaluate its performance on the testing set. This will give you an idea of how well the model will perform on new, unseen data.</p> PythonRSASPySAS evaluate.py evaluate.R evaluate.py evaluate.sas"},{"location":"3-Pipelines/Machine-Learning-Training-Pipelines/#6-deploy-the-model","title":"6. Deploy the model","text":"<p>Finally, you can deploy the trained machine learning model in a production environment.</p> PythonRSASPySAS deploy.py deploy.R deploy.py deploy.sas"},{"location":"3-Pipelines/Machine-Learning-Training-Pipelines/#using-argo-workflows","title":"Using Argo WorkflowsArgo Workflows","text":"<p>MLOps Best Practices</p> <p>Argo Workflows is an excellent tool for anyone looking to implement MLOps practices and streamline the process of training and deploying machine learning models or other data science tasks such as ETL.</p> <p>Argo Workflows  is an open source container-native workflow engine for orchestrating parallel jobs on Kubernetes. Argo Workflows is implemented as a Kubernetes CRD (Custom Resource Definition). It is particularly well-suited for use in machine learning and data science workflows.</p> <p>Argo Workflows allows you to</p> <ul> <li>Define workflows where each step in the workflow is a container.</li> <li>Model multi-step workflows as a sequence of tasks or capture the dependencies between tasks using a directed acyclic graph (DAG).</li> <li>Easily run compute intensive jobs for machine learning or data processing in a fraction of the time using Argo Workflows on Kubernetes.</li> <li>Run CI/CD pipelines natively on Kubernetes without configuring complex software development products.</li> </ul> <p>making it easy to manage the entire end-to-end machine learning pipeline. With Argo Workflows, you can easily build workflows that incorporate tasks such as data preprocessing, model training, and model deployment, all within a Kubernetes environment.</p> <p> </p> <p>Below are the steps to train a machine learning model using Argo Workflows on the AAW.</p>"},{"location":"3-Pipelines/Machine-Learning-Training-Pipelines/#1-write-a-script-to-train-your-model","title":"1. Write a script to train your model","text":"<p>Here's an example script that trains a logistic regression model on the iris dataset. Don't forget to view the code from each language below.</p> PythonR train.py<pre><code>#!/usr/bin/env python\n# Import necessary libraries\nimport argparse\nimport pandas as pd\nfrom sklearn.datasets import load_iris\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nimport joblib\n# Parse input arguments\nparser = argparse.ArgumentParser(description=\"Train logistic regression model on iris dataset.\")\nparser.add_argument(\"--input\", default=\"iris.csv\", help=\"Path to input dataset file.\")\nparser.add_argument(\"--output\", default=\"model.pkl\", help=\"Path to output model file.\")\nargs = parser.parse_args()\n# Load iris dataset\ndata = load_iris()\nX, y = data.data, data.target\n# Split data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n# Train logistic regression model\nclf = LogisticRegression(random_state=42)\nclf.fit(X_train, y_train)\n# Evaluate model on test set\ny_pred = clf.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy:\", accuracy)\n# Save model to file\njoblib.dump(clf, args.output)\n</code></pre> train.R<pre><code>#!/usr/bin/env Rscript\n# Import necessary libraries\nlibrary(caret)\n# Parse input arguments\nargs &lt;- commandArgs(trailingOnly = TRUE)\ninput_file &lt;- ifelse(length(args) &gt; 0, args[1], \"iris.csv\")\noutput_file &lt;- ifelse(length(args) &gt; 1, args[2], \"model.rds\")\n# Load iris dataset\ndata(iris)\nX &lt;- iris[, 1:4]\ny &lt;- iris[, 5]\n# Split data into train and test sets\nset.seed(42)\ntrain_index &lt;- createDataPartition(y, p = 0.8, list = FALSE)\nX_train &lt;- X[train_index, ]\ny_train &lt;- y[train_index]\nX_test &lt;- X[-train_index, ]\ny_test &lt;- y[-train_index]\n# Train logistic regression model\nclf &lt;- train(x = X_train, y = y_train, method = \"glm\")\n# Evaluate model on test set\ny_pred &lt;- predict(clf, newdata = X_test)\naccuracy &lt;- confusionMatrix(y_pred, y_test)$overall[\"Accuracy\"]\nprint(paste0(\"Accuracy: \", accuracy))\n# Save model to file\nsaveRDS(clf, output_file)\n</code></pre>"},{"location":"3-Pipelines/Machine-Learning-Training-Pipelines/#2-write-a-dockerfile-to-run-your-code","title":"2. Write a Dockerfile to run your code","text":"<p>You'll need a Dockerfile that includes all necessary dependencies for training your machine learning model. This could include</p> <ul> <li>packages like<ul> <li><code>scikit-learn</code>, <code>pandas</code> or <code>numpy</code> if you are using <code>Python</code></li> <li><code>caret</code>, <code>janitor</code> and <code>tidyverse</code> if you are using <code>R</code> </li> </ul> </li> <li>your own custom libraries or scripts</li> <li>your machine learning model code in the form of a script as in the above example.</li> </ul> <p>Use the following <code>Dockerfile</code> as a starting point for your <code>R</code> and <code>Python</code> projects.</p> PythonR Dockerfile<pre><code>FROM python:3.8-slim-buster\n# Install any necessary dependencies\nRUN pip install --no-cache-dir scikit-learn pandas numpy\n\n# Set working directory\nWORKDIR /app\n# Copy code into container\nCOPY train.py .\n\n# Set entrypoint\nENTRYPOINT [\"python\", \"train.py\"]\n</code></pre> Dockerfile<pre><code>FROM rocker/r-base:latest\nRUN apt-get update &amp;&amp; apt-get install -y --no-install-recommends \\\nlibssl-dev \\\nlibcurl4-openssl-dev \\\nlibxml2-dev \\\n&amp;&amp; apt-get clean \\\n&amp;&amp; rm -rf /var/lib/apt/lists/*\n\nRUN R -e 'install.packages(c(\"caret\", \"janitor\", \"tidyverse\"))'\nCOPY train.R /app/train.R\n\nWORKDIR /app\nENTRYPOINT [\"Rscript\", \"train.R\"]\n</code></pre>"},{"location":"3-Pipelines/Machine-Learning-Training-Pipelines/#3-write-your-workflow-in-yaml","title":"3. Write your workflow in YAML","text":"<p>YAML is Yet Another Markup Language and you'll need to write down the steps of your training pipeline in an Argo Workflows YAML file. This file should include reference to the Dockerfile you created in Step 1, as well as any input data and output data you'll be working with.</p> <p>Here is an example YAML file for a simple machine learning pipeline that trains a logistic regression model on the iris dataset. The only real difference between the <code>Python</code> and <code>R</code> versions is the command <code>command: [\"python\", \"train.py\"]</code> vs <code>command: [\"Rscript\", \"train.R\"]</code> and the models are stored in different formats, <code>pkl</code> for <code>python</code> and <code>rds</code> for <code>R</code>.</p> <p>The YAML file defines a single step called <code>train</code> that runs  script called <code>train.py</code> or <code>train.R</code> in the Docker image <code>machine-learning:v1</code>. The script takes an input dataset file, specified by a parameter called <code>dataset</code>, and outputs a trained model file to an output artifact called <code>model.pkl</code> or <code>model.rds</code> depending on the language used.</p> PythonR workflow.yaml<pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\ngenerateName: ml-pipeline-\nspec:\nentrypoint: train\ntemplates:\n- name: train\ncontainer:\nimage: machine-learning:v1\ncommand: [\"python\", \"train.py\"]\nargs: [\"--input\", \"{{inputs.parameters.dataset}}\", \"--output\", \"{{outputs.artifacts.model}}\"]\ninputs:\nparameters:\n- name: dataset\ndefault: \"iris.csv\"\noutputs:\nartifacts:\n- name: model\npath: /output/model.pkl\n</code></pre> workflow.yaml<pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\ngenerateName: ml-pipeline-\nspec:\nentrypoint: train\ntemplates:\n- name: train\ncontainer:\nimage: machine-learning:v1\ncommand: [\"Rscript\", \"train.R\"]\nargs: [\"--input\", \"{{inputs.parameters.dataset}}\", \"--output\", \"{{outputs.artifacts.model}}\"]\ninputs:\nparameters:\n- name: dataset\ndefault: \"iris.csv\"\noutputs:\nartifacts:\n- name: model\npath: /output/model.rds\n</code></pre>"},{"location":"3-Pipelines/Machine-Learning-Training-Pipelines/#4-submit-the-workflow-using-the-argo-workflows-cli","title":"4. Submit the workflow using the Argo Workflows CLI","text":"<p>To run the above workflow, you will first need to push the Dockerfile to our container registry and , and then submit the YAML file using the <code>argo submit</code> command. Once the pipeline has completed, you can retrieve the trained model file by downloading the output artifact from the <code>argo logs</code> command.</p> Terminal Emulator<pre><code>$ argo submit workflow.yaml       # submit a workflow spec to Kubernetes\n</code></pre>"},{"location":"3-Pipelines/Machine-Learning-Training-Pipelines/#5-monitor-the-pipeline-using-the-argo-workflows-cli","title":"5. Monitor the pipeline using the Argo Workflows CLI","text":"<p>As the pipeline runs, you can monitor its progress using the Argo Workflows CLI. This will show you which steps have completed successfully and which are still running. Below are some useful commands, for more information about the Argo Workflows CLI, please check out the official Argo Workflows CLI documentation.</p> Terminal Emulator<pre><code>$ argo list                       # list current workflows\n$ argo get workflow-xxx           # get info about a specific workflow\n$ argo logs workflow-xxx          # print the logs from a workflow\n$ argo delete workflow-xxx        # delete workflow\n</code></pre>"},{"location":"3-Pipelines/Machine-Learning-Training-Pipelines/#6-retrieve-the-trained-model","title":"6. Retrieve the trained model","text":"<p>Once the pipeline has completed, you can retrieve the output data using the argo logs command or by viewing the output artifacts using the CLI, i.e. navigate to the directory you specified in your script and locate the file <code>model.pkl</code> or <code>model.rds</code>. The following code snippet, taken from the above training script, tells the respective programming language where to save the trained model.</p> PythonR Saving Output Data<pre><code>#!/usr/bin/env python\n#\nparser.add_argument(\"--output\", default=\"model.pkl\", help=\"Path to output model file.\")\n#\n# Save model to file\njoblib.dump(clf, args.output)\n</code></pre> Saving Output Data<pre><code>#!/usr/bin/env Rscript\n#\noutput_file &lt;- ifelse(length(args) &gt; 1, args[2], \"model.rds\")\n#\n# Save model to file\nsaveRDS(clf, output_file)\n</code></pre>"},{"location":"3-Pipelines/Machine-Learning-Training-Pipelines/#examples-using-argo-workflows-based-sdks","title":"Examples using Argo Workflows-based SDKs","text":"<p>If you prefer to use a higher level framework, then we have <code>Couler</code> and <code>Hera</code>. These frameworks make the creation and management of complex workflows more accessible to a wider audience.</p>"},{"location":"3-Pipelines/Machine-Learning-Training-Pipelines/#hera","title":"Hera","text":"<p>Hera aims to simplify the process of building and submitting workflows by abstracting away many of the technical details through a simple application programming interface. It also uses a consistent set of terminology and concepts that align with Argo Workflows, making it easier for users to learn and use both tools together.</p>"},{"location":"3-Pipelines/Machine-Learning-Training-Pipelines/#couler","title":"Couler","text":"<p>Couler provides a simple, unified application programming interface for defining workflows using an imperative programming style. It also automatically constructs directed acyclic graphs (DAGs) for the workflows, which can help to simplify the process of creating and managing them.</p> CoulerHeraYAMLSeldon? couler.py<pre><code># Prepare your system\n!pip config --user set global.index-url https://jfrog.aaw.cloud.statcan.ca/artifactory/api/pypi/pypi-remote/simple\n!python3 -m pip install git+https://github.com/couler-proj/couler --ignore-installed\n# Define global variable for convenience\nNAMESPACE = \"&lt;your-namespace&gt;\"\n# Import necessary packages\nimport json\nimport random\nimport couler.argo as couler\nfrom couler.argo_submitter import ArgoSubmitter\n# Define the steps (functions) used in the workflow\ndef random_code():\nimport random\nres = \"heads\" if random.randint(0, 1) == 0 else \"tails\"\nprint(res)\ndef flip_coin():\nreturn couler.run_script(\nimage=\"k8scc01covidacr.azurecr.io/ubuntu\",\nsource=random_code\n)\ndef heads():\nreturn couler.run_container(\nimage=\"k8scc01covidacr.azurecr.io/ubuntu\",\ncommand=[\"sh\", \"-c\", 'echo \"it was heads\"']\n)\ndef tails():\nreturn couler.run_container(\nimage=\"k8scc01covidacr.azurecr.io/ubuntu\",\ncommand=[\"sh\", \"-c\", 'echo \"it was tails\"']\n)\nresult = flip_coin()\ncouler.when(couler.equal(result, \"heads\"), lambda: heads())\ncouler.when(couler.equal(result, \"tails\"), lambda: tails())\nsubmitter = ArgoSubmitter(namespace=\"NAMESPACE\")\nresult = couler.run(submitter=submitter)\nprint(json.dumps(result, indent=2))\n</code></pre> hera.py<pre><code># Prepare your system\n!pip config --user set global.index-url https://jfrog.aaw.cloud.statcan.ca/artifactory/api/pypi/pypi-remote/simple\n!pip install hera-workflows\n# Import necessary packages\nimport hera\nfrom hera import Task, Workflow\n# Configure Hera\nhera.global_config.GlobalConfig.token = \"&lt;your-token&gt;\"\nhera.global_config.GlobalConfig.host = \"https://argo-workflows.aaw-dev.cloud.statcan.ca:443\"\nhera.global_config.GlobalConfig.namespace = \"&lt;your-namespace&gt;\"\nhera.global_config.GlobalConfig.service_account_name = \"&lt;your-account-name&gt;\"\n# Define the steps (functions) used in the workflow\ndef random_code():\nres = \"heads\" if random.randint(0, 1) == 0 else \"tails\"\nprint(res)\ndef heads():\nprint(\"it was heads\")\ndef tails():\nprint(\"it was tails\")\n# Define the workflow\nwith Workflow(\"coin-flip\") as w:\nr = Task(\"r\", random_code)\nh = Task(\"h\", heads)\nt = Task(\"t\", tails)\nh.on_other_result(r, \"heads\")\nt.on_other_result(r, \"tails\")\n# Run the workflow\nw.create()\n</code></pre> workflow.yaml<pre><code>\n</code></pre> <pre><code>\n</code></pre>"},{"location":"3-Pipelines/Machine-Learning-Training-Pipelines/#additional-resources-for-argo-workflows","title":"Additional Resources for Argo Workflows","text":"<p>Example Argo Workflows workflows can be found in the following Github repositories:</p> <ul> <li>Argo Workflows Documentation</li> <li>Argo CLI Reference</li> </ul>"},{"location":"3-Pipelines/Machine-Learning/","title":"Machine Learning Models","text":"<p>Machine learning models are computational algorithms that are designed to automatically learn patterns and relationships from data. These models are trained on a dataset, which is typically a collection of examples or instances, each of which consists of a set of features or variables, as well as a target variable or output.</p> <p>The goal of a machine learning model is to identify patterns and relationships within the data that can be used to make predictions or decisions about new, unseen data. This involves developing a mathematical representation of the relationship between the input features and the output variable, based on the patterns observed in the training data. Once the model is trained, it can be used to make predictions or decisions about new, unseen data.</p> <p>There are several different types of machine learning models, each of which is designed to address different types of problems or data. Some of the most common types of machine learning models include:</p> <ol> <li> <p>Regression Models: Regression models are used to predict continuous numerical values, such as stock prices or housing prices.</p> </li> <li> <p>Classification Models: Classification models are used to predict discrete categorical values, such as whether a customer will buy a product or not.</p> </li> <li> <p>Clustering Models: Clustering models are used to identify groups or clusters within a dataset based on similarities between instances.</p> </li> <li> <p>Recommendation Models: Recommendation models are used to recommend products or services to users based on their past behavior or preferences.</p> </li> <li> <p>Neural Networks: Neural networks are a type of machine learning model that is designed to mimic the structure and function of the human brain. They are commonly used in image recognition, speech recognition, and natural language processing applications.</p> </li> </ol> <p>Machine Learning Models Can be Biased</p> <p>Machine learning models are a powerful tool for analyzing and making predictions about data, and they have a wide range of applications in fields such as finance, healthcare, marketing, and more. However, it is important to note that machine learning models are not perfect and can sometimes make errors or produce biased results. Therefore, it is important to carefully evaluate and test machine learning models before using them in real-world applications.</p>"},{"location":"3-Pipelines/Machine-Learning/#examples","title":"Examples","text":""},{"location":"3-Pipelines/Machine-Learning/#linear-regression","title":"Linear Regression","text":"<p>Linear Regression</p> \\[ \\hat{Y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 X_i + \\hat{\\epsilon}_i \\] <p>Where \\(\\hat{Y}_i\\) denotes the \\(i\\)th estimator of the true value \\(Y\\) based on the \\(i\\)th training epoch. Each \\(\\hat{\\beta}\\) is a parameter to be learned. \\(\\hat{\\epsilon}_i\\) is the amount of noise permitted in the model and may vary depending on the training epoch number denoted by \\(i\\). Each \\(X_i\\) represents the \\(i\\)th batch of training data.</p> <p>In classical statistical models like linear regression, the goal is to find a line that best fits the data, allowing us to make predictions about new data points.</p> <p>As the complexity of the problem increases, more sophisticated algorithms are needed, such as decision trees, support vector machines, and random forests. However, these methods have limitations, and they may not be able to capture complex patterns in large datasets.</p>"},{"location":"3-Pipelines/Machine-Learning/#example-code","title":"Example Code","text":"PythonR linear_regression.py<pre><code>#!/usr/bin/env python\n# Load the required libraries\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\n# Load the dataset\ndata = pd.read_csv('path/to/dataset.csv')\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(data.drop('target_variable', axis=1), data['target_variable'], test_size=0.2)\n# Train the linear regression model\nlinear_model = LinearRegression()\nlinear_model.fit(X_train, y_train)\n# Make predictions on the testing set\ny_pred = linear_model.predict(X_test)\n# Evaluate the model performance\nmse = mean_squared_error(y_test, y_pred)\nrmse = mse ** 0.5\nprint('Root Mean Squared Error:', rmse)\n</code></pre> linear_regression.r<pre><code>#!/usr/bin/env Rscript\n# Set random seed for reproducibility\nset.seed(123)\n# Load the dataset\ndata &lt;- read.csv('path/to/dataset.csv')\n# Split the data into training and testing sets\ntrain_index &lt;- sample(1:nrow(data), size=0.8*nrow(data))\ntrain_data &lt;- data[train_index,]\ntest_data &lt;- data[-train_index,]\n# Train the linear regression model\nlm_model &lt;- lm(target_variable ~ ., data=train_data)\n# Make predictions on the testing set\ny_pred &lt;- predict(lm_model, newdata=test_data[,-which(names(test_data)=='target_variable')])\n# Evaluate the model performance\nmse &lt;- mean((y_pred - test_data$target_variable)^2)\nrmse &lt;- sqrt(mse)\nprint(paste('Root Mean Squared Error:', rmse))\n</code></pre>"},{"location":"3-Pipelines/Machine-Learning/#support-vector-machine-svm","title":"Support Vector Machine (SVM)","text":"<p>SVM</p> \\[ \\underset{\\mathbf{w},b,\\boldsymbol{\\xi}}{\\operatorname{minimize}} \\hspace{0.2cm} \\frac{1}{2} ||\\mathbf{w}||^2 + C \\sum_{i=1}^{N} \\xi_i \\] \\[ \\text{where} \\hspace{0.2cm} y_i(\\mathbf{w}^T\\mathbf{x}_i + b) \\geq 1-\\xi_i \\quad \\text{and} \\quad \\hspace{0.2cm} \\xi_i \\geq 0 \\hspace{0.2cm} \\forall i \\in {1,2,...,N} \\] <p>In this formula, we use the standard SVM formulation where \\(\\mathbf{w}\\) is the weight vector, \\(b\\) is the bias term, and \\(\\boldsymbol{\\xi}\\) is the slack variable vector. The objective is to minimize the L2-norm of the weight vector \\(\\mathbf{w}\\), subject to the constraint that all training examples are classified correctly with a margin of at least 1, plus an allowance for some margin violations controlled by the regularization parameter \\(C\\). The target variable \\(y_i\\) takes values of either 1 or -1, representing the two classes in the binary classification problem, and \\(\\mathbf{x}_i\\) is the feature vector for the \\(i\\)th training example.</p> <p>A support vector machine (SVM) is a supervised machine learning algorithm that can be used for classification, regression, and outlier detection. It is a popular algorithm in the field of machine learning, especially for solving classification problems.</p> <p>The basic idea behind SVM is to find a hyperplane that best separates the input data into different classes. In a two-class classification problem, the hyperplane is a line that divides the data points of one class from the data points of the other class. SVM tries to find the hyperplane that maximizes the margin between the two classes, where the margin is the distance between the hyperplane and the nearest data points from each class.</p>"},{"location":"3-Pipelines/Machine-Learning/#example-code_1","title":"Example Code","text":"PythonR svm.py<pre><code>#!/usr/bin/env python\n# Load the required libraries\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score\n# Load the dataset\ndata = pd.read_csv('path/to/dataset.csv')\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(data.drop('target_variable', axis=1), data['target_variable'], test_size=0.2)\n# Train the SVM model\nsvm_model = SVC(kernel='linear', C=1.0, random_state=42)\nsvm_model.fit(X_train, y_train)\n# Make predictions on the testing set\ny_pred = svm_model.predict(X_test)\n# Evaluate the model performance\naccuracy = accuracy_score(y_test, y_pred)\nprint('Accuracy:', accuracy)\n</code></pre> svm.r<pre><code>#!/usr/bin/env Rscript\n# Load the required libraries\nlibrary(e1071)\n# Load the dataset\ndata &lt;- read.csv('path/to/dataset.csv')\n# Split the data into training and testing sets\nset.seed(123)\ntrain_index &lt;- sample(1:nrow(data), size=0.8*nrow(data))\ntrain_data &lt;- data[train_index,]\ntest_data &lt;- data[-train_index,]\n# Train the SVM model\nsvm_model &lt;- svm(target_variable ~ ., data=train_data, kernel='linear', cost=1)\n# Make predictions on the testing set\ny_pred &lt;- predict(svm_model, newdata=test_data[,-which(names(test_data)=='target_variable')])\n# Evaluate the model performance\naccuracy &lt;- mean(y_pred == test_data$target_variable)\nprint(paste('Accuracy:', accuracy))\n</code></pre>"},{"location":"3-Pipelines/Machine-Learning/#random-forest","title":"Random Forest","text":"<p>Random Forest</p> \\[ \\hat{y} = \\frac{1}{T} \\sum_{t=1}^{T} f_t(\\mathbf{x}), \\] <p>where \\(\\hat{y}\\) is the predicted output, \\(f_t(\\mathbf{x})\\) is the prediction of the \\(t\\)th tree in the forest for the input \\(\\mathbf{x}\\), and \\(T\\) is the number of trees in the forest.</p> <p>Random Forests are an ensemble learning method that can be used for classification and regression problems. They are often used for their ability to handle high-dimContinuous Improvement:ensional datasets and nonlinear relationships between features and targets.</p> <p>Each tree is trained on a bootstrapped subset of the original training data, and at each split, a random subset of features is considered for determining the split. The final prediction is obtained by averaging the predictions of all the trees in the forest.</p>"},{"location":"3-Pipelines/Machine-Learning/#example-code_2","title":"Example Code","text":"PythonR random_forest.py<pre><code>#!/usr/bin/env python\n# Load the required libraries\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\n# Load the dataset\ndata = pd.read_csv('path/to/dataset.csv')\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(data.drop('target_variable', axis=1), data['target_variable'], test_size=0.2)\n# Train the random forest model\nrf_model = RandomForestRegressor(n_estimators=100, random_state=42)\nrf_model.fit(X_train, y_train)\n# Make predictions on the testing set\ny_pred = rf_model.predict(X_test)\n# Evaluate the model performance\nmse = mean_squared_error(y_test, y_pred)\nrmse = mse ** 0.5\nprint('Root Mean Squared Error:', rmse)\n</code></pre> random_forest.r<pre><code>#!/usr/bin/env Rscript\n# Load the required libraries   \nlibrary(randomForest)\n# Load the dataset\ndata &lt;- read.csv('path/to/dataset.csv')\n# Split the data into training and testing sets\nset.seed(123)\ntrain_index &lt;- sample(1:nrow(data), size=0.8*nrow(data))\ntrain_data &lt;- data[train_index,]\ntest_data &lt;- data[-train_index,]\n# Train the random forest model\nrf_model &lt;- randomForest(target_variable ~ ., data=train_data, ntree=100, importance=TRUE)\n# Make predictions on the testing set\ny_pred &lt;- predict(rf_model, newdata=test_data[,-which(names(test_data)=='target_variable')])\n# Evaluate the model performance\nmse &lt;- mean((y_pred - test_data$target_variable)^2)\nrmse &lt;- sqrt(mse)\nprint(paste('Root Mean Squared Error:', rmse))\n</code></pre>"},{"location":"3-Pipelines/Machine-Learning/#deep-learning","title":"Deep Learning","text":"<p>Deep Learning</p> \\[ \\hat{y} = f(\\mathbf{W}_L f(\\mathbf{W}_{L-1} f(\\dots f(\\mathbf{W}_1\\mathbf{x}+\\mathbf{b}_1)\\dots)+\\mathbf{b}_{L-1})+\\mathbf{b}_L) \\] <p>where \\(\\mathbf{x}\\) is the input vector, \\(\\mathbf{W}_i\\) and \\(\\mathbf{b}_i\\) are the weight matrix and bias vector, respectively, for the \\(i\\)th layer, and \\(f\\) is the activation function.</p> <p>This formula represents a feedforward neural network with \\(L\\) layers, where each layer applies a linear transformation to the output of the previous layer, followed by a non-linear activation function. The output of the final layer, \\(\\hat{y}\\), represents the predicted output of the neural network for the given input \\(\\mathbf{x}\\).</p> <p>Deep learning is a subset of machine learning that involves training neural networks with many layers of interconnected nodes. This approach can handle large and complex datasets and is used in a wide range of applications, including image recognition, natural language processing, and speech recognition. The training process involves feeding the neural network a large dataset and adjusting the weights of the connections between the nodes to minimize the error between the predicted outputs and the actual outputs. Through repeated iterations, the neural network can learn to recognize patterns in the data and make accurate predictions on new data.</p>"},{"location":"3-Pipelines/Machine-Learning/#example-code_3","title":"Example Code","text":"PythonR deep_learning.py<pre><code>#!/usr/bin/env python\n# Load the required libraries\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score\n# Load the dataset\ndata = pd.read_csv('path/to/dataset.csv')\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(data.drop('target_variable', axis=1), data['target_variable'], test_size=0.2)\n# Standardize the input features\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n# Define the deep learning model\nmodel = keras.Sequential([\nkeras.layers.Dense(64, activation='relu', input_shape=[X_train_scaled.shape[1]]),\nkeras.layers.Dropout(0.2),\nkeras.layers.Dense(1, activation='sigmoid')\n])\n# Compile the model\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n# Train the model\nhistory = model.fit(X_train_scaled, y_train, epochs=50, batch_size=32, validation_split=0.1)\n# Evaluate the model performance\ny_pred = model.predict_classes(X_test_scaled)\naccuracy = accuracy_score(y_test, y_pred)\nprint('Accuracy:', accuracy)\n</code></pre> deep_learning.r<pre><code>#!/usr/bin/env Rscript\n# Load the required libraries\nlibrary(keras)\nlibrary(tensorflow)\n# Load the dataset\ndata &lt;- iris\nx &lt;- as.matrix(data[, 1:4])\ny &lt;- to_categorical(as.numeric(data[, 5])-1)\n# Split the data into training and testing sets\nset.seed(123)\ntrain_index &lt;- sample(1:nrow(data), size=0.8*nrow(data))\nx_train &lt;- x[train_index,]\ny_train &lt;- y[train_index,]\nx_test &lt;- x[-train_index,]\ny_test &lt;- y[-train_index,]\n# Define the neural network architecture\nmodel &lt;- keras_model_sequential() %&gt;%\nlayer_dense(units = 8, input_shape = c(4)) %&gt;%\nlayer_activation('relu') %&gt;%\nlayer_dense(units = 3) %&gt;%\nlayer_activation('softmax')\n# Compile the model\nmodel %&gt;% compile(\nloss = 'categorical_crossentropy',\noptimizer = 'adam',\nmetrics = c('accuracy')\n)\n# Train the model\nhistory &lt;- model %&gt;% fit(\nx_train, y_train,\nepochs = 50,\nbatch_size = 10,\nvalidation_split = 0.2,\nverbose = 0\n)\n# Evaluate the model performance\nmetrics &lt;- model %&gt;% evaluate(x_test, y_test)\nprint(paste('Test Loss:', metrics[1]))\nprint(paste('Test Accuracy:', metrics[2]))\n# Plot the training and validation accuracy over time\nplot(history$metrics$accuracy, type='l', col='blue', ylim=c(0,1), ylab='Accuracy', xlab='Epoch')\nlines(history$metrics$val_accuracy, type='l', col='red')\nlegend('bottomright', legend=c('Training', 'Validation'), col=c('blue', 'red'), lty=1)\n</code></pre>"},{"location":"3-Pipelines/Overview/","title":"Overview","text":"<p>MLOps, or Machine Learning Operations, refers to the set of practices and tools that enable organizations to develop, deploy, and maintain machine learning models at scale. MLOps aims to streamline the end-to-end process of building and deploying machine learning models by integrating the various stages of the machine learning lifecycle into a cohesive and automated workflow.</p> <p>MLOps involves a range of different activities, including data preparation and preprocessing, model training and optimization, model deployment and serving, monitoring and maintenance, and continuous improvement. Some of the key components of MLOps include:</p> <ol> <li> <p>Data Management: MLOps involves managing and processing large amounts of data to ensure the quality and accuracy of machine learning models. This involves activities such as data cleaning, data integration, and data transformation.</p> </li> <li> <p>Model Training and Optimization: MLOps involves developing and testing machine learning models, as well as optimizing them for performance and accuracy. This may involve experimenting with different algorithms, hyperparameters, and data pre-processing techniques.</p> </li> <li> <p>Model Deployment: MLOps involves deploying machine learning models to production environments, making them available for use in real-world applications. This may involve containerizing models for easy deployment and scaling, as well as setting up APIs and other interfaces for model serving.</p> </li> <li> <p>Monitoring and Maintenance: MLOps involves monitoring machine learning models in production to ensure that they are performing as expected. This may involve setting up alerts and notifications for model failures, as well as implementing processes for model maintenance and updates.</p> </li> <li> <p>Continuous Improvement: MLOps involves continually improving machine learning models over time, based on feedback from users and ongoing analysis of performance data. This may involve retraining models with new data or incorporating feedback from users to refine models.</p> </li> </ol> <p>In order to implement MLOps effectively, organizations typically need to adopt a range of different tools and technologies, including data management platforms, machine learning frameworks, containerization tools, and monitoring and logging tools. They also need to establish clear workflows and processes for managing the various stages of the machine learning lifecycle, as well as implementing governance and compliance measures to ensure data privacy and security.</p> <p>In summary, MLOps is a critical component of the machine learning lifecycle, enabling organizations to develop, deploy, and maintain machine learning models at scale. By adopting MLOps practices and tools, organizations can streamline their machine learning workflows, improve model accuracy and performance, and deliver more value to users and stakeholders.</p>"},{"location":"3-Pipelines/PaaS-Integration/","title":"Overview","text":"<p>One of the main advantages of the AAW platform is its ability to integrate with popular machine learning platforms such as Databricks and AzureML.</p> <p>The Advanced Analytics Workspace (AAW) is an open source data analytics platform that is designed to be highly integrable. This means that it can be easily integrated with other platforms and tools to extend its capabilities and streamline workflows.</p> <p>An example diagram depicting a possible PaaS connection strategy:</p> <p></p> <p> </p> <p>Setup: If you need help integrating with a platform as a service offering, we're happy to help!</p>"},{"location":"3-Pipelines/PaaS-Integration/#integration-with-external-platform-as-a-service-paas-offerings","title":"Integration with External Platform as a Service (PaaS) Offerings","text":"<p>Integration is key to success.</p> <p></p> <p>Our open source platform offers unparalleled optionality to our users. By allowing users to use open source tools, we empower them to use their preferred data science and machine learning frameworks. But the real power of our platform comes from its ability to integrate with many Platform as a Service (PaaS) offerings, like Databricks or AzureML. This means that our users can leverage the power of the cloud to run complex data processing and machine learning pipelines at scale. With the ability to integrate with PaaS offerings, our platform enables our users to take their work to the next level, by giving them the power to scale their workloads with ease, and take advantage of the latest innovations in the field of data science and machine learning. By providing this level of optionality, we ensure that our users can always choose the right tool for the job, and stay ahead of the curve in an ever-changing field.</p> <p>We can integrate with many Platform as a Service (PaaS) offerings, like Databricks or AzureML.</p>"},{"location":"3-Pipelines/PaaS-Integration/#databricks","title":"Databricks","text":"<ul> <li>Databricks from Microsoft</li> </ul> <p>Databricks is a cloud-based platform that provides a unified analytics platform for big data processing and machine learning. With its powerful distributed computing engine and streamlined workflow tools, Databricks is a popular choice for building and deploying machine learning models. By integrating with Databricks, the AAW platform can leverage its distributed computing capabilities to train and deploy machine learning models at scale.</p>"},{"location":"3-Pipelines/PaaS-Integration/#azureml","title":"AzureML","text":"<ul> <li>Azure ML from Microsoft</li> </ul> <p>AzureML is another popular machine learning platform that provides a wide range of tools for building, training, and deploying machine learning models. By integrating with AzureML, the AAW platform can leverage its powerful tools for building and training models, as well as its ability to deploy models to the cloud.</p>"},{"location":"3-Pipelines/PaaS-Integration/#examples","title":"Examples","text":"<p>Examples of how to integrate the AAW platform with these and other platforms can be found on the MLOps Github repository.</p> <ul> <li>MLOps Github Repository</li> </ul> <p>This repository contains a range of examples and tutorials for using the AAW platform in various machine learning workflows, including data preparation, model training, and model deployment.</p>"},{"location":"3-Pipelines/PaaS-Integration/#conclusion","title":"Conclusion","text":"<p>By integrating with popular machine learning platforms like Databricks and AzureML, the AAW platform provides a powerful and flexible solution for building, deploying, and managing machine learning workflows at scale.</p> <p>By leveraging the integrations and tools provided by these platforms, data scientists and machine learning engineers can accelerate their workflows and achieve better results with less effort.</p>"},{"location":"3-Pipelines/PaaS/","title":"Overview","text":""},{"location":"3-Pipelines/PaaS/#integrate-with-platforms-like-databricks-and-azureml","title":"Integrate with Platforms like Databricks and AzureML","text":"<p>The AAW platform is built around the idea of integrations, and so we can integrate with many Platform as a Service (PaaS) offerings, such as Azure ML and Databricks.</p> <p>See some examples on our \"MLOps\" github Repo.</p> <p></p>"},{"location":"3-Pipelines/PaaS/#setup","title":"Setup","text":"<p>If you need help integrating with a platform as a service offering, we're happy to help!</p>"},{"location":"3-Pipelines/Serving/","title":"Model Serving with Seldon Core and KFServing","text":"<p>\u2692 This page is under construction \u2692</p> <p>The person writing this entry does not know enough about  this feature to write about it, but you can ask on our Slack channel.</p>"},{"location":"3-Pipelines/Serving/#serverless-with-knative","title":"Serverless with KNative","text":"<p>Kubernetes and KNative let your services scale up and down on demand. This lets you create APIs to serve Machine Learning models, without the need to manage load balancing or scale-up. The platform can handle all of your scaling for you, so that you can focus on the program logic.</p> <p>\u2692 This page is under construction \u2692</p> <p>The person writing this entry does not know enough about this  feature to write about it, but you can ask on our Slack channel.</p>"},{"location":"4-Collaboration/Geospatial-Analytical-Environment/","title":"Geospatial Analytical Environment (GAE) - Cross Platform Access","text":"<p>Unprotected data only, SSI Coming Soon!</p> <p>At this time, our Geospatial server can only host and provide access to non-sensitive statistical information.  </p>"},{"location":"4-Collaboration/Geospatial-Analytical-Environment/#getting-started","title":"Getting Started","text":"<p>Prerequisites</p> <ol> <li>An onboarded project with access to DAS GAE ArcGIS Portal    </li> <li>An ArcGIS Portal Client Id (API Key)</li> </ol> <p>The ArcGIS Enterprise Portal can be accessed in either the AAW or CAE using the API, from any service which leverages the Python programming language. </p> <p>For example, in AAW and the use of Jupyter Notebooks within the space, or in CAE the use of Databricks, DataFactory, etc.</p> <p>The DAS GAE ArcGIS Enterprise Portal can be accessed directly here</p> <p>For help with self-registering as a DAS Geospatial Portal user</p>"},{"location":"4-Collaboration/Geospatial-Analytical-Environment/#using-the-arcgis-api-for-python","title":"Using the ArcGIS API for Python","text":""},{"location":"4-Collaboration/Geospatial-Analytical-Environment/#connecting-to-arcgis-enterprise-portal-using-arcgis-api","title":"Connecting to ArcGIS Enterprise Portal using ArcGIS API","text":"<ol> <li> <p>Install packages:</p> <pre><code>conda install -c esri arcgis\n</code></pre> <p>or using Artifactory</p> <pre><code>conda install -c https://jfrog.aaw.cloud.statcan.ca/artifactory/api/conda/esri-remote arcgis\n</code></pre> </li> <li> <p>Import the necessary libraries that you will need in the Notebook.</p> <pre><code>from arcgis.gis import GIS\nfrom arcgis.gis import Item\n</code></pre> </li> <li> <p>Access the Portal</p> <p>Your project group will be provided with a Client ID upon onboarding. Paste the Client ID in between the quotations <code>client_id='######'</code>. </p> <pre><code>gis = GIS(\"https://geoanalytics.cloud.statcan.ca/portal\", client_id=' ')\nprint(\"Successfully logged in as: \" + gis.properties.user.username)\n</code></pre> </li> <li> <p>The output will redirect you to a login Portal.</p> <p>- Use the StatCan Azure Login option, and your Cloud ID    - After successful login, you will receive a code to sign in using SAML.    - Paste this code into the output. </p> <p></p> </li> </ol>"},{"location":"4-Collaboration/Geospatial-Analytical-Environment/#display-user-information","title":"Display user information","text":"<p>Using the 'me' function, we can display various information about the user logged in.</p> <pre><code>me = gis.users.me\nusername = me.username\ndescription = me.description\ndisplay(me)\n</code></pre>"},{"location":"4-Collaboration/Geospatial-Analytical-Environment/#search-for-content","title":"Search for Content","text":"<p>Search for the content you have hosted on the DAaaS Geo Portal. Using the 'me' function we can search for all of the hosted content on the account. There are multiple ways to search for content. Two different methods are outlined below.</p> <p>Search all of your hosted items in the DAaaS Geo Portal.</p> <pre><code>my_content = me.items()\nmy_content\n</code></pre> <p>Search for specific content you own in the DAaaS Geo Portal.</p> <p>This is similar to the example above, however if you know the title of they layer you want to use, you can save it as a function.</p> <pre><code>my_items = me.items()\nfor items in my_items:\nprint(items.title, \" | \", items.type)\nif items.title == \"Flood in Sorel-Tracy\":\nflood_item = items\nelse:\ncontinue\nprint(flood_item)\n</code></pre> <p>Search all content you have access to, not just your own.</p> <pre><code>flood_item = gis.content.search(\"tags: flood\", item_type =\"Feature Service\")\nflood_item\n</code></pre>"},{"location":"4-Collaboration/Geospatial-Analytical-Environment/#get-content","title":"Get Content","text":"<p>We need to get the item from the DAaaS Geo Portal in order to use it in the Jupyter Notebook. This is done by providing the unique identification number of the item you want to use. Three examples are outlined below, all accessing the identical layer.</p> <pre><code>item1 = gis.content.get(my_content[5].id) #from searching your content above\ndisplay(item1)\nitem2 = gis.content.get(flood_item.id) #from example above -searching for specific content\ndisplay(item2)\nitem3 = gis.content.get('edebfe03764b497f90cda5f0bfe727e2') #the actual content id number\ndisplay(item3)\n</code></pre>"},{"location":"4-Collaboration/Geospatial-Analytical-Environment/#perform-analysis","title":"Perform Analysis","text":"<p>Once the layers are brought into the Jupyter notebook, we are able to perform similar types of analysis you would expect to find in a GIS software such as ArcGIS. There are many modules containing many sub-modules of which can perform multiple types of analyses. </p> <p>Using the <code>arcgis.features</code> module, import the use_proximity submodule <code>from arcgis.features import use_proximity</code>. This submodule allows us to <code>.create_buffers</code> - areas of equal distance from features. Here, we specify the layer we want to use, distance, units, and output name (you may also specify other characteristics such as field, ring type, end type, and others). By specifying an output name, after running the buffer command, a new layer will be automatically uploaded into the DAaaS GEO Portal containing the new feature you just created. </p> <pre><code>buffer_lyr = use_proximity.create_buffers(item1, distances=[1], \nunits = \"Kilometers\", \noutput_name='item1_buffer')\ndisplay(item1_buffer)\n</code></pre> <p>Some users prefer to work with open source packages.  Translating from ArcGIS to Spatial Dataframes is simple.</p> <pre><code># create a Spatially Enabled DataFrame object\nsdf = pd.DataFrame.spatial.from_layer(feature_layer)\n</code></pre>"},{"location":"4-Collaboration/Geospatial-Analytical-Environment/#update-items","title":"Update Items","text":"<p>By getting the item as we did similar to the example above, we can use the <code>.update</code> function to update existing item within the DAaaS GEO Portal. We can update item properties, data, thumbnails, and metadata.</p> <pre><code>item1_buffer = gis.content.get('c60c7e57bdb846dnbd7c8226c80414d2')\nitem1_buffer.update(\nitem_properties={\n'title': 'Enter Title'\n'tags': 'tag1, tag2, tag3, tag4',\n'description': 'Enter description of item'\n})\n</code></pre>"},{"location":"4-Collaboration/Geospatial-Analytical-Environment/#visualize-your-data-on-an-interactive-map","title":"Visualize Your Data on an Interactive Map","text":"<p>Example: MatplotLib Library In the code below, we create an ax object, which is a map style plot. We then plot our data ('Population Change') change column on the axes <pre><code>import matplotlib.pyplot as plt\nax = sdf.boundary.plot(figsize=(10, 5))\nshape.plot(ax=ax, column='Population Change', legend=True)\nplt.show()\n</code></pre></p> <p>Example: ipyleaflet Library</p> <p>In this example we will use the library <code>ipyleaflet</code> to create an interactive map. This map will be centered around Toronto, ON. The data being used will be outlined below.</p> <p>Begin by pasting <code>conda install -c conda-forge ipyleaflet</code> allowing you to install <code>ipyleaflet</code> libraries in the Python environment.</p> <p></p> <p>Import the necessary libraries.</p> <pre><code>import ipyleaflet \nfrom ipyleaflet import *\n</code></pre> <p>Now that we have imported the ipyleaflet module, we can create a simple map by specifying the latitude and longitude of the location we want, zoom level, and basemap (more basemaps). Extra controls have been added such as layers and scale.</p> <p><pre><code>toronto_map = Map(center=[43.69, -79.35], zoom=11, basemap=basemaps.Esri.WorldStreetMap)\ntoronto_map.add_control(LayersControl(position='topright'))\ntoronto_map.add_control(ScaleControl(position='bottomleft'))\ntoronto_map\n</code></pre> </p>"},{"location":"4-Collaboration/Geospatial-Analytical-Environment/#learn-more-about-the-arcgis-api-for-python","title":"Learn More about the ArcGIS API for Python","text":"<p>Full documentation for the ArcGIS API can be located here</p>"},{"location":"4-Collaboration/Geospatial-Analytical-Environment/#learn-more-about-das-geospatial-analytical-environment-gae-and-services","title":"Learn More about DAS Geospatial Analytical Environment (GAE) and Services","text":"<p>GAE Help Guide</p>"},{"location":"4-Collaboration/Overview/","title":"Overview","text":""},{"location":"4-Collaboration/Overview/#collaboration","title":"Collaboration","text":"<p>Collaboration is essential in data science because it allows individuals with different perspectives and backgrounds to work together to solve complex problems and generate new insights. In data science, collaboration can involve working with individuals from diverse fields such as mathematics, computer science, and business, as well as subject matter experts who have deep knowledge of a particular industry or domain.</p> <p>There are many ways collaborate on the AAW. Which is best for your situation depends on what you're sharing and how many people you want to share with. Content to be shared breaks roughly into Data, Code, or Compute Environments (e.g.: sharing the same virtual machines) and who you want to share it with (No one, My Team, or Everyone). This leads to the following table of options</p> Private Team StatCan Code GitLab/GitHub or personal folder GitLab/GitHub or team folder GitLab/GitHub Data Personal folder or bucket Team folder or bucket, or shared namespace Shared Bucket Compute Personal namespace Shared namespace N/A <p>Sharing code, disks, and workspaces (e.g.: two people sharing the same virtual machine) is described in more detail below. Sharing data through buckets is described in more detail in the MinIO section.</p> What is the difference between a bucket and a folder? <p>Buckets are like Network Storage. See the Storage overview for more discussion of the differences between these two ideas.</p> <p>Choosing the best way to share code, data, and compute all involve different factors, but you can generally mix and match (share code with your team through Github, but store your data privately in a personal bucket). These cases are described more in the below sections.</p>"},{"location":"4-Collaboration/Overview/#share-code-among-team-members","title":"Share code among team members","text":"<p>In most cases, it is easiest to share code using GitHub or GitLab to share code. The advantage of sharing with GitHub or GitLab is that it works with users across namespaces, and keeping code in git is a great way to manage large software projects.</p> Don't forget to include a License! <p>If your code is public, do not forget to keep with the Innovation Team's guidelines and use a proper License if your work is done for Statistics Canada.</p> <p>If you need to share code without publishing it on a repository, sharing a namespace might work as well.</p>"},{"location":"4-Collaboration/Overview/#share-compute-namespace-in-kubeflow","title":"Share compute (namespace) in Kubeflow","text":"<p>Sharing a namespace means you share everything in the namespace</p> <p>Kubeflow does not support granular sharing of one resource (one notebook, one MinIO bucket, etc.), but instead sharing of all resources. If you want to share a Jupyter Notebook server with someone, you must share your entire namespace and they will have access to all other resources (MinIO buckets, etc.).</p> <p>In Kubeflow every user has a namespace that contains their work (their notebook servers, pipelines, disks, etc.). Your namespace belongs to you, but can be shared if you want to collaborate with others. You can also request a new namespace (either for yourself or to share with a team). One option for collaboration is to share namespaces with others.</p> <p>The advantage of sharing a Kubeflow namespace is that it lets you and your colleagues share the compute environment and MinIO buckets associated with the namespace. This makes it a very easy and free-form way to share.</p> <p>To share your namespace, see managing contributors</p> Ask for help in production <p>The Advanced Analytics Workspace support staff are happy to help with production oriented use cases, and we can probably save you lots of time. Don't be shy about asking us for help!</p>"},{"location":"4-Collaboration/Overview/#share-data","title":"Share data","text":"<p>Once you have a shared namespace, you have two shared storage approaches</p> Storage Option Benefits Shared Jupyter Servers/Workspaces More amenable to small files, notebooks, and little experiments. Shared Buckets (see Storage) Better suited for use in pipelines, APIs, and for large files. <p>To learn more about the technology behind these, check out the Storage overview.</p>"},{"location":"4-Collaboration/Overview/#sharing-with-statcan","title":"Sharing with StatCan","text":"<p>In addition to private buckets, or team-shared private buckets, you can also place your files in shared storage. Within all bucket storage options (<code>minimal</code>, <code>premium</code>, <code>pachyderm</code>), you have a private bucket, and a folder inside of the <code>shared</code> bucket. Take a look, for instance, at the link below:</p> <ul> <li><code>shared/blair-drummond/</code></li> </ul> <p>Any logged in user can see these files and read them freely.</p>"},{"location":"4-Collaboration/Overview/#sharing-with-the-world","title":"Sharing with the world","text":"<p>Ask about that one in our Slack channel. There are many ways to do this from the IT side, but it's important for it to go through proper processes, so this is not done in a \"self-serve\" way that the others are. That said, it is totally possible.</p>"},{"location":"4-Collaboration/Overview/#recommendation-combine-them-all","title":"Recommendation: Combine them all","text":"<p>It's a great idea to always use git, and using git along with shared workspaces is a great way to combine ad hoc sharing (through files) while also keeping your code organized and tracked.</p>"},{"location":"4-Collaboration/Overview/#managing-contributors","title":"Managing contributors","text":"<p>You can add or remove people from a namespace you already own through the Manage Contributors menu in Kubeflow.</p> <p></p> <p>Now you and your colleagues can share access to a server!</p> <p>Try it out!</p>"},{"location":"4-Collaboration/Request-a-Namespace/","title":"Overview","text":"<p>By default, everyone gets their own personal namespace, <code>firstname-lastname</code>. If you want to collaborate with your team, you can request a new namespace to share.</p>"},{"location":"4-Collaboration/Request-a-Namespace/#setup","title":"Setup","text":""},{"location":"4-Collaboration/Request-a-Namespace/#requesting-a-namespace","title":"Requesting a namespace","text":"<p>To create a namespace for a team, go to the AAW portal. Click the \u22ee menu on the Kubeflow section of the portal.</p> <p></p> <p>Enter the name you are requesting and submit the request. Be sure to use only lower case letters plus dashes. </p> <p>The namespace cannot have special characters other than hyphens</p> <p>The namespace name must only be lower-case letters <code>a-z</code> with dashes. Otherwise, the namespace will not be created.</p> <p>You will receive an email notification when the namespace is created. Once the shared namespace is created, you can access it the same as any other namespace you have through the Kubeflow UI, like shown below. You will then be able to share and manage to your namespace.</p> <p>To switch namespaces, take a look at the top of your window, just to the right of the Kubeflow Logo.</p> <p></p>"},{"location":"5-Storage/AzureBlobStorage/","title":"Overview","text":"<p>Azure Blob Storage is Microsoft's object storage solution for the cloud. Blob Storage is optimized for storing massive amounts of unstructured data. Unstructured data is data that doesn't adhere to a particular data model or definition, such as text or binary data. Azure Blob Storage Containers are good at three things:</p> <ul> <li>Large amounts of data - Containers can be huge: way bigger than hard drives. And   they are still fast.</li> <li>Accessible by multiple consumers at once - You can access the same data source   from multiple Notebook Servers and pipelines at the same time without needing   to duplicate the data.</li> <li>Sharing - Project namespaces can share a container. This is great for sharing data with people   outside of your workspace.</li> </ul>"},{"location":"5-Storage/AzureBlobStorage/#setup","title":"Setup","text":"<p>Azure Blob Storage containers and buckets mount will be replacing the Minio Buckets and Minio storage mounts</p> <p>Users will be responsible for migrating data from Minio Buckets to the Azure Storage folders. For larger files, users may contact AAW for assistance.</p>"},{"location":"5-Storage/AzureBlobStorage/#blob-container-mounted-on-a-notebook-server","title":"Blob Container Mounted on a Notebook Server","text":"<p>The Blob CSI volumes are persisted under <code>/home/jovyan/buckets</code> when creating a Notebook Server. Files under <code>/buckets</code> are backed by Blob storage. All AAW notebooks will have the <code>/buckets</code> mounted to the file-system, making data accessible from everywhere.</p> <p></p>"},{"location":"5-Storage/AzureBlobStorage/#unclassified-notebook-aaw-folder-mount","title":"Unclassified Notebook AAW folder mount","text":""},{"location":"5-Storage/AzureBlobStorage/#protected-b-notebook-aaw-folder-mount","title":"Protected-b Notebook AAW folder mount","text":"<p>These folders can be used like any other - you can copy files to/from using the file browser, write from Python/R, etc. The only difference is that the data is being stored in the Blob storage container rather than on a local disk (and is thus accessible wherever you can access your Kubeflow notebook).</p>"},{"location":"5-Storage/AzureBlobStorage/#container-types","title":"Container Types","text":"<p>The following Blob containers are available:</p> <p>Accessing all Blob containers is the same. The difference between containers is the storage type behind them:</p> <ul> <li>aaw-unclassified: By default,   use this one. Stores unclassified data.</li> </ul> <ul> <li>aaw-protected-b: Stores sensitive data protected-b.</li> </ul> <ul> <li>aaw-unclassified-ro: This classification is protected-b but read-only access. This is so users can view unclassified data within a protected-b notebook.</li> </ul>"},{"location":"5-Storage/AzureBlobStorage/#accessing-internal-data","title":"Accessing Internal Data","text":"<p>Accessing internal data uses the DAS common storage connection which has use for internal and external users that require access to unclassified or protected-b data.</p> <p>AAW has an integration with the FAIR Data Infrastructure team that allows users to transfer unclassified and protected-b data to Azure Storage Accounts, thus allowing users to access this data from Notebook Servers.</p> <p>Please reach out to the FAIR Data Infrastructure team if you have a use case for this data.</p>"},{"location":"5-Storage/AzureBlobStorage/#pricing","title":"Pricing","text":"<p>Pricing models are based on CPU and Memory usage</p> <p>Pricing is covered by KubeCost for user namespaces (In Kubeflow at the bottom of the Notebooks tab).</p> <p>In general, Blob Storage is much cheaper than Azure Manage Disks and has better I/O than managed SSD.</p>"},{"location":"5-Storage/Disks/","title":"Overview","text":"<p>Disks are the familiar hard drive style file systems you're used to, provided to you from fast solid state drives (SSDs)!</p>"},{"location":"5-Storage/Disks/#setup","title":"Setup","text":"<p>When creating your notebook server, you request disks by adding Data Volumes to your notebook server (pictured below, with <code>Type = New</code>). They are automatically mounted at the directory (<code>Mount Point</code>) you choose, and serve as a simple and reliable way to preserve data attached to a Notebook Server.</p> <p></p> You pay for all disks you own, whether they're attached to a Notebook Server or not <p>As soon as you create a disk, you're paying for it until it is deleted, even if it's original Notebook Server is deleted.  See Deleting Disk Storage for more info</p>"},{"location":"5-Storage/Disks/#once-youve-got-the-basics","title":"Once you've got the basics ...","text":"<p>When you delete your Notebook Server, your disks are not deleted. This let's you reuse that same disk (with all its contents) on a new Notebook Server later (as shown above with <code>Type = Existing</code> and the <code>Name</code> set to the volume you want to reuse). If you're done with the disk and it's contents, delete it.</p>"},{"location":"5-Storage/Disks/#deleting-disk-storage","title":"Deleting Disk Storage","text":"<p>To see your disks, check the Notebook Volumes section of the Notebook Server page (shown below). You can delete any unattached disk (orange icon on the left) by clicking the trash can icon.</p> <p></p>"},{"location":"5-Storage/Disks/#pricing","title":"Pricing","text":"Pricing models are tentative and may change <p>As of writing, pricing is covered by the platform for initial users.  This guidance explains how things are expected to be priced priced in future, but this may change.</p> <p>When mounting a disk, you get an Azure Managed Disk. The Premium SSD Managed Disks pricing shows the cost per disk based on size. Note that you pay for the size of disk requested, not the amount of space you are currently using.</p> Tips to minimize costs <p>As disks can be attached to a Notebook Server and reused, a typical usage pattern could be:</p> <ul> <li>At 9AM, create a Notebook Server (request 2CPU/8GB RAM and a 32GB attached   disk)</li> <li>Do work throughout the day, saving results to the attached disk</li> <li>At 5PM, shut down your Notebook Server to avoid paying for it overnight<ul> <li>NOTE: The attached disk is not destroyed by this action</li> </ul> </li> <li>At 9AM the next day, create a new Notebook Server and attach your existing   disk</li> <li>Continue your work...</li> </ul> <p>This keeps all your work safe without paying for the computer when you're not using it</p>"},{"location":"5-Storage/Overview/","title":"Storage","text":"<p>The platform provides several types of storage:</p> <ul> <li>Disk (also called Volumes on the Notebook Server creation screen)</li> <li>Containers (Azure Blob Storage)</li> <li>Data Lakes (coming soon)</li> </ul> <p>Depending on your use case, either disk or bucket may be most suitable:</p> Type Simultaneous Users Speed Total size Shareable with Other Users Disk One machine/notebook server at a time Fastest (throughput and latency) &lt;=512GB total per drive No Container (via Azure Blob Storage) Simultaneous access from many machines/notebook servers at the same time Fast-ish (Fast download, modest upload, modest latency) Infinite (within reason) [Yes] If you're unsure which to choose, don't sweat it <p>These are guidelines, not an exact science - pick what sounds best now and run with it.  The best choice for a complicated usage is non-obvious and often takes hands-on experience, so just trying something will help.  For most situations both options work well even if they're not perfect, and remember that data can always be copied later if you change your mind.</p>"},{"location":"6-Gitlab/Gitlab/","title":"Gitlab","text":""},{"location":"6-Gitlab/Gitlab/#important-notes","title":"IMPORTANT NOTES","text":"<p>1) Please do NOT store your token anywhere in your workspace server file system. Contributors to a namespace will have access to them. 2) If there is a contributor external to Statistics Canada in your namespace, you will lose access to cloud main GitLab access!</p> <p>Thankfully, using the cloud main GitLab on the AAW is just like how you would regularly use git. </p>"},{"location":"6-Gitlab/Gitlab/#step-1-locate-the-git-repo-you-want-to-clone-and-copy-the-clone-with-https-option","title":"Step 1: Locate the Git repo you want to clone and copy the clone with HTTPS option","text":"<p>If your repository is private, you will need to also do Step 4 (Creating a Personal Access Token) for this to go through.  For me this was a test repo  </p>"},{"location":"6-Gitlab/Gitlab/#step-2-paste-the-copied-link-into-one-of-your-workspace-servers","title":"Step 2: Paste the copied link into one of your workspace servers","text":""},{"location":"6-Gitlab/Gitlab/#step-3-success","title":"Step 3: Success!","text":"<p>As seen in the above screenshot I have cloned the repo!</p>"},{"location":"6-Gitlab/Gitlab/#step-4-create-a-personal-access-token-for-pushing-also-used-if-pulling-from-a-private-repository","title":"Step 4: Create a Personal Access Token for pushing (also used if pulling from a private repository)","text":"<p>If you try to <code>git push ....</code> you will encounter an error eventually leading you to the GitLab help documentation</p> <p>You will need to make a Personal Access Token for this. To achieve this go in GitLab, click your profile icon and then hit <code>Preferences</code> and then <code>Access Tokens</code>  Follow the prompts entering the name, the token expiration date and granting the token permissions (I granted <code>write_repository</code>)</p>"},{"location":"6-Gitlab/Gitlab/#step-5-personalize-git-to-be-you","title":"Step 5: Personalize <code>Git</code> to be you","text":"<p>Run <code>git config user.email ....</code> and <code>git config user.name ...</code> to match your GitLab identity.</p>"},{"location":"6-Gitlab/Gitlab/#step-6-supply-the-generated-token-when-asked-for-your-password","title":"Step 6: Supply the Generated Token when asked for your password","text":"<p>The token will by copy-able at the top once you hit <code>Create personal access token</code> at the bottom </p> <p>Once you have prepared everything it's time </p>"},{"location":"6-Gitlab/Gitlab/#step-7-see-the-results-of-your-hard-work-in-gitlab","title":"Step 7: See the results of your hard work in GitLab","text":""}]}