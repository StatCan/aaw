{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to the Advanced Analytics Workspace","text":""},{"location":"#what-are-you-looking-for","title":"What are you looking for?","text":"<ul> <li>Getting Started with AAW</li> <li>Data Analysis</li> <li>Data Science Experimentation</li> <li>Data Science Pipelines</li> <li>Statistical Publishing</li> <li>Cloud Storage</li> <li>Integration with External Platform as a Service (PaaS) Offerings</li> <li>Collaboration</li> </ul>"},{"location":"#the-advanced-analytics-workspace-documentation","title":"The Advanced Analytics Workspace Documentation","text":"<p>Welcome to the world of data science and machine learning!</p> <p>The Advanced Analytics Workspace is your one-stop-shop for all things data. It's like a secret passage to a treasure trove of knowledge, insights, and cutting-edge tools that'll take your data skills to the next level. With just a few clicks, you can unlock a world of possibility and connect to a community of like-minded data wizards. So get ready to don your cape and wizard hat, and join us on an exciting adventure through the Advanced Analytics Workspace portal!</p> <p>What is the AAW?</p> <p>Advanced Analytics Workspace (AAW) is an open source platform designed for data science and machine learning (ML) practitioners. Developed by data scientists for data scientists, AAW provides an unrestricted environment that enables advanced practitioners to get their work done with ease.</p> <p>Built on the Kubeflow project, the AAW is a comprehensive solution for deploying and managing end-to-end ML workflows. It simplifies the deployment of ML workflows on Kubernetes, making it simple, portable, and scalable. With the AAW, you can customize notebook server deployments to suit your specific data science needs. We have a small number of expertly crafted Docker images made by our team of data science experts.</p> <p>What is Kubeflow?</p> <p>The AAW is based on the Kubeflow project which is an open source comprehensive solution for deploying and managing end-to-end ML workflows. Kubeflow is designed to make deployments of ML workflows on Kubernetes simple, portable and scalable.</p> <p>Whether you're just getting started or already knee-deep in data analysis, the Advanced Analytics Workspace has everything you need to take your work to the next level. From powerful tools for data pipelines to cloud storage for your datasets, our platform has it all. Need to collaborate with colleagues or publish your results? No problem. We offer seamless collaboration features that make it easy to work together and share your work with others.</p> <p>No matter what stage of your data science journey you're at, the Advanced Analytics Workspace has the resources you need to succeed.</p>"},{"location":"#getting-started-with-the-aaw","title":"Getting Started with the AAW","text":""},{"location":"#the-aaw-portal","title":"The AAW Portal","text":"<p>The AAW portal homepage is available for internal users only. However, external users with a cloud account granted access by the business sponsor can access the platform through the analytics-platform URL.</p> <p>AAW Portal Homepage</p> <ul> <li>Portal Homepage for Statistics Canada Employees</li> <li>Portal Homepage for External Users</li> </ul> <p>Kubeflow Dashboard</p> <ul> <li>Kubeflow Dashboard Use this link once you have your cloud account!</li> </ul> <p>Getting started with the Advanced Analytics Workspace (AAW) is easy and quick. First, you'll want to set up Kubeflow for MLOps and Jupyter Notebooks. Kubeflow makes it easy to deploy and manage end-to-end machine learning workflows, while Jupyter Notebooks provide a flexible and powerful environment for data analysis and experimentation. Once you have Kubeflow and Jupyter Notebooks set up, you can start exploring the many resources and tools available through the AAW portal. Additionally, we encourage you to join our Slack channel to connect with other data scientists and analysts, ask questions, and share your experiences with the AAW platform.</p>"},{"location":"#kubeflow-account","title":"Kubeflow Account\ud83d\udc49 Click here to setup your Kubeflow account! \ud83d\udc48","text":"<p>Attention External Users!</p> <p>Users external to Statistics Canada will require a cloud account granted access by the business sponsor.</p> <p> </p> <p>Kubeflow is a powerful and flexible open source platform that has revolutionized the way we build, deploy, and manage machine learning workflows. With its robust set of tools and frameworks, Kubeflow is the perfect starting point for data scientists and developers who want to accelerate their projects and bring them to the next level. By using up Kubeflow, you can harness the full potential of cloud-native machine learning and unlock new opportunities for innovation and growth.</p> <p>Attention Statistics Canada Employees!</p> <p>Users internal to Statistics Canada can get started right away without any additional sign up procedures, just head to  https://kubeflow.aaw.cloud.statcan.ca/.</p> <p>Kubeflow offers an unparalleled level of flexibility and customization, empowering you to create and deploy machine learning workflows that meet your specific needs and requirements. By embracing Kubeflow, you'll be at the forefront of the machine learning revolution, and who knows what incredible insights and breakthroughs you'll discover along the way.</p>"},{"location":"#slack","title":"SlackUse your @statcan.gc.ca email address so that you will be automatically approved.","text":"<ul> <li>Click here sign in to our Slack Support Workspace</li> </ul> <ul> <li>Use the General Channel!</li> </ul> <p>At StatCan, we understand that embarking on a new project can be overwhelming, and you're likely to have many questions along the way. That's why we've created a dedicated  Slack channel to provide you with the support you need. Our team of experts is standing by to answer your questions, address any concerns, and guide you through every step of the process.</p> <p>To join our  Slack channel, simply click on the link provided and follow the instructions. You'll be prompted to create an account in the upper right-hand corner of the page. If you have an <code>@statcan.gc.ca</code> email address, use it when signing up as this will ensure that you are automatically approved and can start engaging with our community right away.</p> <p>Once you've created your account, you'll have access to a wealth of resources and information, as well as the opportunity to connect with other users who are working on similar projects. Our  Slack channel is the perfect place to ask questions, share insights, and collaborate with your peers in real-time. Whether you're just getting started with a new project or you're looking for expert advice on a complex issue, our team is here to help.</p> <p>So don't hesitate - join our Slack channel today and start getting the answers you need to succeed. We look forward to welcoming you to our community!</p> <p>Click on the link, then choose \"Create an account\" in the upper right-hand corner.</p> <p> </p>"},{"location":"#getting-started","title":"\ud83e\udded Getting Started","text":"<p>To access AAW services, you need to log in to Kubeflow with your StatCan guest cloud account. Once logged in, select Notebook Servers and click the \"New Server\" button to get started.</p> <ol> <li>Login to Kubeflow with your StatCan guest cloud account. You will be prompted to authenticate the account.</li> <li>Select Notebook Servers.</li> <li>Click the \"\u2795 New Server\" button.</li> </ol>"},{"location":"#tools-offered","title":"\ud83e\uddf0 Tools Offered","text":"<p>AAW is a flexible platform for data analysis and machine learning. It offers a range of languages, including Python, R, and Julia. AAW also supports development environments such as VS Code, R Studio, and Jupyter Notebooks. Additionally, Linux virtual desktops are available for users who require additional tools such as OpenM++ and QGIS.</p> <p>Here's a list of tools we offer:</p> <ul> <li>\ud83d\udcdc Languages:<ul> <li>\ud83d\udc0d Python</li> <li>\ud83d\udcc8 R</li> <li>\ud83d\udc69\u200d\ud83d\udd2c Julia</li> </ul> </li> <li>\ud83e\uddee Development environments:<ul> <li>VS Code</li> <li>R Studio</li> <li>Jupyter Notebooks</li> </ul> </li> <li>\ud83d\udc27 Linux virtual desktops for additional tools (\ud83e\uddeb OpenM++, \ud83c\udf0f QGIS etc.)</li> </ul> <p>Sharing code, disks, and workspaces (e.g.: two people sharing the same virtual machine) is described in more detail in the Collaboration section. Sharing data through buckets is described in more detail in the Azure Blob Storage section.</p>"},{"location":"#help","title":"\ud83d\udca1 Help","text":"<ul> <li>Disk (also called Volumes on the Notebook Server creation screen)</li> <li>Containers (Blob Storage)</li> <li>Data Lakes (coming soon)</li> </ul> <ul> <li>\ud83d\udcd7 AAW Portal Documentation<ul> <li>https://statcan.github.io/daaas/</li> </ul> </li> <li>\ud83d\udcd8 Kubeflow Documentation<ul> <li>https://www.kubeflow.org/docs/ </li> </ul> </li> <li>\ud83e\udd1d Slack Support Channel<ul> <li>https://statcan-aaw.slack.com</li> </ul> </li> <li>Github Issues<ul> <li>https://github.com/StatCan/daaas/issues</li> </ul> </li> </ul>"},{"location":"#demos","title":"\ud83d\udc31 Demos","text":"<p>If you require a quick onboarding demo session, need help, or have any questions, please reach out to us through our \ud83e\udd1d Slack Support Channel.</p> <p>Thank you for choosing Advanced Analytics Workspace!</p>"},{"location":"Help/","title":"Have questions? Or feedback?","text":"<p>Come join us on the Advanced Analytics Workspace Slack channel! You will find a bunch of other users of the platform there who may be able to answer your questions, and some of the engineers will usually be present on the channel. You can ask questions and provide feedback there.</p> <ul> <li>Slack (en)</li> </ul> <p>We will also post notices there if there are updates or downtime.</p>"},{"location":"Help/#video-tutorials","title":"Video tutorials","text":"<p>After you have joined our Slack community, go and check out the following tutorials:</p> <ul> <li>Platform official</li> <li>Community driven content</li> </ul>"},{"location":"Help/#github","title":"GitHub","text":"<p>Want to know even more about our platform? Find everything about it on our GitHub page.</p> <ul> <li>Advanced Analytics Workspace on GitHub</li> </ul>"},{"location":"welcome-message/","title":"Welcome message","text":""},{"location":"welcome-message/#welcome-to-advanced-analytics-workspace-aaw","title":"\ud83e\uddd9\ud83d\udd2e Welcome to Advanced Analytics Workspace (AAW)","text":"<p>Please find below additional information, videos and links to help better understand how to get started with Advanced Analytics Workspace (AAW). </p> <p>Advanced Analytics Workspace (AAW) is our open source platform for data science and machine learning (ML) for advanced practitioners to get their work done in an unrestricted environment made by data scientists for data scientists. With AAW, you can customize your notebook deployments to suit your data science needs. We also have a small number of expertly crafted images made by our expert data science team.</p> <p>AAW is based on the Kubeflow project which is an open source comprehensive solution for deploying and managing end-to-end ML workflows. Kubeflow is designed to make deployments of ML workflows on Kubernetes simple, portable and scalable.</p> <p>\ud83d\udd14 Important! Users external to Statistics Canada will require a cloud account granted access by the business sponsor.</p> <p>\ud83d\udd14 Important! Users internal to Statistics Canada can get started right away without any additional sign up procedures, just head to  https://kubeflow.aaw.cloud.statcan.ca/.</p>"},{"location":"welcome-message/#helpful-links","title":"\ud83d\udd17 Helpful Links","text":""},{"location":"welcome-message/#aaw-services","title":"\ud83d\udece\ufe0f AAW Services","text":"<ul> <li>\ud83c\udf00 AAW Portal Homepage<ul> <li>Internal Only https://www.statcan.gc.ca/data-analytics-service/aaw</li> <li>Internal/External https://analytics-platform.statcan.gc.ca/covid19</li> </ul> </li> </ul> <ul> <li>\ud83e\udd16 Kubeflow Dashboard<ul> <li>https://kubeflow.aaw.cloud.statcan.ca/ </li> </ul> </li> </ul>"},{"location":"welcome-message/#help","title":"\ud83d\udca1 Help","text":"<ul> <li>\ud83d\udcd7 AAW Portal Documentation<ul> <li>https://statcan.github.io/daaas/</li> </ul> </li> <li>\ud83d\udcd8 Kubeflow Documentation<ul> <li>https://www.kubeflow.org/docs/ </li> </ul> </li> <li>\ud83e\udd1d Slack Support Channel<ul> <li>https://statcan-aaw.slack.com</li> </ul> </li> </ul>"},{"location":"welcome-message/#getting-started","title":"\ud83e\udded Getting Started","text":"<p>In order to access the AAW services, you will need to:</p> <ol> <li>Login to Kubeflow with your StatCan guest cloud account. You will be prompted to authenticate the account.</li> <li>Select Notebook Servers.</li> <li>Click the \"\u2795 New Server\" button.</li> </ol>"},{"location":"welcome-message/#tools-offered","title":"\ud83e\uddf0 Tools Offered","text":"<p>AAW is a flexible platform for data analysis and machine learning, featuring:</p> <p>- \ud83d\udcdc Languages     - \ud83d\udc0d Python     - \ud83d\udcc8 R     - \ud83d\udc69\u200d\ud83d\udd2c Julia   - \ud83e\uddee Development environments     - VS Code     - R Studio     - Jupyter Notebooks   - \ud83d\udc27 Linux virtual desktops for additional tools (\ud83e\uddeb OpenM++, \ud83c\udf0f QGIS etc.)</p>"},{"location":"welcome-message/#demos","title":"\ud83d\udc31 Demos","text":"<p>If you would like a quick Onboarding Demo session or require any help or have any questions, please do not hesitate to reach out through our \ud83e\udd1d Slack Support Channel.</p>"},{"location":"welcome-message/#faq","title":"FAQ","text":"<ul> <li>Frequently Asked Questions are located here.</li> </ul> <p>Thank you!</p>"},{"location":"welcome/","title":"Welcome","text":""},{"location":"welcome/#welcome-to-advanced-analytics-workspace-aaw","title":"\ud83e\uddd9\ud83d\udd2e Welcome to Advanced Analytics Workspace (AAW)","text":"<p>\"What we want to do is make a leapfrog product that is way smarter than any mobile device has ever been, and super-easy to use. This is what iPhone is. OK? So, we're going to reinvent the phone.\" -- Steve Jobs</p> <p>Please find below additional information, videos and links to help better understand how to get started with Advanced Analytics Workspace (AAW).</p> <p>Advanced Analytics Workspace (AAW) is an open source platform designed for data science and machine learning (ML) practitioners. Developed by data scientists for data scientists, AAW provides an unrestricted environment that enables advanced practitioners to get their work done with ease.</p> <p>Built on the Kubeflow project, AAW is a comprehensive solution for deploying and managing end-to-end ML workflows. It simplifies the deployment of ML workflows on Kubernetes, making it simple, portable, and scalable. With AAW, you can customize your notebook deployments to suit your specific data science needs. Additionally, we have a small number of expertly crafted images made by our team of data science experts.</p> <p>Advanced Analytics Workspace (AAW) is our open source platform for data science and machine learning (ML) for advanced practitioners to get their work done in an unrestricted environment made by data scientists for data scientists. With AAW, you can customize your notebook deployments to suit your data science needs. We also have a small number of expertly crafted images made by our expert data science team.</p> <p>AAW is based on the Kubeflow project which is an open source comprehensive solution for deploying and managing end-to-end ML workflows. Kubeflow is designed to make deployments of ML workflows on Kubernetes simple, portable and scalable.</p> <p>\ud83d\udd14 Important! Users external to Statistics Canada will require a cloud account granted access by the business sponsor.</p> <p>\ud83d\udd14 Important! Users internal to Statistics Canada can get started right away without any additional sign up procedures, just head to  https://kubeflow.aaw.cloud.statcan.ca/.</p>"},{"location":"welcome/#helpful-links","title":"\ud83d\udd17 Helpful Links","text":""},{"location":"welcome/#aaw-services","title":"\ud83d\udece\ufe0f AAW Services","text":"<p>The AAW portal homepage is available for internal users only. However, external users with a cloud account granted access by the business sponsor can access the platform through the analytics-platform URL.</p> <ul> <li>\ud83c\udf00 AAW Portal Homepage<ul> <li>Internal Only https://www.statcan.gc.ca/data-analytics-service/aaw</li> <li>Internal/External https://analytics-platform.statcan.gc.ca/covid19</li> </ul> </li> </ul> <ul> <li>\ud83e\udd16 Kubeflow Dashboard<ul> <li>https://kubeflow.aaw.cloud.statcan.ca/ </li> </ul> </li> </ul>"},{"location":"welcome/#help","title":"\ud83d\udca1 Help","text":"<p>The AAW Portal Documentation and Kubeflow Documentation provide helpful resources to get started with AAW. If you need further assistance, our Slack Support Channel is available for support.</p> <ul> <li>\ud83d\udcd7 AAW Portal Documentation<ul> <li>https://statcan.github.io/daaas/</li> </ul> </li> <li>\ud83d\udcd8 Kubeflow Documentation<ul> <li>https://www.kubeflow.org/docs/ </li> </ul> </li> <li>\ud83e\udd1d Slack Support Channel<ul> <li>https://statcan-aaw.slack.com</li> </ul> </li> </ul>"},{"location":"welcome/#getting-started","title":"\ud83e\udded Getting Started","text":"<p>To access AAW services, you need to log in to Kubeflow with your StatCan guest cloud account. Once logged in, select Notebook Servers and click the \"New Server\" button to get started.</p> <ol> <li>Login to Kubeflow with your StatCan guest cloud account. You will be prompted to authenticate the account.</li> <li>Select Notebook Servers.</li> <li>Click the \"\u2795 New Server\" button.</li> </ol>"},{"location":"welcome/#tools-offered","title":"\ud83e\uddf0 Tools Offered","text":"<p>AAW is a flexible platform for data analysis and machine learning. It offers a range of languages, including Python, R, and Julia. AAW also supports development environments such as VS Code, R Studio, and Jupyter Notebooks. Additionally, Linux virtual desktops are available for users who require additional tools such as OpenM++ and QGIS.</p> <p>Here's a list of tools we offer:</p> <p>- \ud83d\udcdc Languages:     - \ud83d\udc0d Python     - \ud83d\udcc8 R     - \ud83d\udc69\u200d\ud83d\udd2c Julia   - \ud83e\uddee Development environments:     - VS Code     - R Studio     - Jupyter Notebooks   - \ud83d\udc27 Linux virtual desktops for additional tools (\ud83e\uddeb OpenM++, \ud83c\udf0f QGIS etc.)</p>"},{"location":"welcome/#demos","title":"\ud83d\udc31 Demos","text":"<p>If you require a quick onboarding demo session, need help, or have any questions, please reach out to us through our \ud83e\udd1d Slack Support Channel.</p>"},{"location":"welcome/#frequently-asked-questions","title":"Frequently Asked Questions","text":"<p>For frequently asked questions, please refer to the FAQ section in our Github repository, located here.</p> <p>Thank you for choosing Advanced Analytics Workspace!</p>"},{"location":"1-Experiments/Jupyter/","title":"Overview","text":""},{"location":"1-Experiments/Jupyter/#jupyter-friendly-r-and-python-experience","title":"Jupyter: friendly R and Python experience","text":"<p>Jupyter gives you notebooks to write your code and make visualizations. You can quickly iterate, visualize, and share your analyses. Because it's running on a server (that you set up in the Kubeflow section) you can do really big analyses on centralized hardware, adding as much horsepower as you need! And because it's on the cloud, you can share it with your colleagues too.</p>"},{"location":"1-Experiments/Jupyter/#explore-your-data","title":"Explore your data","text":"<p>Jupyter comes with a number of features (and we can add more)</p> <ul> <li>Integrated visuals within your notebook</li> <li>Data volume for storing your data</li> <li>You can share your workspace with colleagues.</li> </ul> <p></p>"},{"location":"1-Experiments/Jupyter/#explore-your-data-with-an-api","title":"Explore Your Data with an API","text":"<p>Use Datasette , an instant JSON API for your SQLite databases. Run SQL queries in a more interactive way!</p>"},{"location":"1-Experiments/Jupyter/#ide-in-the-browser","title":"IDE in the browser","text":"<p>Create for exploring, and also great for writing code</p> <ul> <li>Linting and a debugger</li> <li>Git integration</li> <li>Built in Terminal</li> <li>Light/Dark theme (change settings at the top)</li> </ul> <p></p> <p>More information on Jupyter here</p>"},{"location":"1-Experiments/Jupyter/#setup","title":"Setup","text":""},{"location":"1-Experiments/Jupyter/#get-started-with-the-examples","title":"Get started with the examples","text":"<p>When you started your server, it got loaded with a bunch of example notebooks. Double click to open the jupyter-notebooks folder. Great notebooks to start with are <code>R/01-R-Notebook-Demo.ipynb</code>, or the notebooks in <code>scikitlearn</code>. <code>pytorch</code> and <code>tensorflow</code> are great if you are familiar with machine learning. The <code>mapreduce-pipeline</code> and <code>ai-pipeline</code> are more advanced.</p> Some notebooks only work in certain server versions <p>For instance, <code>gdal</code> is only in the geomatics image. So if you use another image then a notebook using <code>gdal</code> might not work.</p>"},{"location":"1-Experiments/Jupyter/#adding-software","title":"Adding software","text":"<p>You do not have <code>sudo</code> in Jupyter, but you can use</p> <pre><code>conda install --use-local your_package_name\n</code></pre> <p>or</p> <pre><code>pip install --user your_package_name\n</code></pre> <p>Don't forget to restart your Jupyter kernel afterwards, to make new packages available.</p> Make sure to restart the Jupyter kernel after installing new software <p>If you install software in a terminal, but your Jupyter kernel was already running, then it will not be updated.</p> Is there something that you can't install? <p>If you need something installed, reach us or open a GitHub issue. We can add it to the default software.</p>"},{"location":"1-Experiments/Jupyter/#once-youve-got-the-basics","title":"Once you've got the basics ...","text":""},{"location":"1-Experiments/Jupyter/#getting-data-in-and-out-of-jupyter","title":"Getting Data in and out of Jupyter","text":"<p>You can upload and download data to/from JupyterHub directly in the menu. There is an upload button at the top, and you can right-click most files or folders to download them.</p>"},{"location":"1-Experiments/Jupyter/#shareable-bucket-storage","title":"Shareable \"Bucket\" storage","text":"<p>There is also a mounted <code>minio</code> folder in your home directory, which holds files in MinIO.</p> <p>Refer to the Storage section for details.</p>"},{"location":"1-Experiments/Jupyter/#data-analysis","title":"Data Analysis","text":"<p>Data analysis is an underappreciated art.</p> <p>Data analysis is the process of examining and interpreting large amounts of data to extract useful insights and draw meaningful conclusions. This can be done using various techniques and tools, such as statistical analysis, machine learning, and visualization. The goal of data analysis is to uncover patterns, trends, and relationships in the data, which can then be used to inform decisions and solve problems. Data analysis is used in a wide range of fields, from business and finance to healthcare and science, to help organizations make more informed decisions based on evidence and data-driven insights.</p>"},{"location":"1-Experiments/Jupyter/#jupyterlab","title":"JupyterLab","text":"<p>Process data using R, Python, or Julia in JupyterLab</p> <p> </p> <p>Processing data using R, Python, or Julia is made easy with the Advanced Analytics Workspace. Whether you're new to data analysis or an experienced data scientist, our platform supports a range of programming languages to fit your needs. You can install and run packages for R or Python to perform data processing tasks such as data cleaning, transformation, and modeling. If you prefer Julia, our platform also offers support for this programming language.</p>"},{"location":"1-Experiments/Kubeflow/","title":"Overview","text":""},{"location":"1-Experiments/Kubeflow/#what-does-kubeflow-do","title":"What does Kubeflow do?","text":"<p>Kubeflow runs your workspaces. You can have notebook servers (called Jupyter Servers), and in them you can create analyses in R and Python with interactive visuals. You can save and upload data, download it, and create shared workspaces for your team.</p> <p></p> <p>Let's get started!</p>"},{"location":"1-Experiments/Kubeflow/#video-tutorial","title":"Video Tutorial","text":"<p>This video is not up to date, some things have changed since.</p> <p></p>"},{"location":"1-Experiments/Kubeflow/#setup","title":"Setup","text":""},{"location":"1-Experiments/Kubeflow/#log-into-kubeflow","title":"Log into Kubeflow","text":"Log into the Azure Portal using your Cloud Credentials <p>You have to login to the Azure Portal using your StatCan cloud credentials. <code>first.lastname@cloud.statcan.ca</code> or StatCan credentials <code>first.lastname@statcan.gc.ca</code>. You can do that using the Azure Portal. </p> <ul> <li>Log into Kubeflow</li> </ul> <ul> <li>Navigate to the Notebook Servers tab</li> </ul> <p></p> <ul> <li>Then click + New Server</li> </ul>"},{"location":"1-Experiments/Kubeflow/#server-name-and-namespace","title":"Server Name and Namespace","text":"<ul> <li>You will get a template to create your notebook server. Note: the name of   your server can consist of only lower-case letters, numbers, and hyphens. No spaces, and no   underscores.</li> </ul> <ul> <li>You will need to specify a namespace. By default you will have a default   namespace for your account, but for projects you may need to select the   namespace created specifically for that project. Otherwise the notebook server   you create may not have access rights to resources required for the project.</li> </ul>"},{"location":"1-Experiments/Kubeflow/#image","title":"Image","text":"<p>You will need to choose an image. There are JupyterLab, RStudio, Ubuntu remote desktop, and SAS images available. The SAS image is only available for StatCan employees (due to license limitations), the others are available for everyone. Select the drop down menu to select additional options within these (for instance, CPU, PyTorch, and TensorFlow images for JupyterLab).</p> <p>Check the name of the images and choose one that matches what you want to do. Don't know which one to choose? Check out your options here.</p> <p></p>"},{"location":"1-Experiments/Kubeflow/#cpu-and-memory","title":"CPU and Memory","text":"<p>At the time of writing (December 23, 2021) there are two types of computers in the cluster</p> <ul> <li>CPU: <code>D16s v3</code> (16 CPU cores, 64 GiB memory; for user use 15 CPU cores    and 48 GiB memory are available; 1 CPU core and 16 GiB memory reserved for    system use).</li> <li>GPU: <code>NC6s_v3</code> (6 CPU cores, 112 GiB memory, 1 GPU; for user use 96 GiB    memory are available; 16 GiB memory reserved for system use). The available    GPU is the NVIDIA Tesla V100 GPU with specifications    here.</li> </ul> <p>When creating a notebook server, the system will limit you to the maximum specifications above. For CPU notebook servers, you can specify the exact amount of CPU and memory that you require. This allows you to meet your compute needs while minimising cost. For a GPU notebook server, you will always get the full server (6 CPU cores, 96 GiB accessible memory, and 1 GPU). See below section on GPUs for information on how to select a GPU server.</p> <p>In the advanced options, you can select a higher limit than the number of CPU cores and RAM requested. The amount requested is the amount guaranteed to be available for your notebook server and you will always pay for at least this much. If the limit is higher than the amount requested, if additional RAM and CPU cores are available on that shared server in the cluster your notebook server can use them as needed. One use case for this is jobs that usually need only one CPU core but can benefit from multithreading to speed up certain operations. By requesting one CPU core but a higher limit, you can pay much less for the notebook server while allowing it to use spare unused CPU cores as needed to speed up computations.</p> <p></p>"},{"location":"1-Experiments/Kubeflow/#gpus","title":"GPUs","text":"<p>If you want a GPU server, select <code>1</code> as the number of GPUs and <code>NVIDIA</code> as the GPU vendor (the create button will be greyed out until the GPU vendor is selected if you have a GPU specified). Multi-GPU servers are currently supported on the AAW system only on a special on-request basis, please contact the AAW maintainers if you would like a multi-GPU server.</p> <p></p> <p>As mentioned before, if you select a GPU server you will automatically get 6 CPU cores and 112 GiB of memory.</p> <p>Use GPU machines responsibly</p> <p>GPU machines are significantly more expensive than CPU machines, so use them responsibly.</p>"},{"location":"1-Experiments/Kubeflow/#workspace-volume","title":"Workspace Volume","text":"<p>You will need a workspace volume, which is where the home folder will be mounted. There are various configuration options available:</p> <ul> <li>You can either reuse an existing workspace volume from before, or create a new one.</li> </ul> <ul> <li>You can specify the size of the workspace volume, from 4 GiB to 32 GiB.</li> </ul> <p></p> <p>Check for old volumes by looking at the Existing option</p> <p>When you create your server you have the option of reusing an old volume or creating a new one. You probably want to reuse your old volume.</p>"},{"location":"1-Experiments/Kubeflow/#data-volumes","title":"Data Volumes","text":"<p>You can also create data volumes that can be used to store additional data. Multiple data volumes can be created. Click the add new volume button to create a new volume and specify its configuration. Click the attach existing volume button to mount an existing data volume to the notebook server. There are the following configuration parameters for data volumes:</p> <ul> <li>Name: Name of the volume.</li> </ul> <ul> <li>Size in GiB: From 4 GiB to 512 GiB.</li> </ul> <ul> <li>Mount path: Path where the data volume can be accessed on the notebook server, by   default <code>/home/jovyan/vol-1</code>, <code>/home/jovyan/vol-2</code>, etc. (incrementing counter per data   volume mounted).</li> </ul> <p>When mounting an existing data volume, the name option becomes a drop-down list of the existing data volumes. Only a volume not currently mounted to an existing notebook server can be used. The mount path option remains user-configurable with the same defaults as creating a new volume.</p> <p>The garbage can icon on the right can be used to delete an existing or accidentally created data volume.</p> <p></p>"},{"location":"1-Experiments/Kubeflow/#configurations","title":"Configurations","text":"<p>There are currently three checkbox options available here:</p> <ul> <li>Mount MinIO storage to ~/minio (experimental): This should make MinIO   repositories accessible as subfolders / files of the <code>minio/</code> folder. This is   still experimental and may not work properly currently.</li> <li>Run a Protected B notebook: Enable this if the server you create needs   access to any Protected B resources. Protected B notebook servers run with many   security restrictions and have access to separate MinIO instances specifically   designed for Protected B data.</li> </ul>"},{"location":"1-Experiments/Kubeflow/#miscellaneous-settings","title":"Miscellaneous Settings","text":"<p>The following can be customized here:</p> <ul> <li>Enable Shared Memory: This is required if you use PyTorch with multiple data   loaders, which otherwise will generate an error. If using PyTorch make sure this   is enabled, otherwise it does not matter unless you have another application   that requires shared memory.</li> <li>System Language: Can specify English or French here.</li> </ul> <p></p>"},{"location":"1-Experiments/Kubeflow/#and-create","title":"And... Create!!!","text":"<ul> <li>If you're satisfied with the settings, you can now create the server! It may   take a few minutes to spin up depending on the resources you asked for. GPUs   take longer.</li> </ul> <p>Your server is running</p> <p>If all goes well, your server should be running!!! You will now have the option to connect, and try out Jupyter!</p>"},{"location":"1-Experiments/Kubeflow/#once-youve-got-the-basics","title":"Once you've got the basics ...","text":""},{"location":"1-Experiments/Kubeflow/#share-your-workspace","title":"Share your workspace","text":"<p>In Kubeflow every user has a namespace that contains their work (their notebook servers, pipelines, disks, etc.). Your namespace belongs to you, but can be shared if you want to collaborate with others. For more details on collaboration on the platform, see Collaboration.</p>"},{"location":"1-Experiments/MLflow/","title":"Overview","text":"<p>!!! danger \"MLflow has been removed from the AAW.     If you need it, contact the development team\"</p> <p>MLflow is an open source platform for managing the Machine Learning lifecycle. It is a \"Model Registry\" for storing your machine learning models and associated metrics. You can use the web interface to examine your models, and you can use its REST API to register your models from Python, using the mlflow pip package.</p> <p></p>"},{"location":"1-Experiments/Overview/","title":"Overview","text":""},{"location":"1-Experiments/Overview/#data-science-experimentation","title":"Data Science Experimentation","text":"<p>Process data using R, Python, or Julia with Kubeflow, a machine learning platform that provides a simple, unified, and scalable infrastructure for machine learning workloads.</p> <p>With Kubeflow, you can process data in a scalable and efficient way using the programming language of your choice. Once you have Kubeflow set up, use Jupyter Notebooks to create and share documents that contain live code, equations, or visualizations.</p> <p>You can also run Ubuntu as a virtual desktop with Kubeflow, giving you access to a powerful development environment that can be customized to your needs. With R Shiny, a web application framework for R, you can easily create and publish static and interactive dashboards to communicate your analysis results to stakeholders.</p> <p>Kubeflow also provides integration with external platforms as a service, such as Google Cloud Platform (GCP) and Amazon Web Services (AWS), allowing you to easily move data and workloads between different cloud services. Additionally, with Kubeflow's collaboration features, you can work on your projects with your team in real-time, sharing your analysis, code, and results seamlessly.</p> <p>Data science experimentation refers to the process of designing, conducting, and analyzing experiments in order to test hypotheses and gain insights from data. This process typically involves several steps:</p> <ol> <li> <p>Formulating a hypothesis: Before conducting an experiment, it is important to have a clear idea of what you are trying to test or learn. This may involve formulating a hypothesis about a relationship between variables, or trying to answer a specific research question.</p> </li> <li> <p>Designing the experiment: Once you have a hypothesis, you need to design an experiment that will allow you to test it. This may involve selecting a sample of data, choosing variables to manipulate or measure, and deciding on the experimental conditions.</p> </li> <li> <p>Collecting and cleaning the data: With the experiment designed, you need to collect the data necessary to test your hypothesis. This may involve gathering data from existing sources or conducting your own experiments. Once the data is collected, you need to clean it to remove any errors or anomalies.</p> </li> <li> <p>Analyzing the data: Once the data is clean, you can begin to analyze it to test your hypothesis. This may involve running statistical tests or machine learning algorithms, visualizing the data to identify patterns or trends, or using other analytical techniques to gain insights.</p> </li> <li> <p>Drawing conclusions: Based on the results of your analysis, you can draw conclusions about whether your hypothesis is supported or not. You may also be able to identify areas for further research or experimentation.</p> </li> </ol> <p>Data analysis is a key component of data science experimentation, and involves using various techniques and tools to make sense of large amounts of data. This may involve exploratory data analysis, where you use visualizations and summary statistics to gain an initial understanding of the data, or more advanced techniques such as machine learning or statistical modeling. Data analysis can be used to answer a wide range of questions, from simple descriptive questions about the data to more complex predictive or prescriptive questions.</p> <p>In summary, data science experimentation and data analysis are important components of the broader field of data science, and involve using data to test hypotheses, gain insights, and make informed decisions.</p>"},{"location":"1-Experiments/RStudio/","title":"Overview","text":"<p>RStudio is an integrated development environment (IDE) for R. It includes a console, editor, and tools for plotting, history, debugging and workspace management.</p>"},{"location":"1-Experiments/RStudio/#video-tutorial","title":"Video Tutorial","text":""},{"location":"1-Experiments/RStudio/#setup","title":"Setup","text":"<p>You can use the <code>rstudio</code> image to get an RStudio environment! When you create your notebook, choose RStudio from the list of available images. </p> <p></p> <p>You can install <code>R</code> or <code>python</code> packages with <code>conda</code> or <code>install.packages()</code>.</p>"},{"location":"1-Experiments/RStudio/#once-youve-got-the-basics","title":"Once you've got the basics ...","text":""},{"location":"1-Experiments/RStudio/#r-shiny","title":"R-Shiny","text":"<p>You can use <code>Shiny</code>, too! Shiny is an open source R package that provides a web framework for building web applications using R. Shiny helps you turn your analyses into interactive web applications.</p> <p></p>"},{"location":"1-Experiments/RStudio/#r-studio","title":"R Studio","text":"<p>Process data using R or Python in R Studio</p> <p> </p> <p>R Studio is a powerful integrated development environment (IDE) that supports both the R and Python programming languages, making it an ideal choice for data analysts and scientists. With R Studio, you can perform a wide range of data processing tasks, from data cleaning and transformation to statistical analysis and machine learning. The software provides a user-friendly interface and a variety of tools and libraries that simplify complex data analysis tasks. In addition, R Studio makes it easy to share your work with others by creating dynamic, interactive reports and visualizations that can be published online or shared as standalone documents. Overall, R Studio is a versatile and powerful tool that is essential for anyone working with data in R or Python.</p> <p>R Studio gives you an integrated development environment for R and Python. Use the r-studio-cpu image to get an R Studio environment.</p>"},{"location":"1-Experiments/Remote-Desktop/","title":"Overview","text":""},{"location":"1-Experiments/Remote-Desktop/#ubuntu-virtual-desktop","title":"Ubuntu Virtual Desktop","text":"<p>You can run a full Ubuntu Desktop, with typical applications, right inside your browser, using Kubeflow!</p> <p> </p> <p>The Ubuntu Virtual Desktop is a powerful tool for data scientists and machine learning engineers who need to run resource-intensive workloads in the cloud. Ubuntu is a popular Linux distribution that is widely used in the data science and machine learning communities due to its strong support for open source tools such as R and Python. With the Ubuntu Virtual Desktop, you can quickly spin up a virtual machine with Ubuntu pre-installed and access it from anywhere with an internet connection. This means you can perform data analysis and machine learning tasks from your laptop, tablet, or phone without having to worry about hardware limitations.</p>"},{"location":"1-Experiments/Remote-Desktop/#what-is-remote-desktop","title":"What is Remote Desktop?","text":"<p>Remote Desktop provides an in-browser GUI Ubuntu desktop experience as well as quick access to supporting tools. The operating system is Ubuntu 18.04 with the XFCE desktop environment.</p> <p></p>"},{"location":"1-Experiments/Remote-Desktop/#geomatics","title":"Geomatics","text":"<p>Our version of Remote Desktop is built on an R Geospatial image.</p>"},{"location":"1-Experiments/Remote-Desktop/#customization","title":"Customization","text":"<p>pip, conda, npm and yarn are available to install various packages.</p>"},{"location":"1-Experiments/Remote-Desktop/#setup","title":"Setup","text":""},{"location":"1-Experiments/Remote-Desktop/#accessing-the-remote-desktop","title":"Accessing the Remote Desktop","text":"<p>To launch the Remote Desktop or any of its supporting tools, create a Notebook Server in Kubeflow and select the remote desktop option.</p> <p></p> <p>Once it has been created, click <code>Connect</code> to be redirected to the Remote Desktop.</p> <p>Remote Desktop brings you to the Desktop GUI through a noVNC session. Click on the &gt; on the left side of the screen to expand a panel with options such as fullscreen and clipboard access.</p> <p></p>"},{"location":"1-Experiments/Remote-Desktop/#accessing-the-clipboard","title":"Accessing the Clipboard","text":"<p>This is done via the second button from the top of the panel on the left. It brings up a text box which we can modify to change the contents of the clipboard or copy stuff from the clipboard of the remote desktop.</p> <p>For example, suppose we want to execute the command <code>head -c 20 /dev/urandom | md5sum</code> and copy-paste the result into a text file on our computer used to connect to the remote desktop.</p> <p>We first open the clipboard from the panel on the left and paste in that command into the text box:</p> <p></p> <p>To close the clipboard window over the remote desktop, simply click the clipboard button again.</p> <p>We then right click on a terminal window to paste in that command and press enter to execute the command. At that point we select the MD5 result, right click, and click copy:</p> <p></p> <p>If we open the clipboard from the panel on the left again, it will now have the new contents:</p> <p></p> <p>The clipboard window will even update in-place if we leave it open the whole time and we simply select new material on the remote desktop and press copy again. We can simply copy what we have in that text box and paste it into any other software running on the computer used to connect.</p>"},{"location":"1-Experiments/Remote-Desktop/#in-browser-tools","title":"In-browser Tools","text":""},{"location":"1-Experiments/Remote-Desktop/#vs-code","title":"VS Code","text":"<p>Visual Studio Code is a lightweight but powerful source code editor. It comes with built-in support for JavaScript, TypeScript and Node.js and has a rich ecosystem of extensions for several languages (such as C++, C#, Java, Python, PHP, Go).</p> <p></p>"},{"location":"1-Experiments/Remote-Desktop/#footnotes","title":"Footnotes","text":"<p>Remote Desktop is based on ml-tooling/ml-workspace.</p>"},{"location":"1-Experiments/Selecting-an-Image/","title":"Selecting an Image for your Notebook Server","text":"<p>Depending on your project or use case of the Notebook Server, some images may be more suitable than others. The following will go through the main features of each to help you pick the most appropriate image for you.</p> <p>When selecting an image, you have 3 main options:</p> <ul> <li>Jupyter Notebook (CPU, TensorFlow, PyTorch)</li> <li>RStudio</li> <li>Remote Desktop (r, geomatics)</li> </ul>"},{"location":"1-Experiments/Selecting-an-Image/#jupyter-notebooks","title":"Jupyter Notebooks","text":"<p>Jupyter Notebooks are used to create and share interactive documents that contain a mix of live code, visualizations, and text. These can be written in <code>Python</code>, <code>Julia</code>, or <code>R</code>.</p> <p></p> Common uses include: <p>data transformation, numerical simulation, statistical modelling, machine learning and more.</p> <p>The jupyter notebooks are great launchpads for analytics including machine learning. The <code>jupyterlab-cpu</code> image gives a good core experience for python, including common packages such as <code>numpy</code>, <code>pandas</code> and <code>scikit-learn</code>. If you're interested specifically in using TensorFlow or PyTorch, we also have <code>jupyterlab-tensorflow</code> and <code>jupyterlab-pytorch</code> which come with those tools pre-installed.</p> <p>For the <code>jupyterlab-pytorch</code> image, the PyTorch packages (torch, torchvision, and torchaudio) are installed in the <code>torch</code> conda environment. You must activate this environment to use PyTorch.</p> <p>For the <code>jupyterlab-cpu</code>, <code>jupyterlab-tensorflow</code>, and <code>jupyterlab-pytorch</code> images, in the default shell the <code>conda activate</code> command may not work. This is due to the environment not being initialized properly. In this case run <code>bash</code>, you should see the AAW logo and a few instructions appear. After this <code>conda activate</code> should work properly. If you see the AAW logo on startup it means the environment is correctly initialized and <code>conda activate</code> should work properly. A fix for this bug is in the works, once this is fixed this paragraph will be removed.</p> <p>Each image comes pre-loaded with VS Code in the browser if you prefer a full IDE experience.</p>"},{"location":"1-Experiments/Selecting-an-Image/#rstudio","title":"RStudio","text":"<p>RStudio gives you an integrated development environment specifically for <code>R</code>. If you're coding in <code>R</code>, this is typically the Notebook Server to use. Use the <code>rstudio</code> image to get an RStudio environment.</p> <p></p>"},{"location":"1-Experiments/Selecting-an-Image/#remote-desktop","title":"Remote-Desktop","text":"<p>For a full Ubuntu desktop experience, use the remote desktop image. It comes pre-loaded with Python, R and Geomatics tooling, but are delivered in a typical desktop experience that also comes with Firefox, VS Code, and open office tools. The operating system is Ubuntu 18.04 with the XFCE desktop environment.</p> <p></p>"},{"location":"2-Publishing/Custom/","title":"Overview","text":""},{"location":"2-Publishing/Custom/#custom-web-apps","title":"Custom Web Apps","text":"<p>We can deploy anything as long as it's open source and we can put it in a Docker container. For instance, Node.js apps, Flask or Dash apps. Etc.</p> <p></p> <p>See the source code for this app</p> <p>We just push these kinds of applications through GitHub into the server. The source for the above app is <code>StatCan/covid19</code></p>"},{"location":"2-Publishing/Custom/#setup","title":"Setup","text":""},{"location":"2-Publishing/Custom/#how-to-get-your-app-hosted","title":"How to get your app hosted","text":"<p>If you already have a web app in a git repository then, as soon as it's containerized, we can fork the Git repository into the StatCan GitHub repository and point a URL to it. To update it, you'll just interact with the StatCan GitHub repository with Pull Requests.</p> <p>Contact us if you have questions.</p>"},{"location":"2-Publishing/Dash/","title":"Overview","text":"<p>Dash is a great tool used by many for data analysis, data exploration, visualization, modelling, instrument control, and reporting.</p> <p>The following example demonstrates a highly reactive and customised Dash app with little code.</p> <p>Running your Notebook Server and accessing the port</p> <p>When running any tool from your Jupyter Notebook that posts a website to a port, you will not be able to simply access it from <code>http://localhost:5000/</code> as normally suggested in the output upon running the web-app.</p> <p>To access the web server you will need to use the base URL. In your notebook terminal run:</p> <pre><code>echo https://kubeflow.covid.cloud.statcan.ca${JUPYTER_SERVER_URL:19}proxy/5000/\n</code></pre>"},{"location":"2-Publishing/Dash/#data-visualization-with-dash","title":"Data Visualization with Dash","text":"<p>Dash makes it simple to build an interactive GUI around your data analysis code. This is an example of a Layout With Figure and Slider from Dash.</p> <p></p>"},{"location":"2-Publishing/Dash/#plotly-dash","title":"Plotly Dash","text":"<p>Publish with Canadian-made software.</p> <p>Plotly Dash is a popular Python library that allows you to create interactive web-based visualizations and dashboards with ease. Developed by the Montreal-based company Plotly, Dash has gained a reputation for being a powerful and flexible tool for building custom data science graphics. With Dash, you can create everything from simple line charts to complex, multi-page dashboards with interactive widgets and controls. Because it's built on open source technologies like Flask, React, and Plotly.js, Dash is highly customizable and can be easily integrated with other data science tools and workflows. Whether you're a data scientist, analyst, or developer, Dash can help you create engaging and informative visualizations that bring your data to life.</p>"},{"location":"2-Publishing/Dash/#getting-started","title":"Getting Started","text":"<p>Open a terminal window in your Jupyter notebook and run the following commands:</p> <pre><code># required installations if not already installed\npip3 install dash==1.16.3\npip3 install pandas\n</code></pre> <p>Create a file called app.py with the following content:</p> <pre><code># app.py\n#!/usr/bin/env python3\nimport dash\nimport dash_core_components as dcc\nimport dash_html_components as html\nfrom dash.dependencies import Input, Output\nimport plotly.express as px\nimport pandas as pd\ndf = pd.read_csv('https://raw.githubusercontent.com/plotly/datasets/master/gapminderDataFiveYear.csv')\nexternal_stylesheets = ['https://codepen.io/chriddyp/pen/bWLwgP.css']\napp = dash.Dash(__name__, external_stylesheets=external_stylesheets)\napp.layout = html.Div([\ndcc.Graph(id='graph-with-slider'),\ndcc.Slider(\nid='year-slider',\nmin=df['year'].min(),\nmax=df['year'].max(),\nvalue=df['year'].min(),\nmarks={str(year): str(year) for year in df['year'].unique()},\nstep=None\n)\n])\n@app.callback(\nOutput('graph-with-slider', 'figure'),\n[Input('year-slider', 'value')])\ndef update_figure(selected_year):\nfiltered_df = df[df.year == selected_year]\nfig = px.scatter(filtered_df, x=\"gdpPercap\", y=\"lifeExp\",\nsize=\"pop\", color=\"continent\", hover_name=\"country\",\nlog_x=True, size_max=55)\nfig.update_layout(transition_duration=500)\nreturn fig\nif __name__ == '__main__':\napp.run_server(debug=True)\n</code></pre>"},{"location":"2-Publishing/Dash/#run-your-app","title":"Run your app","text":"<pre><code>python app.py\n# or you can use:\nexport FLASK_APP=app.py\nflask run\n</code></pre>"},{"location":"2-Publishing/Datasette/","title":"Overview","text":"<p>Datasette is an instant JSON API for your SQLite databases allowing you to explore the DB and run SQL queries in a more interactive way.</p> <p>You can find a list of example datasettes here.</p> <p>The Datasette Ecosystem</p> <p>There are all sorts of tools for converting data to and from sqlite here. For example, you can load shapefiles into sqlite, or create Vega plots from a sqlite database. SQLite works well with <code>R</code>, <code>Python</code>, and many other tools.</p>"},{"location":"2-Publishing/Datasette/#example-datasette","title":"Example Datasette","text":"<p>Below are some screenshots from the global-power-plants Datasette, you can preview and explore the data in the browser, either with clicks or SQL queries.</p> <p></p> <p>You can even explore maps within the tool!</p> <p></p>"},{"location":"2-Publishing/Datasette/#video-tutorial","title":"Video Tutorial","text":""},{"location":"2-Publishing/Datasette/#getting-started","title":"Getting Started","text":""},{"location":"2-Publishing/Datasette/#installing-datasette","title":"Installing Datasette","text":"<p>In your Jupyter Notebook, open a terminal window and run the command <code>pip3 install datasette</code>. </p>"},{"location":"2-Publishing/Datasette/#starting-datasette","title":"Starting Datasette","text":"<p>To view your own database in your Jupyter Notebook, create a file called start.sh in your project directory and copy the below code into it. Make the file executable using <code>chmod +x start.sh</code>. Run the file with <code>./start.sh</code>. Access the web server using the base URL with the port number you are using in the below file.</p> <p>start.sh</p> <pre><code>#!/bin/bash\n# This script just starts Datasette with the correct URL, so\n# that you can use it within kubeflow.\n# Get an example database\nwget https://github.com/StatCan/aaw-contrib-r-notebooks/raw/master/database-connections/latin_phrases.db\n\n# If you have your own database, you can change this line!\nDATABASE=latin_phrases.db\n\nexport BASE_URL=\"https://kubeflow.covid.cloud.statcan.ca${JUPYTER_SERVER_URL:19}proxy/8001/\"\necho \"Base url: ${BASE_URL}\"\ndatasette $DATABASE --cors --config max_returned_rows:100000 --config sql_time_limit_ms:5500 --config base_url:${BASE_URL}\n</code></pre> <p>Check out this video tutorial</p> <p>One user of the platform used Datasette along with a javascript dashboard. See this video for a demo.</p> <p>Running your Notebook Server and accessing the port</p> <p>When running any tool from your Jupyter Notebook that posts a website to a port, you will not be able to simply access it from <code>http://localhost:5000/</code> as normally suggested in the output upon running the web-app.</p> <p>To access the web server you will need to use the base URL. In your notebook terminal, run:</p> <pre><code>echo https://kubeflow.covid.cloud.statcan.ca${JUPYTER_SERVER_URL:19}proxy/5000/\n</code></pre>"},{"location":"2-Publishing/Overview/","title":"Overview","text":""},{"location":"2-Publishing/Overview/#statistical-publishing","title":"Statistical Publishing","text":"<p>Beautiful graphics is important in statistical publishing because it makes the data more accessible and understandable to a wider audience.</p> <p>Publishing is an essential aspect of data science and statistics. It allows researchers to share their findings with the broader scientific community, enabling others to build upon their work and push the field forward. By sharing their data and methods openly, researchers can receive feedback on their work and ensure that their findings are accurate and reliable.</p> <p>Publishing allows researchers to establish their reputation and gain recognition for their work, which can help secure funding and future research opportunities. In addition, publishing research findings can have important implications for public policy and decision-making, as policymakers often rely on scientific evidence to make informed decisions. Overall, publishing is an integral part of the scientific process and plays a critical role in advancing knowledge and solving real-world problems.</p> <p>Statistical publishing involves communicating statistical information to a broader audience using various forms of media, such as charts, graphs, and infographics. Having beautiful graphics is important in statistical publishing because it makes the data more accessible and understandable to a wider audience. Well-designed visualizations can help communicate complex statistical concepts and patterns in a clear and compelling way, allowing the audience to quickly grasp the main insights and conclusions.</p> <p>Beautiful graphics can enhance the overall impact of statistical publications, making them more engaging and memorable. This can be particularly important when communicating important information to decision-makers, stakeholders, or the general public, where the ability to clearly and effectively communicate data-driven insights can be critical to achieving success.</p> <p>In summary, data science and statistical publishing are essential for turning complex data into meaningful insights, and having beautiful graphics is a crucial aspect of effectively communicating those insights to a broader audience.</p>"},{"location":"2-Publishing/PowerBI/","title":"Overview","text":""},{"location":"2-Publishing/PowerBI/#loading-data-into-power-bi","title":"Loading data into Power BI","text":"<p>We do not offer a Power BI server, but you can pull your data into Power BI from our Storage system, and use the data as a <code>pandas</code> data frame.</p> <p></p>"},{"location":"2-Publishing/PowerBI/#setup","title":"Setup","text":""},{"location":"2-Publishing/PowerBI/#what-youll-need","title":"What you'll need","text":"<ol> <li>A computer with Power BI, and Python 3.6</li> <li>Your MinIO <code>ACCESS_KEY</code> and <code>SECRET_KEY</code> on hand. (See    Storage)</li> </ol>"},{"location":"2-Publishing/PowerBI/#set-up-power-bi","title":"Set up Power BI","text":"<p>Open up your Power BI system, and open up this Power BI quick start in your favourite text editor.</p> <p>You'll have to make sure that <code>pandas</code>, <code>boto3</code>, and <code>numpy</code> are installed, and that you're using the right Conda virtual environment (if applicable).</p> <p></p> <p>You'll then need to make sure that Power BI is using the correct Python environment. This is modified from the options menu, and the exact path is specified in the quick start guide.</p>"},{"location":"2-Publishing/PowerBI/#edit-your-python-script","title":"Edit your python script","text":"<p>Then, edit your Python script to use your MinIO <code>ACCESS_KEY</code> and <code>SECRET_KEY</code>, and then click \"Get Data\" and copy it in as a Python Script.</p> <p></p>"},{"location":"2-Publishing/R-Shiny/","title":"Overview","text":"<p>R-Shiny is an R package that makes it easy to build interactive web apps in R. </p> <p>R Shiny App Hosting</p> <p>We currently do not support hosting R Shiny apps but you are able to create them. We want to enable R Shiny app hosting in the future.</p> <p></p>"},{"location":"2-Publishing/R-Shiny/#r-shiny","title":"R Shiny","text":"<p>Publish Professional Quality Graphics</p> <p></p> <p>R Shiny is an open source web application framework that allows data scientists and analysts to create interactive, web-based dashboards and data visualizations using the R programming language. One of the main advantages of R Shiny is that it offers a straightforward way to create high-quality, interactive dashboards without the need for extensive web development expertise. With R Shiny, data scientists can leverage their R coding skills to create dynamic, data-driven web applications that can be shared easily with stakeholders.</p> <p>Another advantage of R Shiny is that it supports a variety of data visualizations that can be easily customized to meet the needs of the project. Users can create a wide range of charts and graphs, from simple bar charts and scatter plots to more complex heatmaps and network graphs. Additionally, R Shiny supports a variety of interactive widgets that allow users to manipulate and explore data in real-time.</p> <p></p> <p>R Shiny is also highly extensible and can be integrated with other open source tools and platforms to build end-to-end data science workflows. With its powerful and flexible features, R Shiny is a popular choice for building data visualization dashboards for a wide range of applications, from scientific research to business analytics. Overall, R Shiny offers a powerful, customizable, and cost-effective solution for creating interactive dashboards and data visualizations.</p> <p>Use R-Shiny to build interactive web apps straight from R. You can deploy your R Shiny dashboard by submitting a pull request to our R-Dashboards GitHub repository.</p>"},{"location":"2-Publishing/R-Shiny/#setup","title":"Setup","text":""},{"location":"2-Publishing/R-Shiny/#just-send-a-pull-request","title":"Just send a pull request!","text":"<p>All you have to do is send a pull request to our R-Dashboards repository. Include your repository in a folder with the name you want (for example, \"air-quality-dashboard\"). Then we will approve it and it will come online.</p> <p>If you need extra R libraries to be installed, send your list to the R-Shiny repository by creating a GitHub Issue and we will add the dependencies.</p> <p></p> <p>See the above dashboard here</p> <p>The above dashboard is in GitHub. Take a look at the source, and see the dashboard live.</p>"},{"location":"2-Publishing/R-Shiny/#once-youve-got-the-basics","title":"Once you've got the basics ...","text":""},{"location":"2-Publishing/R-Shiny/#embedding-dashboards-into-your-websites","title":"Embedding dashboards into your websites","text":"<p>Embedding dashboards in other sites</p> <p>We have not had a chance to look at this or prototype it yet, but if you have a use-case, feel free to reach out to engineering. We will work with you to figure something out.</p>"},{"location":"3-Pipelines/Kubeflow-Pipelines/","title":"Overview","text":"<p>Kubeflow pipelines are in the process of being removed from AAW.</p> <p>No new development should use Kubeflow pipelines. If you have questions about this removal, please speak with the AAW maintainers.</p> <p>Kubeflow Pipelines is a platform for building machine learning workflows for deployment in a Kubernetes environment. It enables authoring pipelines that encapsulate analytical workflows (transforming data, training models, building visuals, etc.). These pipelines can be shared, reused, and scheduled, and are built to run on compute provided via Kubernetes. Here is an example of a pipeline with many <code>sample</code> steps feeding into a single <code>average</code> step. This image comes from the Kubeflow Pipelines UI.</p> <p></p> <p>In the context of the Advanced Analytics Workspace, Kubeflow Pipelines are interacted with through:</p> <ul> <li>The Kubeflow UI, where from the Pipelines menu   you can upload pipelines, view the pipelines you have and their results, etc.</li> <li>The Kubeflow Pipelines python   SDK, accessible   through the   Jupyter Notebook Servers,   where you can define your components and pipelines, submit them to run now, or   even save them for later.</li> </ul> More examples in the notebooks <p>More comprehensive pipeline examples specifically made for this platform are available on GitHub (and in every Notebook Server at <code>/jupyter-notebooks</code>). You can also check out public sources.</p> <p>See the official Kubeflow docs for a more detailed explanation of Kubeflow Pipelines.</p>"},{"location":"3-Pipelines/Kubeflow-Pipelines/#what-are-pipelines-and-how-do-they-work","title":"What are pipelines and how do they work?","text":"<p>A pipeline in Kubeflow Pipelines consists of one or more pipeline components chained together to form a workflow. The components are like functions, describing the individual steps in your workflow (such as pulling columns from a data store, transforming data, or training a model). The pipeline is the logic that glues components together, such as:</p> <ol> <li>Run Component-A</li> <li>Pass the output from Component-A to Component-B and Component-C</li> <li>...</li> </ol> <p>In the above image, the logic would be running many <code>sample</code> steps followed by a single <code>average</code> step.</p> <p>At their core, each component has:</p> <ul> <li>A standalone application, packaged as a   Docker image, for doing the actual   work. The code in the Docker image could be a shell script, Python script, or   anything else you can run from a Linux terminal, and generally will have a   command line interface for data exchange (accessible through <code>docker run</code>)</li> <li>A YAML file that describes how Kubeflow Pipelines runs this code (what Docker   image should be run, what command line arguments does it accept, what output   does it generate)</li> </ul> <p>Each component should be single purpose, modular, and reusable.</p>"},{"location":"3-Pipelines/Kubeflow-Pipelines/#setup","title":"Setup","text":""},{"location":"3-Pipelines/Kubeflow-Pipelines/#define-and-run-your-first-pipeline-using-the-python-sdk","title":"Define and run your first pipeline using the Python SDK","text":"<p>While pipelines and components are defined in Kubeflow Pipelines by YAML files that use Docker images, that does not mean we have to work directly with either YAML files or Docker images. The Kubeflow Pipelines SDK provides a way for us to define our pipeline and components directly in Python code, where the SDK then translates our Python code to YAML files for us.</p> <p>For our first example, let's define a simple pipeline using only the Python SDK. The purpose of this section is to give a high level view of component and pipeline authoring, not a deep dive. More detailed looks into defining your own components, passing data between components, and returning data from your pipeline are explained in more detail in further sections.</p> <p>The demo pipeline we define will do the following:</p> <ol> <li>Accept five numbers as arguments</li> <li>Average of the first three numbers</li> <li>Average of the last two numbers</li> <li>Average of the results of (2) and (3)</li> </ol> <p>To do this, we will first define our component. Our <code>average</code> component will call a Docker image that does the following:</p> <ul> <li>Accepts one or more numbers as command line arguments</li> <li>Returns the average of these numbers by writing them to an output file in the   container (by default, to <code>out.txt</code>)</li> </ul> <p>This Docker image is already built for us and stored in our container registry here: <code>k8scc01covidacr.azurecr.io/kfp-components/average:v1</code>. Don't worry if you don't know Docker - since the image is built already, we only have to tell Kubeflow Pipelines where it is.</p> <p>??? info \"Full details of the <code>average</code> component's Docker image are in     GitHub\"     This image effectively runs the following code (slightly cleaned up for     brevity).  By making <code>average.py</code> accept an arbitrary set of numbers as     inputs, we can use the same <code>average</code> component for all steps in our     pipeline:</p> <pre><code>    import argparse\n\n    def parse_args():\n        parser = argparse.ArgumentParser(description=\"Returns the average of one or \"\n                                         \"more numbers as a JSON file\")\n        parser.add_argument(\"numbers\", type=float, nargs=\"+\", help=\"One or more numbers\")\n        parser.add_argument(\"--output_file\", type=str, default=\"out.txt\", help=\"Filename \"\n                            \"to write output number to\")\n        return parser.parse_args()\n\n    if __name__ == '__main__':\n        args = parse_args()\n        numbers = args.numbers\n        output_file = args.output_file\n\n        print(f\"Averaging numbers: {numbers}\")\n        avg = sum(numbers) / len(numbers)\n        print(f\"Result = {avg}\")\n\n        print(f\"Writing output to {output_file}\")\n        with open(output_file, 'w') as fout:\n            fout.write(str(avg))\n\n        print(\"Done\")\n</code></pre> <p>To make our <code>average</code> image into a Kubeflow Pipelines component, we make a <code>kfp.dsl.ContainerOp</code> in Python that defines how Kubeflow Pipelines interacts with our container, specifying:</p> <ul> <li>The Docker image location to use</li> <li>How to pass arguments to the running container</li> <li>What outputs to expect from the container</li> </ul> <p>We could use <code>ContainerOp</code> directly, but since we'll use <code>average</code> a few times we instead create a factory function we can reuse:</p> <pre><code>from kfp import dsl\ndef average_op(*numbers):\n\"\"\"\n    Factory for average ContainerOps\n    Accepts an arbitrary number of input numbers, returning a ContainerOp that\n    passes those numbers to the underlying Docker image for averaging\n    Returns output collected from ./out.txt from inside the container\n    \"\"\"\n# Input validation\nif len(numbers) &lt; 1:\nraise ValueError(\"Must specify at least one number to take the average of\")\nreturn dsl.ContainerOp(\nname=\"average\",  # What will show up on the pipeline viewer\nimage=\"k8scc01covidacr.azurecr.io/kfp-components/average:v1\",  # The image that KFP runs to do the work\narguments=numbers,  # Passes each number as a separate command line argument\n# Note that these arguments get serialized to strings\nfile_outputs={'data': './out.txt'},  # Expect an output file called out.txt to be generated\n# KFP can read this file and bring it back automatically\n)\n</code></pre> <p>To define our pipeline, we create a Python function decorated by the <code>@dsl.pipeline</code> decorator. We invoke our <code>average_op</code> factory to use our average container. We pass each <code>average</code> some inputs, and even use their outputs by accessing <code>avg_*.output</code>.</p> <pre><code>@dsl.pipeline(\nname=\"my pipeline's name\"\n)\ndef my_pipeline(a, b, c, d, e):\n\"\"\"\n    Averaging pipeline which accepts five numbers and does some averaging\n    operations on them\n    \"\"\"\n# Compute averages for two groups\navg_1 = average_op(a, b, c)\navg_2 = average_op(d, e)\n# Use the results from _1 and _2 to compute an overall average\naverage_result_overall = average_op(avg_1.output, avg_2.output)\n</code></pre> <p>Finally, while we've defined our pipeline in Python, Kubeflow Pipelines itself needs everything defined as a YAML file. This final step uses the Kubeflow Pipelines Python SDK to translate our pipeline function into a YAML file that describes exactly how Kubeflow Pipelines can interact with our component. Unzip it and take a look for yourself!</p> <pre><code>from kfp import compiler\npipeline_yaml = 'pipeline.yaml.zip'\ncompiler.Compiler().compile(\nmy_pipeline,\npipeline_yaml\n)\nprint(f\"Exported pipeline definition to {pipeline_yaml}\")\n</code></pre> Kubeflow Pipelines is a lazy beast <p>It is useful to keep in mind what computation is happening when you run this python code versus what happens when you submit the pipeline to Kubeflow Pipelines. Although it seems like everything is happening in the moment, try adding <code>print(avg_1.output)</code> to the above pipeline and see what happens when you compile your pipeline. The Python SDK we're using is for authoring pipelines, not for running them, so results from components will never be available when you run this Python code. The is discussed more below in Understanding what computation occurs when.</p> <p>To actually run our pipeline, we define an experiment:</p> <pre><code>experiment_name = \"averaging-pipeline\"\nimport kfp\nclient = kfp.Client()\nexp = client.create_experiment(name=experiment_name)\npl_params = {\n'a': 5,\n'b': 5,\n'c': 8,\n'd': 10,\n'e': 18,\n}\n</code></pre> <p>And then run an instance of our pipeline with the arguments we want:</p> <pre><code>import time\nrun = client.run_pipeline(\nexp.id,  # Run inside the above experiment\nexperiment_name + '-' + time.strftime(\"%Y%m%d-%H%M%S\"),  # Give our job a name with a timestamp so its unique\npipeline_yaml,  # Pass the .yaml.zip we created above.  This defines the pipeline\nparams=pl_params  # Pass our parameters we want to run the pipeline with\n)\n</code></pre> <p>This can all be seen in the Kubeflow Pipelines UI:</p> <p></p> <p></p> <p>Later when we want to reuse the pipeline, we can pass different arguments and do it all again.</p> <p>!!! info \"We create our experiment, upload our pipeline, and run from Python in     this example, but we could also do all this through the Kubeflow Pipelines     UI above.</p>"},{"location":"3-Pipelines/Kubeflow-Pipelines/#understanding-what-computation-occurs-when","title":"Understanding what computation occurs when","text":"<p>The above example uses Python code to define:</p> <ul> <li>The interface between Kubeflow Pipelines and our Docker containers doing the   work (by defining <code>ContainerOp</code>'s)</li> <li>The logic of our pipeline (by defining <code>my_pipeline</code>).</li> </ul> <p>But when we run <code>compiler.Compiler().compile()</code> and <code>client.run_pipeline()</code>, what actually happens?</p> <p>It is important to remember that everything we run in Python here is specifying the pipeline and its components in order to write YAML definitions, it is not doing the work of the pipeline. When running <code>compiler.Compiler().compile()</code>, we are not running our pipeline in the typical sense. Instead, KFP uses <code>my_pipeline</code> to build a YAML version of it. When we <code>compile</code>, the KFP SDK is passing placeholder arguments to <code>my_pipeline</code> and tracing where they (and any other runtime data) go, such as any output a component produces. When <code>compile</code> encounters a <code>ContainerOp</code>, nothing runs now - instead it takes note that a container will be there in future and remembers what data it will consume/generate. This can be seen by modifying and recompiling our pipeline:</p> <pre><code>@dsl.pipeline(\nname=\"my pipeline's name\"\n)\ndef my_pipeline(a, b, c, d, e):\n\"\"\"\n    Averaging pipeline which accepts five numbers and does some averaging operations on them\n    \"\"\"\n# NEW CODE\nx = 1 + 1\nprint(f\"The value of x is {x}\")\nprint(f\"The value of a is {a}\")\n# Compute averages for two groups\navg_1 = average_op(a, b, c)\navg_2 = average_op(d, e)\n# NEW CODE\nprint(f\"The value of avg_1.output is {avg_1.output}\")\n# Use the results from _1 and _2 to compute an overall average\naverage_result_overall = average_op(avg_1.output, avg_2.output)\n</code></pre> <p>And when we <code>compile</code>, we see print statements:</p> <pre><code>The value of x is 2\nThe value of a is {{pipelineparam:op=;name=a}}\nThe value of avg_1.output is {{pipelineparam:op=averge;name=data}}\n</code></pre> <p>In the first print statement everything is normal. In the second and third print statements, however, we see string placeholders rather than actual output. So while <code>compile</code> does \"execute\" <code>my_pipeline</code>, the KFP-specific parts of the code don't actually generate results. This can also be seen in the YAML file that <code>compile</code> generates, for example looking at the portion defining our <code>average_result_overall</code> component:</p> <pre><code>- name: average-3\ncontainer:\nargs:\n[\n\"{{inputs.parameters.average-data}}\",\n\"{{inputs.parameters.average-2-data}}\",\n]\nimage: k8scc01covidacr.azurecr.io/kfp-components/average:v1\ninputs:\nparameters:\n- { name: average-2-data }\n- { name: average-data }\noutputs:\nartifacts:\n- { name: average-3-data, path: ./out.txt }\nmetadata:\nlabels: { pipelines.kubeflow.org/pipeline-sdk-type: kfp }\n</code></pre> <p>In this YAML we see the input parameters passed are placeholders for data from previous components rather than their actual value. This is because while KFP knows a result from <code>average-data</code> and <code>average-2-data</code> will be passed to average, but the value of that result is not available until the pipeline is actually run.</p> Component naming within the YAML file <p>Because we made an <code>average_op</code> factory function with <code>name='average'</code> above, our YAML file has component names that automatically increment to avoid recreating the same name twice. We could have been fancier with our factory functions to more directly control our names, giving an argument like <code>name='average_first_input_args'</code>, or could even have explicitly defined the name in our pipeline by using <code>avg_1 = average_op(a, b, c).set_display_name(\"Average 1\")</code>.</p> <p>As one more example, let's try two more pipelines. One has a for loop inside which prints \"Woohoo!\" a fixed number of times. whereas the other does the same but loops <code>n</code> times (where <code>n</code> is a pipeline parameter):</p> <p>!!! info \"Pipeline parameters are described more below, but they work like     parameters for functions. Pipelines can accept data (numbers, string URL's     to large files in MinIO, etc.) as arguments, allowing a single generic     pipeline to work in many situations.\"</p> <pre><code>@dsl.pipeline(\nname=\"my pipeline's name\"\n)\ndef another_pipeline():\n\"\"\"\n    Prints to the screen 10 times\n    \"\"\"\nfor i in range(10):\nprint(\"Woohoo!\")\n# And just so we've got some component going too...\navg = average_op(n)\ncompiler.Compiler().compile(\nanother_pipeline,\n\"another.yaml.zip\"\n)\n</code></pre> <pre><code>@dsl.pipeline(\nname=\"my pipeline's name\"\n)\ndef another_another_pipeline(n):\n\"\"\"\n    Prints to the screen n times\n    \"\"\"\nfor i in range(n):\nprint(\"Woohoo!\")\n# And just so we've got some component going too...\navg = average_op(n)\ncompiler.Compiler().compile(\nanother_another_pipeline,\n\"another.yaml.zip\"\n)\n</code></pre> <p>The first works as you'd expect, but the second raises the exception:</p> <p><code>TypeError: 'PipelineParam' object cannot be interpreted as an integer</code></p> <p>Why? Because when authoring the pipeline <code>n</code> is a placeholder and has no value. KFP cannot define a pipeline from this because it does not know how many times to loop. We'd hit similar problems if using <code>if</code> statements. There are some ways around this, but they're left to the reader to explore through the Kubeflow Pipelines docs.</p> <p>Why does pipeline authoring behave this way? Because pipelines (and components) are meant to be reusable definitions of logic that are defined in static YAML files, with all dynamic decision making done inside components. This can make them a little awkward to define, but also helps them be more reusable.</p>"},{"location":"3-Pipelines/Kubeflow-Pipelines/#once-youve-got-the-basics","title":"Once you've got the basics ...","text":""},{"location":"3-Pipelines/Kubeflow-Pipelines/#data-exchange","title":"Data Exchange","text":""},{"location":"3-Pipelines/Kubeflow-Pipelines/#passing-data-into-within-and-from-a-pipeline","title":"Passing data into, within, and from a pipeline","text":"<p>In the first example above, we pass:</p> <ul> <li>Numbers into our pipeline</li> <li>Numbers between components within our pipeline</li> <li>A number back to the user at the end</li> </ul> <p>But as discussed above, pipeline arguments and component results are just placeholder objects \u2013 so how does KFP know our values are numeric? The answer is: it doesn't. In fact, it didn't even treat them as numbers above. Instead it treated them as strings. It is just that our pipeline components worked just as well with <code>\"5\"</code> as they would have with <code>5</code>.</p> <p>A safe default assumption is that all data exchange happens as a string. When we passed <code>a, b, ...</code> into the pipeline, those numbers were implicitly stringified because they eventually become command line arguments for our Docker container. When we read the result of <code>avg_1</code> from its <code>out.txt</code>, that result was read as a string. By calling <code>average_op(avg_1.output, avg_2.output)</code>, we ask KFP to pass the string output from <code>avg_1</code> and <code>avg_2</code> to a new <code>average_op</code>. It just so happened that, since <code>average_op</code> passes each string as a command line argument to our Docker image, it didn't really matter they were strings.</p> <p>You can still use non-string data types, but you need to pass them as serialized versions. So if we wanted our <code>avg_1</code> component to return both the numbers passed to it and the average returned as a dictionary, for example:</p> <pre><code>{\n'numbers': [5, 5, 8],\n'result': 6.0,\n}\n</code></pre> <p>We could modify our <code>average.py</code> in the Docker image write our dictionary of numbers and result to <code>out.txt</code> as JSON. But then when we pass the result to make <code>average_result_overall</code>, that component needs to deserialize the above JSON and pull the data from it that it needs. And because these results are not available when authoring the pipeline, something like this does not work:</p> <pre><code>def my_pipeline(a, b, c, d, e):\n\"\"\"\n    Averaging pipeline which accepts five numbers and does some averaging operations on them\n    \"\"\"\n# Compute averages for two groups\navg_1 = average_op_that_returns_json(a, b, c)\navg_2 = average_op_that_returns_json(d, e)\n# THIS DOES NOT WORK!\nimport json\navg_1_result = json.loads(avg_1.output)['result']\navg_2_result = json.loads(avg_2.output)['result']\n# Use the results from _1 and _2 to compute an overall average\naverage_result_overall = average_op(avg_1.output, avg_2.output)\n</code></pre> <p>At <code>compile</code> time, <code>avg_1.output</code> is just a placeholder and can't be treated like the JSON it will eventually become. To do something like this, we need to interpret the JSON string within a container.</p>"},{"location":"3-Pipelines/Kubeflow-Pipelines/#passing-secrets","title":"Passing Secrets","text":"<p>Pipelines often need sensitive information (passwords, API keys, etc.) to operate. To keep these secure, these cannot be passed to a Kubeflow Pipeline using a pipeline argument, environment variable, or as a hard coded value in python code, as they will be exposed as plain text for others to read.</p> <p>To address this issue, we use Kubernetes secrets as a way to securely store and pass sensitive information. Each secret is a key-value store containing some number of key-value pairs. The secrets can be passed by reference to the pipeline.</p> Secrets are only accessible within their own namespace <p>Secrets can only be accessed within the namespace they are created in. If you need to use a secret in another namespace, you need to add it there manually.</p>"},{"location":"3-Pipelines/Kubeflow-Pipelines/#create-a-key-value-store","title":"Create a key-value store","text":"<p>The example below creates a key-value store called elastic-credentials which contains two key-value pairs:</p> <pre><code>\"username\": \"USERNAME\",\n\"password\": \"PASSWORD\"\n</code></pre> <pre><code>kubectl create secret generic elastic-credentials \\\n--from-literal=username=\"YOUR_USERNAME\" \\\n--from-literal=password=\"YOUR_PASSWORD\"\n</code></pre>"},{"location":"3-Pipelines/Kubeflow-Pipelines/#get-an-existing-key-value-store","title":"Get an existing key-value store","text":"<pre><code># The secrets will be base64 encoded.\nkubectl get secret elastic-credentials\n</code></pre>"},{"location":"3-Pipelines/Kubeflow-Pipelines/#mounting-kubernetes-secrets-to-environment-variables-in-container-operations","title":"Mounting Kubernetes Secrets to Environment Variables in Container Operations","text":"<p>Once the secrets are defined in the project namespace, you can mount specific secrets as environment variables in your container using the Kubeflow SDK.</p> <p>Example</p> <p>This example is based off of a snippet from the Python KFP source code.</p> <p>This example shows how (1) an Elasticsearch username(2) an Elasticsearch password, and (3) a GitLab deploy token are passed to the container operation as environment variables.</p> <pre><code># Names of k8s secret key-value stores\nES_CREDENTIALS_STORE = \"elastic-credentials\"\nGITLAB_CREDENTIALS_STORE = \"gitlab-credentials\"\n# k8s secrets key names\nES_USER_KEY = \"username\"\nES_PASSWORD_KEY = \"password\"\nGITLAB_DEPLOY_TOKEN_KEY = 'token'\n# Names of environment variables that secrets should be mounted to in the\n# container\nES_USER_ENV = \"ES_USER\"\nES_PASS_ENV = \"ES_PASS\"\nGITLAB_DEPLOY_TOKEN_ENV = \"GITLAB_DEPLOY_TOKEN\"\n# ...\ncontainer_operation = dsl.ContainerOp(\nname='some-op',\nimage='some-image',\n).add_env_variable(\nk8s_client.V1EnvVar(\nname=ES_USER_ENV,\nvalue_from=k8s_client.V1EnvVarSource(\nsecret_key_ref=k8s_client.V1SecretKeySelector(\nname=ES_CREDENTIALS_STORE,\nkey=ES_USER_KEY\n)\n)\n)\n) \\\n    .add_env_variable(\nk8s_client.V1EnvVar(\nname=ES_PASS_ENV,\nvalue_from=k8s_client.V1EnvVarSource(\nsecret_key_ref=k8s_client.V1SecretKeySelector(\nname=ES_CREDENTIALS_STORE,\nkey=ES_PASSWORD_KEY\n)\n)\n)\n) \\\n    .add_env_variable(\nk8s_client.V1EnvVar(\nname=GITLAB_DEPLOY_TOKEN_ENV,\nvalue_from=k8s_client.V1EnvVarSource(\nsecret_key_ref=k8s_client.V1SecretKeySelector(\nname=GITLAB_CREDENTIALS_STORE,\nkey=GITLAB_DEPLOY_TOKEN_KEY\n)\n)\n)\n)\n</code></pre>"},{"location":"3-Pipelines/Kubeflow-Pipelines/#parameterizing-pipelines","title":"Parameterizing pipelines","text":"<p>Whenever possible, create pipelines in a generic way: define parameters that might change as pipeline inputs instead of writing values directly in your Python code. For example, if you want a pipeline to process data from <code>minimal-tenant/john-smith/data1.csv</code>, don't hard code that path - instead, accept it as a pipeline parameter. This way you can call the same pipeline repeatedly by passing it the data location as an argument. You can see this approach in our example notebooks, where we accept MinIO credentials and the location to store our results as pipeline parameters.</p>"},{"location":"3-Pipelines/Kubeflow-Pipelines/#passing-complexlarge-data-tofrom-a-pipeline","title":"Passing complex/large data to/from a pipeline","text":"<p>Although small data can often be stringified, passing by string is not suitable for complex data (large parquet files, images, etc.). It is common to use blob storage (for example: MinIO) or other outside storage methods to persist data between components or even for later use. A typical pattern would be:</p> <ul> <li>Upload large/complex input data to blob storage (e.g. training data, a saved   model, etc.)</li> <li>Pass the location of this data into the pipeline as parameters, and make your   pipeline/components fetch the data as required</li> <li>For each component in a pipeline, specify where they place outputs in the same   way</li> <li>For each component also <code>return</code> the path where it has stored its data (in   this case, the string we passed it in the above bullet). This feels redundant,   but it is a common pattern that lets you chain operations together</li> </ul> <p>Here is a schematic example of this pattern:</p> <pre><code>def my_blobby_pipeline(path_to_numbers_1, path_to_numbers_2, path_for_output):\n\"\"\"\n    Averaging pipeline which accepts two groups of numbers and does some averaging operations on them\n    \"\"\"\n# Compute averages for two groups\navg_1 = average_op_that_takes_path_to_blob(path_to_numbers=path_to_numbers_1,\noutput_location=path_for_output + \"/avg_1\"\n)\navg_2 = average_op_that_takes_path_to_blob(numbers=path_to_numbers_2,\noutput_location=path_for_output + \"/avg_2\"\n)\n# Note that this assumes the average_op can take multiple paths to numbers.  You could also have an\n# aggregation component that combines avg_1 and avg_2 into a single file of numbers\npaths_to_numbers = [\navg_1.output,\navg_2.output\n]\naverage_result_overall = average_op(path_to_numbers=paths_to_numbers,\noutput_location=path_for_output + \"/average_result_overall\"\n)\n</code></pre> <p>Within this platform, the primary method for persisting large files is through MinIO as described in our Storage documentation. Examples of this are also described in our example notebooks (also found in <code>jupyter-notebooks/self-serve-storage/</code> on any notebook server).</p>"},{"location":"3-Pipelines/Kubeflow-Pipelines/#typical-development-patterns","title":"Typical development patterns","text":""},{"location":"3-Pipelines/Kubeflow-Pipelines/#end-to-end-pipeline-development","title":"End-to-end pipeline development","text":"<p>A typical pattern for building pipelines in Kubeflow Pipelines is:</p> <ol> <li>Define components for each of your tasks</li> <li>Compose your components in a <code>@dsl.pipeline</code> decorated function</li> <li><code>compile()</code> your pipeline, upload your YAML files, and run</li> </ol> <p>This pattern lets you define portable components that can be individually tested before combining them into a full pipeline. Depending on the type and complexity of task, there are different methods for building the components.</p>"},{"location":"3-Pipelines/Kubeflow-Pipelines/#methods-for-authoring-components","title":"Methods for authoring components","text":"<p>Fundamentally, every component in Kubeflow Pipelines runs a container. Kubeflow Pipelines offers several methods to define these components with different levels of flexibility and complexity.</p>"},{"location":"3-Pipelines/Kubeflow-Pipelines/#user-defined-container-components","title":"User-defined container components","text":"<p>You can define tasks through custom Docker images. The design pattern for this is:</p> <ul> <li>Define (update) code for your task and commit to Git</li> <li>Build an image from your task (through manual command or CI pipeline)</li> <li>Test running this Docker image locally (and iterate if needed)</li> <li>Push the image to a container registry (usually Docker hub, but it will be   Azure Container Registry in our case on the Advanced Analytics Workspace)</li> <li>Update the Kubeflow Pipeline to point to the new image (via <code>dsl.ContainerOp</code>   like above) and test the pipeline</li> </ul> <p>This lets you run anything you can put into a Docker image as a task in Kubeflow Pipelines. You can manage and test your images and have complete control over how they run and what dependencies use. The <code>docker run</code> interface for each container becomes the API that Kubeflow Pipelines <code>dsl.ContainerOp</code> interacts with \u2013 running the containers is effectively like running them locally using a terminal. Anything you can make into a container with that interface can be run in Kubeflow Pipelines.</p> <p>!!! danger \"...however, for security reasons the platform currently does not     allow users to build/run custom Docker images. This is planned for the     future, but in interim see Lightweight components for a way to develop     pipelines without custom images\"</p>"},{"location":"3-Pipelines/Kubeflow-Pipelines/#lightweight-python-components","title":"Lightweight Python components","text":"<p>While full custom containers offer great flexibility, sometimes they're heavier than needed. The Kubeflow Pipelines SDK also allows for Lightweight Python Components, which are components that can be built straight from Python without building new container images for each change. These components are great for fast iteration during development, as well as for simple tasks that can be written and managed easily.</p> <p>This is an example of a lightweight pipeline with a single component that concatenates strings:</p> <pre><code>import kfp\nfrom kfp import dsl\nfrom kfp.components import func_to_container_op\ndef concat_string(a, b) -&gt; str:\nreturn f\"({a} | {b})\"\nconcat_string_component = func_to_container_op(concat_string,\nbase_image=\"python:3.8.3-buster\"\n)\n@dsl.pipeline(\nname=\"My lightweight pipeline\",\n)\ndef pipeline(str1, str2, str3):\n# Note that we use the concat_string_component, not the\n# original concat_string() function\nconcat_result_1 = concat_string_component(str1, str2)\n# By using cancat_result_1's output, we define the dependency of\n# concat_result_2 on concat_result_1\nconcat_result_2 = concat_string_component(concat_result_1.output, str3)\n</code></pre> <p>We see that our <code>concat_string</code> component is defined directly in Python rather than from a Docker image. In the end, our function still runs in a container, but we don't have to built it ourselves: <code>func_to_container_op()</code> runs our Python code inside the provided base image (<code>python:3.8.3-buster</code>). This lets use avoid building every time we change our code. The base image can be anything accessible by Kubeflow, which includes all images in the Azure Container Registry and any whitelisted images from Docker hub.</p> Lightweight components have a number of advantages but also some drawbacks <p>See this description of their basic characteristics, as well as this example which uses them in a more complex pipeline</p> A convenient base image to use is the the image your notebook server is running <p>By using the same image as your notebook server, you ensure Kubeflow Pipelines has the same packages available to it as the notebook where you do your analysis.  This can help avoid errors from importing packages specific to your environment. You can find that link from the notebook server page as shown below, but make sure you prepend the registry URL (so the below image would have <code>base_image=k8scc01covidacr.azurecr.io/machine-learning-notebook-cpu:562fa4a2899eeb9ae345c51c2491447ec31a87d7</code> ).  Note that while using a fully featured base image for iteration is fine, it's good practice to keep production pipelines lean and only supply the necessary software.  That way you reduce the startup time for each step in your pipeline. </p>"},{"location":"3-Pipelines/Kubeflow-Pipelines/#defining-components-directly-in-yaml","title":"Defining components directly in YAML","text":"<p>Components can be defined directly with a YAML file, where the designer can run terminal commands from a given Docker image. This can be a great way to make non-Python pipeline components from existing containers. As with all components, we can pass both arguments and data files into/out of the component. For example:</p> <pre><code>name: Concat Strings\ninputs:\n- {\n      name: Input text 1,\n      type: String,\n      description: \"Some text to echo into a terminal and tee to a file\",\n}\n- {\n      name: Input text 2,\n      type: String,\n      description: \"Some text to echo into a terminal and tee to a file\",\n}\noutputs:\n- { name: Output filename, type: String }\nimplementation:\ncontainer:\nimage: bash:5\ncommand:\n- bash\n- -ex\n- -c\n- |\necho \"$0 | $1\" | tee $2\n- { inputValue: Input text 1 }\n- { inputValue: Input text 2 }\n- { outputPath: Output filename }\n</code></pre> <p>This example concatenates two strings like our lightweight example above. We then define a component in python from this YAML:</p> <pre><code>from kfp.components import load_component_from_file\necho_and_tee = load_component_from_file('path/to/echo_and_tee.yaml')\n@dsl.pipeline\ndef my_pipeline():\necho_and_tee_task_1 = echo_and_tee(\"My text to echo\")\n# A second use that consumes the return of the first one\necho_and_tee_task_2 = echo_and_tee(echo_and_tee_task_1.output)\n</code></pre> <p>See this example for more details on using existing components.</p>"},{"location":"3-Pipelines/Kubeflow-Pipelines/#reusing-existing-components","title":"Reusing existing components","text":"<p>Similar to well abstracted functions, well abstracted components can reduce the amount of code you have to write for any given project. For example, rather than teaching your machine learning <code>train_model</code> component to also save the resulting model to MinIO, you can instead have <code>train_model</code> return the model and then Kubeflow Pipelines can pass the model to a reusable <code>copy_to_minio</code> component. This reuse pattern applies to components defined through any means (containers, lightweight, or YAML). Take a look at our example notebook, which reuses provided components for simple file IO tasks.</p>"},{"location":"3-Pipelines/PaaS/","title":"Overview","text":""},{"location":"3-Pipelines/PaaS/#integrate-with-platforms-like-databricks-and-azureml","title":"Integrate with Platforms like Databricks and AzureML","text":"<p>The AAW platform is built around the idea of integrations, and so we can integrate with many Platform as a Service (PaaS) offerings, such as Azure ML and Databricks.</p> <p>See some examples on our \"MLOps\" github Repo.</p> <p></p>"},{"location":"3-Pipelines/PaaS/#setup","title":"Setup","text":"<p>If you need help integrating with a platform as a service offering, we're happy to help!</p>"},{"location":"3-Pipelines/Serving/","title":"Model Serving with Seldon Core and KFServing","text":"<p>\u2692 This page is under construction \u2692</p> <p>The person writing this entry does not know enough about  this feature to write about it, but you can ask on our Slack channel.</p>"},{"location":"3-Pipelines/Serving/#serverless-with-knative","title":"Serverless with KNative","text":"<p>Kubernetes and KNative let your services scale up and down on demand. This lets you create APIs to serve Machine Learning models, without the need to manage load balancing or scale-up. The platform can handle all of your scaling for you, so that you can focus on the program logic.</p> <p>\u2692 This page is under construction \u2692</p> <p>The person writing this entry does not know enough about this  feature to write about it, but you can ask on our Slack channel.</p>"},{"location":"4-Collaboration/Geospatial-Analytical-Environment/","title":"Geospatial Analytical Environment (GAE) - Cross Platform Access","text":"Unprotected data only; SSI coming soon: <p>At this time, our Geospatial server can only host and provide access to non-sensitive statistical information.  </p>"},{"location":"4-Collaboration/Geospatial-Analytical-Environment/#getting-started","title":"Getting Started","text":"Prerequisites <ol> <li>An onboarded project with access to DAS GAE ArcGIS Portal    </li> <li>An ArcGIS Portal Client Id (API Key)</li> </ol> <p>The ArcGIS Enterprise Portal can be accessed in either the AAW or CAE using the API, from any service which leverages the Python programming language. </p> <p>For example, in AAW and the use of Jupyter Notebooks within the space, or in CAE the use of Databricks, DataFactory, etc.</p> <p>The DAS GAE ArcGIS Enterprise Portal can be accessed directly here</p> <p>For help with self-registering as a DAS Geospatial Portal user</p>"},{"location":"4-Collaboration/Geospatial-Analytical-Environment/#using-the-arcgis-api-for-python","title":"Using the ArcGIS API for Python","text":""},{"location":"4-Collaboration/Geospatial-Analytical-Environment/#connecting-to-arcgis-enterprise-portal-using-arcgis-api","title":"Connecting to ArcGIS Enterprise Portal using ArcGIS API","text":"<ol> <li> <p>Install packages:</p> <pre><code>conda install -c esri arcgis\n</code></pre> <p>or using Artifactory</p> <pre><code>conda install -c https://jfrog.aaw.cloud.statcan.ca/artifactory/api/conda/esri-remote arcgis\n</code></pre> </li> <li> <p>Import the necessary libraries that you will need in the Notebook.     <pre><code>from arcgis.gis import GIS\nfrom arcgis.gis import Item\n</code></pre></p> </li> <li> <p>Access the Portal     Your project group will be provided with a Client ID upon onboarding. Paste the Client ID in between the quotations <code>client_id='######'</code>. </p> <pre><code>gis = GIS(\"https://geoanalytics.cloud.statcan.ca/portal\", client_id=' ')\nprint(\"Successfully logged in as: \" + gis.properties.user.username)\n</code></pre> </li> <li> <ul> <li> <p>The output will redirect you to a login Portal.     - Use the StatCan Azure Login option, and your Cloud ID      - After successful login, you will receive a code to sign in using SAML.      - Paste this code into the output. </p> <p></p> </li> </ul> </li> </ol>"},{"location":"4-Collaboration/Geospatial-Analytical-Environment/#display-user-information","title":"Display user information","text":"<p>Using the 'me' function, we can display various information about the user logged in. <pre><code>me = gis.users.me\nusername = me.username\ndescription = me.description\ndisplay(me)\n</code></pre></p>"},{"location":"4-Collaboration/Geospatial-Analytical-Environment/#search-for-content","title":"Search for Content","text":"<p>Search for the content you have hosted on the DAaaS Geo Portal. Using the 'me' function we can search for all of the hosted content on the account. There are multiple ways to search for content. Two different methods are outlined below.</p> <p>Search all of your hosted items in the DAaaS Geo Portal.</p> <p><pre><code>my_content = me.items()\nmy_content\n</code></pre> Search for specific content you own in the DAaaS Geo Portal.</p> <p>This is similar to the example above, however if you know the title of they layer you want to use, you can save it as a function.</p> <pre><code>my_items = me.items()\nfor items in my_items:\nprint(items.title, \" | \", items.type)\nif items.title == \"Flood in Sorel-Tracy\":\nflood_item = items\nelse:\ncontinue\nprint(flood_item)\n</code></pre> <p>Search all content you have access to, not just your own.</p> <pre><code>flood_item = gis.content.search(\"tags: flood\", item_type =\"Feature Service\")\nflood_item\n</code></pre>"},{"location":"4-Collaboration/Geospatial-Analytical-Environment/#get-content","title":"Get Content","text":"<p>We need to get the item from the DAaaS Geo Portal in order to use it in the Jupyter Notebook. This is done by providing the unique identification number of the item you want to use. Three examples are outlined below, all accessing the identical layer.</p> <pre><code>item1 = gis.content.get(my_content[5].id) #from searching your content above\ndisplay(item1)\nitem2 = gis.content.get(flood_item.id) #from example above -searching for specific content\ndisplay(item2)\nitem3 = gis.content.get('edebfe03764b497f90cda5f0bfe727e2') #the actual content id number\ndisplay(item3)\n</code></pre>"},{"location":"4-Collaboration/Geospatial-Analytical-Environment/#perform-analysis","title":"Perform Analysis","text":"<p>Once the layers are brought into the Jupyter notebook, we are able to perform similar types of analysis you would expect to find in a GIS software such as ArcGIS. There are many modules containing many sub-modules of which can perform multiple types of analyses. </p> <p>Using the <code>arcgis.features</code> module, import the use_proximity submodule <code>from arcgis.features import use_proximity</code>. This submodule allows us to '.create_buffers' - areas of equal distance from features. Here, we specify the layer we want to use, distance, units, and output name (you may also specify other characteristics such as field, ring type, end type, and others). By specifying an output name, after running the buffer command, a new layer will be automatically uploaded into the DAaaS GEO Portal containing the new feature you just created. </p> <pre><code>buffer_lyr = use_proximity.create_buffers(item1, distances=[1], \nunits = \"Kilometers\", \noutput_name='item1_buffer')\ndisplay(item1_buffer)\n</code></pre> <p>Some users prefer to work with open source packages.  Translating from ArcGIS to Spatial Dataframes is simple. <pre><code># create a Spatially Enabled DataFrame object\nsdf = pd.DataFrame.spatial.from_layer(feature_layer)\n</code></pre></p>"},{"location":"4-Collaboration/Geospatial-Analytical-Environment/#update-items","title":"Update Items","text":"<p>By getting the item as we did similar to the example above, we can use the <code>.update</code> function to update existing item within the DAaaS GEO Portal. We can update item properties, data, thumbnails, and metadata. <pre><code>item1_buffer = gis.content.get('c60c7e57bdb846dnbd7c8226c80414d2')\nitem1_buffer.update(item_properties={'title': 'Enter Title'\n'tags': 'tag1, tag2, tag3, tag4',\n'description': 'Enter description of item'}\n</code></pre></p>"},{"location":"4-Collaboration/Geospatial-Analytical-Environment/#visualize-your-data-on-an-interactive-map","title":"Visualize Your Data on an Interactive Map","text":"<p>Example: MatplotLib Library In the code below, we create an ax object, which is a map style plot. We then plot our data ('Population Change') change column on the axes <pre><code>import matplotlib.pyplot as plt\nax = sdf.boundary.plot(figsize=(10, 5))\nshape.plot(ax=ax, column='Population Change', legend=True)\nplt.show()\n</code></pre></p> <p>Example: ipyleaflet Library In this example we will use the library <code>ipyleaflet</code> to create an interactive map. This map will be centered around Toronto, ON. The data being used will be outlined below. Begin by pasting <code>conda install -c conda-forge ipyleaflet</code> allowing you to install <code>ipyleaflet</code> libraries in the Python environment.</p> <p></p> <p>Import the necessary libraries.</p> <pre><code>import ipyleaflet \nfrom ipyleaflet import *\n</code></pre> <p>Now that we have imported the ipyleaflet module, we can create a simple map by specifying the latitude and longitude of the location we want, zoom level, and basemap (more basemaps). Extra controls have been added such as layers and scale.</p> <p><pre><code>toronto_map = Map(center=[43.69, -79.35], zoom=11, basemap=basemaps.Esri.WorldStreetMap)\ntoronto_map.add_control(LayersControl(position='topright'))\ntoronto_map.add_control(ScaleControl(position='bottomleft'))\ntoronto_map\n</code></pre> </p>"},{"location":"4-Collaboration/Geospatial-Analytical-Environment/#learn-more-about-the-arcgis-api-for-python","title":"Learn More about the ArcGIS API for Python","text":"<p>Full documentation for the ArcGIS API can be located here</p>"},{"location":"4-Collaboration/Geospatial-Analytical-Environment/#learn-more-about-das-geospatial-analytical-environment-gae-and-services","title":"Learn More about DAS Geospatial Analytical Environment (GAE) and Services","text":"<p>GAE Help Guide</p>"},{"location":"4-Collaboration/Overview/","title":"Overview","text":""},{"location":"4-Collaboration/Overview/#collaboration","title":"Collaboration","text":"<p>Collaboration is essential in data science because it allows individuals with different perspectives and backgrounds to work together to solve complex problems and generate new insights. In data science, collaboration can involve working with individuals from diverse fields such as mathematics, computer science, and business, as well as subject matter experts who have deep knowledge of a particular industry or domain.</p> <p>There are many ways collaborate on the AAW. Which is best for your situation depends on what you're sharing and how many people you want to share with. Content to be shared breaks roughly into Data, Code, or Compute Environments (e.g.: sharing the same virtual machines) and who you want to share it with (No one, My Team, or Everyone). This leads to the following table of options</p> Private Team StatCan Code GitLab/GitHub or personal folder GitLab/GitHub or team folder GitLab/GitHub Data Personal folder or bucket Team folder or bucket, or shared namespace Shared Bucket Compute Personal namespace Shared namespace N/A <p>Sharing code, disks, and workspaces (e.g.: two people sharing the same virtual machine) is described in more detail below. Sharing data through buckets is described in more detail in the MinIO section.</p> What is the difference between a bucket and a folder? <p>Buckets are like Network Storage. See the Storage overview for more discussion of the differences between these two ideas.</p> <p>Choosing the best way to share code, data, and compute all involve different factors, but you can generally mix and match (share code with your team through Github, but store your data privately in a personal bucket). These cases are described more in the below sections.</p>"},{"location":"4-Collaboration/Overview/#share-code-among-team-members","title":"Share code among team members","text":"<p>In most cases, it is easiest to share code using GitHub or GitLab to share code. The advantage of sharing with GitHub or GitLab is that it works with users across namespaces, and keeping code in git is a great way to manage large software projects.</p> Don't forget to include a License! <p>If your code is public, do not forget to keep with the Innovation Team's guidelines and use a proper License if your work is done for Statistics Canada.</p> <p>If you need to share code without publishing it on a repository, sharing a namespace might work as well.</p>"},{"location":"4-Collaboration/Overview/#share-compute-namespace-in-kubeflow","title":"Share compute (namespace) in Kubeflow","text":"<p>Sharing a namespace means you share everything in the namespace</p> <p>Kubeflow does not support granular sharing of one resource (one notebook, one MinIO bucket, etc.), but instead sharing of all resources. If you want to share a Jupyter Notebook server with someone, you must share your entire namespace and they will have access to all other resources (MinIO buckets, etc.).</p> <p>In Kubeflow every user has a namespace that contains their work (their notebook servers, pipelines, disks, etc.). Your namespace belongs to you, but can be shared if you want to collaborate with others. You can also request a new namespace (either for yourself or to share with a team). One option for collaboration is to share namespaces with others.</p> <p>The advantage of sharing a Kubeflow namespace is that it lets you and your colleagues share the compute environment and MinIO buckets associated with the namespace. This makes it a very easy and free-form way to share.</p> <p>To share your namespace, see managing contributors</p> Ask for help in production <p>The Advanced Analytics Workspace support staff are happy to help with production oriented use cases, and we can probably save you lots of time. Don't be shy about asking us for help!</p>"},{"location":"4-Collaboration/Overview/#share-data","title":"Share data","text":"<p>Once you have a shared namespace, you have two shared storage approaches</p> Storage Option Benefits Shared Jupyter Servers/Workspaces More amenable to small files, notebooks, and little experiments. Shared Buckets (see Storage) Better suited for use in pipelines, APIs, and for large files. <p>To learn more about the technology behind these, check out the Storage overview.</p>"},{"location":"4-Collaboration/Overview/#sharing-with-statcan","title":"Sharing with StatCan","text":"<p>In addition to private buckets, or team-shared private buckets, you can also place your files in shared storage. Within all bucket storage options (<code>minimal</code>, <code>premium</code>, <code>pachyderm</code>), you have a private bucket, and a folder inside of the <code>shared</code> bucket. Take a look, for instance, at the link below:</p> <ul> <li><code>shared/blair-drummond/</code></li> </ul> <p>Any logged in user can see these files and read them freely.</p>"},{"location":"4-Collaboration/Overview/#sharing-with-the-world","title":"Sharing with the world","text":"<p>Ask about that one in our Slack channel. There are many ways to do this from the IT side, but it's important for it to go through proper processes, so this is not done in a \"self-serve\" way that the others are. That said, it is totally possible.</p>"},{"location":"4-Collaboration/Overview/#recommendation-combine-them-all","title":"Recommendation: Combine them all","text":"<p>It's a great idea to always use git, and using git along with shared workspaces is a great way to combine ad hoc sharing (through files) while also keeping your code organized and tracked.</p>"},{"location":"4-Collaboration/Overview/#managing-contributors","title":"Managing contributors","text":"<p>You can add or remove people from a namespace you already own through the Manage Contributors menu in Kubeflow.</p> <p></p> <p>Now you and your colleagues can share access to a server!</p> <p>Try it out!</p>"},{"location":"4-Collaboration/Request-a-Namespace/","title":"Overview","text":"<p>By default, everyone gets their own personal namespace, <code>firstname-lastname</code>. If you want to collaborate with your team, you can request a new namespace to share.</p>"},{"location":"4-Collaboration/Request-a-Namespace/#setup","title":"Setup","text":""},{"location":"4-Collaboration/Request-a-Namespace/#requesting-a-namespace","title":"Requesting a namespace","text":"<p>To create a namespace for a team, go to the AAW portal. Click the \u22ee menu on the Kubeflow section of the portal.</p> <p></p> <p>Enter the name you are requesting and submit the request. Be sure to use only lower case letters plus dashes. </p> <p>The namespace cannot have special characters other than hyphens</p> <p>The namespace name must only be lower-case letters <code>a-z</code> with dashes. Otherwise, the namespace will not be created.</p> <p>You will receive an email notification when the namespace is created. Once the shared namespace is created, you can access it the same as any other namespace you have through the Kubeflow UI, like shown below. You will then be able to share and manage to your namespace.</p> <p>To switch namespaces, take a look at the top of your window, just to the right of the Kubeflow Logo.</p> <p></p>"},{"location":"5-Storage/AzureBlobStorage/","title":"Overview","text":"<p>Azure Blob Storage is Microsoft's object storage solution for the cloud. Blob Storage is optimized for storing massive amounts of unstructured data. Unstructured data is data that doesn't adhere to a particular data model or definition, such as text or binary data. Azure Blob Storage Containers are good at three things:</p> <ul> <li>Large amounts of data - Containers can be huge: way bigger than hard drives. And   they are still fast.</li> <li>Accessible by multiple consumers at once - You can access the same data source   from multiple Notebook Servers and pipelines at the same time without needing   to duplicate the data.</li> <li>Sharing - Project namespaces can share a container. This is great for sharing data with people   outside of your workspace.</li> </ul>"},{"location":"5-Storage/AzureBlobStorage/#setup","title":"Setup","text":"<p>Azure Blob Storage containers and buckets mount will be replacing the Minio Buckets and Minio storage mounts</p> <p>Users will be responsible for migrating data from Minio Buckets to the Azure Storage folders. For larger files, users may contact AAW for assistance.</p>"},{"location":"5-Storage/AzureBlobStorage/#blob-container-mounted-on-a-notebook-server","title":"Blob Container Mounted on a Notebook Server","text":"<p>The Blob CSI volumes are persisted under <code>/home/jovyan/buckets</code> when creating a Notebook Server. Files under <code>/buckets</code> are backed by Blob storage. All AAW notebooks will have the <code>/buckets</code> mounted to the file-system, making data accessible from everywhere.</p> <p></p>"},{"location":"5-Storage/AzureBlobStorage/#unclassified-notebook-aaw-folder-mount","title":"Unclassified Notebook AAW folder mount","text":""},{"location":"5-Storage/AzureBlobStorage/#protected-b-notebook-aaw-folder-mount","title":"Protected-b Notebook AAW folder mount","text":"<p>These folders can be used like any other - you can copy files to/from using the file browser, write from Python/R, etc. The only difference is that the data is being stored in the Blob storage container rather than on a local disk (and is thus accessible wherever you can access your Kubeflow notebook).</p>"},{"location":"5-Storage/AzureBlobStorage/#container-types","title":"Container Types","text":"<p>The following Blob containers are available:</p> <p>Accessing all Blob containers is the same. The difference between containers is the storage type behind them:</p> <ul> <li>aaw-unclassified: By default,   use this one. Stores unclassified data.</li> </ul> <ul> <li>aaw-protected-b: Stores sensitive data protected-b.</li> </ul> <ul> <li>aaw-unclassified-ro: This classification is protected-b but read-only access. This is so users can view unclassified data within a protected-b notebook.</li> </ul>"},{"location":"5-Storage/AzureBlobStorage/#accessing-internal-data","title":"Accessing Internal Data","text":"<p>TBA: Awaiting DAS common storage connection</p> <p>AAW has an integration with the FAIR Data Infrastructure team that allows users to transfer unclassified and protected-b data to Azure Storage Accounts, thus allowing users to access this data from Notebook Servers.</p> <p>Please reach out to the FAIR Data Infrastructure team if you have a use case for this data.</p>"},{"location":"5-Storage/AzureBlobStorage/#pricing","title":"Pricing","text":"Pricing models are based on CPU and Memory usage <p>Pricing is covered by KubeCost for user namespaces (In Kubeflow at the bottom of the Notebooks tab).</p> <p>In general, Blob Storage is much cheaper than Azure Manage Disks and has better I/O than managed SSD.</p>"},{"location":"5-Storage/Disks/","title":"Overview","text":"<p>Disks are the familiar hard drive style file systems you're used to, provided to you from fast solid state drives (SSDs)!</p>"},{"location":"5-Storage/Disks/#setup","title":"Setup","text":"<p>When creating your notebook server, you request disks by adding Data Volumes to your notebook server (pictured below, with <code>Type = New</code>). They are automatically mounted at the directory (<code>Mount Point</code>) you choose, and serve as a simple and reliable way to preserve data attached to a Notebook Server.</p> <p></p> You pay for all disks you own, whether they're attached to a Notebook Server or not <p>As soon as you create a disk, you're paying for it until it is deleted, even if it's original Notebook Server is deleted.  See Deleting Disk Storage for more info</p>"},{"location":"5-Storage/Disks/#once-youve-got-the-basics","title":"Once you've got the basics ...","text":"<p>When you delete your Notebook Server, your disks are not deleted. This let's you reuse that same disk (with all its contents) on a new Notebook Server later (as shown above with <code>Type = Existing</code> and the <code>Name</code> set to the volume you want to reuse). If you're done with the disk and it's contents, delete it.</p>"},{"location":"5-Storage/Disks/#deleting-disk-storage","title":"Deleting Disk Storage","text":"<p>To see your disks, check the Notebook Volumes section of the Notebook Server page (shown below). You can delete any unattached disk (orange icon on the left) by clicking the trash can icon.</p> <p></p>"},{"location":"5-Storage/Disks/#pricing","title":"Pricing","text":"Pricing models are tentative and may change <p>As of writing, pricing is covered by the platform for initial users.  This guidance explains how things are expected to be priced priced in future, but this may change.</p> <p>When mounting a disk, you get an Azure Managed Disk. The Premium SSD Managed Disks pricing shows the cost per disk based on size. Note that you pay for the size of disk requested, not the amount of space you are currently using.</p> Tips to minimize costs <p>As disks can be attached to a Notebook Server and reused, a typical usage pattern could be:</p> <ul> <li>At 9AM, create a Notebook Server (request 2CPU/8GB RAM and a 32GB attached   disk)</li> <li>Do work throughout the day, saving results to the attached disk</li> <li>At 5PM, shut down your Notebook Server to avoid paying for it overnight<ul> <li>NOTE: The attached disk is not destroyed by this action</li> </ul> </li> <li>At 9AM the next day, create a new Notebook Server and attach your existing   disk</li> <li>Continue your work...</li> </ul> <p>This keeps all your work safe without paying for the computer when you're not using it</p>"},{"location":"5-Storage/Overview/","title":"Storage","text":"<p>The platform provides several types of storage:</p> <ul> <li>Disk (also called Volumes on the Notebook Server creation screen)</li> <li>Containers (Azure Blob Storage)</li> <li>Data Lakes (coming soon)</li> </ul> <p>Depending on your use case, either disk or bucket may be most suitable:</p> Type Simultaneous Users Speed Total size Shareable with Other Users Disk One machine/notebook server at a time Fastest (throughput and latency) &lt;=512GB total per drive No Container (via Azure Blob Storage) Simultaneous access from many machines/notebook servers at the same time Fast-ish (Fast download, modest upload, modest latency) Infinite (within reason) [Yes] If you're unsure which to choose, don't sweat it <p>These are guidelines, not an exact science - pick what sounds best now and run with it.  The best choice for a complicated usage is non-obvious and often takes hands-on experience, so just trying something will help.  For most situations both options work well even if they're not perfect, and remember that data can always be copied later if you change your mind.</p>"},{"location":"6-Gitlab/Gitlab/","title":"Gitlab","text":""},{"location":"6-Gitlab/Gitlab/#important-notes","title":"IMPORTANT NOTES","text":"<p>1) Please do NOT store your token anywhere in your workspace server file system. Contributors to a namespace will have access to them. 2) If there is a contributor external to Statistics Canada in your namespace, you will lose access to cloud main GitLab access!</p> <p>Thankfully, using the cloud main GitLab on the AAW is just like how you would regularly use git. </p>"},{"location":"6-Gitlab/Gitlab/#step-1-locate-the-git-repo-you-want-to-clone-and-copy-the-clone-with-https-option","title":"Step 1: Locate the Git repo you want to clone and copy the clone with HTTPS option","text":"<p>If your repository is private, you will need to also do Step 4 (Creating a Personal Access Token) for this to go through.  For me this was a test repo  </p>"},{"location":"6-Gitlab/Gitlab/#step-2-paste-the-copied-link-into-one-of-your-workspace-servers","title":"Step 2: Paste the copied link into one of your workspace servers","text":""},{"location":"6-Gitlab/Gitlab/#step-3-success","title":"Step 3: Success!","text":"<p>As seen in the above screenshot I have cloned the repo!</p>"},{"location":"6-Gitlab/Gitlab/#step-4-create-a-personal-access-token-for-pushing-also-used-if-pulling-from-a-private-repository","title":"Step 4: Create a Personal Access Token for pushing (also used if pulling from a private repository)","text":"<p>If you try to <code>git push ....</code> you will encounter an error eventually leading you to the GitLab help documentation</p> <p>You will need to make a Personal Access Token for this. To achieve this go in GitLab, click your profile icon and then hit <code>Preferences</code> and then <code>Access Tokens</code>  Follow the prompts entering the name, the token expiration date and granting the token permissions (I granted <code>write_repository</code>)</p>"},{"location":"6-Gitlab/Gitlab/#step-5-personalize-git-to-be-you","title":"Step 5: Personalize <code>Git</code> to be you","text":"<p>Run <code>git config user.email ....</code> and <code>git config user.name ...</code> to match your GitLab identity.</p>"},{"location":"6-Gitlab/Gitlab/#step-6-supply-the-generated-token-when-asked-for-your-password","title":"Step 6: Supply the Generated Token when asked for your password","text":"<p>The token will by copy-able at the top once you hit <code>Create personal access token</code> at the bottom </p> <p>Once you have prepared everything it's time </p>"},{"location":"6-Gitlab/Gitlab/#step-7-see-the-results-of-your-hard-work-in-gitlab","title":"Step 7: See the results of your hard work in GitLab","text":""}]}