{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Starting on the Advanced Analytics Workspace \u00b6 The Advanced Analytics Workspace portal is a great place to discover and connect to the available resources we'll be talking about here. What are you looking for? \u00b6 Get Started with AAW \u00b6 Everything starts with Kubeflow ! Start by setting it up. You're going to have questions. Join our Slack channel so we can get you answers! Click on the link, then choose \"Create an account\" in the upper right-hand corner. Use your @statcan.gc.ca email address so that you will be automatically approved. Experiments \u00b6 Process data using R , Python , or Julia \u00b6 Once you have Kubeflow set up, use Jupyter Notebooks to create and share documents that contain live code, equations, or visualizations. Process data using 'R' or 'Python' \u00b6 R Studio gives you an integrated development environment for R and Python. Use the r-studio-cpu image to get an R Studio environment. Run a virtual desktop \u00b6 You can run a full Ubuntu desktop, with typical applications, right inside your browser, using ML Workspaces Publishing \u00b6 Build and publish an interactive dashboard \u00b6 Use R-Shiny to build interactive web apps straight from R. You can deploy your R-Shiny dashboard by submitting a pull request to our R-Dashboards GitHub repository . Dash is a data visualization tool that lets you build an interactive GUI around your data analysis code. Explore your data \u00b6 Use Datasette , an instant JSON API for your SQLite databases. Run SQL queries in a more interactive way! Pipelines \u00b6 Build and schedule data/analysis pipelines \u00b6 Kubeflow Pipelines allows you to set up pipelines. Each pipeline encapsulates analytical workflows, and can be shared, reused, and scheduled. Integrate with Platform as a Service (PaaS) offerings \u00b6 We can integrate with many Platform as a Service (PaaS) offerings, like Databricks or AzureML. Collaboration \u00b6 There are many ways collaborate on the platform. Which is best for your situation depends on what you're sharing and how many people you want to share with. See the Collaboration Overview for details. Content to be shared breaks roughly into Data , Code , or Compute Environments (e.g.: sharing the same virtual machines) and who you want to share it with ( No one , My Team , or Everyone ). This leads to the following table of options Private Team StatCan Code GitLab/GitHub or personal folder GitLab/GitHub or team folder GitLab/GitHub Data Personal folder or bucket Team folder or bucket, or shared namespace Shared Bucket Compute Personal namespace Shared namespace N/A What is the difference between a bucket and a folder? Buckets are like Network Storage. See the Storage section section for more discussion of the differences between these two ideas. Sharing code, disks, and workspaces (e.g.: two people sharing the same virtual machine) is described in more detail in the Collaboration section. Sharing data through buckets is described in more detail in the Azure Blob Storage section. Storage \u00b6 The platform provides several types of storage: Disk (also called Volumes on the Notebook Server creation screen) Containers (Blob Storage) Data Lakes (coming soon) Depending on your use case, either disk or bucket may be most suitable. Our storage overview will help you compare them. Disks \u00b6 Disks are added to your notebook server by adding Data Volumes.","title":"Getting Started"},{"location":"#starting-on-the-advanced-analytics-workspace","text":"The Advanced Analytics Workspace portal is a great place to discover and connect to the available resources we'll be talking about here.","title":"Starting on the Advanced Analytics Workspace"},{"location":"#what-are-you-looking-for","text":"","title":"What are you looking for?"},{"location":"#get-started-with-aaw","text":"Everything starts with Kubeflow ! Start by setting it up. You're going to have questions. Join our Slack channel so we can get you answers! Click on the link, then choose \"Create an account\" in the upper right-hand corner. Use your @statcan.gc.ca email address so that you will be automatically approved.","title":"Get Started with AAW"},{"location":"#experiments","text":"","title":"Experiments"},{"location":"#process-data-using-r-python-or-julia","text":"Once you have Kubeflow set up, use Jupyter Notebooks to create and share documents that contain live code, equations, or visualizations.","title":"Process data using R, Python, or Julia"},{"location":"#process-data-using-r-or-python","text":"R Studio gives you an integrated development environment for R and Python. Use the r-studio-cpu image to get an R Studio environment.","title":"Process data using 'R' or 'Python'"},{"location":"#run-a-virtual-desktop","text":"You can run a full Ubuntu desktop, with typical applications, right inside your browser, using ML Workspaces","title":"Run a virtual desktop"},{"location":"#publishing","text":"","title":"Publishing"},{"location":"#build-and-publish-an-interactive-dashboard","text":"Use R-Shiny to build interactive web apps straight from R. You can deploy your R-Shiny dashboard by submitting a pull request to our R-Dashboards GitHub repository . Dash is a data visualization tool that lets you build an interactive GUI around your data analysis code.","title":"Build and publish an interactive dashboard"},{"location":"#explore-your-data","text":"Use Datasette , an instant JSON API for your SQLite databases. Run SQL queries in a more interactive way!","title":"Explore your data"},{"location":"#pipelines","text":"","title":"Pipelines"},{"location":"#build-and-schedule-dataanalysis-pipelines","text":"Kubeflow Pipelines allows you to set up pipelines. Each pipeline encapsulates analytical workflows, and can be shared, reused, and scheduled.","title":"Build and schedule data/analysis pipelines"},{"location":"#integrate-with-platform-as-a-service-paas-offerings","text":"We can integrate with many Platform as a Service (PaaS) offerings, like Databricks or AzureML.","title":"Integrate with Platform as a Service (PaaS) offerings"},{"location":"#collaboration","text":"There are many ways collaborate on the platform. Which is best for your situation depends on what you're sharing and how many people you want to share with. See the Collaboration Overview for details. Content to be shared breaks roughly into Data , Code , or Compute Environments (e.g.: sharing the same virtual machines) and who you want to share it with ( No one , My Team , or Everyone ). This leads to the following table of options Private Team StatCan Code GitLab/GitHub or personal folder GitLab/GitHub or team folder GitLab/GitHub Data Personal folder or bucket Team folder or bucket, or shared namespace Shared Bucket Compute Personal namespace Shared namespace N/A What is the difference between a bucket and a folder? Buckets are like Network Storage. See the Storage section section for more discussion of the differences between these two ideas. Sharing code, disks, and workspaces (e.g.: two people sharing the same virtual machine) is described in more detail in the Collaboration section. Sharing data through buckets is described in more detail in the Azure Blob Storage section.","title":"Collaboration"},{"location":"#storage","text":"The platform provides several types of storage: Disk (also called Volumes on the Notebook Server creation screen) Containers (Blob Storage) Data Lakes (coming soon) Depending on your use case, either disk or bucket may be most suitable. Our storage overview will help you compare them.","title":"Storage"},{"location":"#disks","text":"Disks are added to your notebook server by adding Data Volumes.","title":"Disks"},{"location":"Help/","text":"Have questions? Or feedback? \u00b6 Come join us on the Advanced Analytics Workspace Slack channel ! You will find a bunch of other users of the platform there who may be able to answer your questions, and some of the engineers will usually be present on the channel. You can ask questions and provide feedback there. Slack (en) We will also post notices there if there are updates or downtime. Video tutorials \u00b6 After you have joined our Slack community, go and check out the following tutorials: Platform official Community driven content GitHub \u00b6 Want to know even more about our platform? Find everything about it on our GitHub page. Advanced Analytics Workspace on GitHub","title":"Help/Contact"},{"location":"Help/#have-questions-or-feedback","text":"Come join us on the Advanced Analytics Workspace Slack channel ! You will find a bunch of other users of the platform there who may be able to answer your questions, and some of the engineers will usually be present on the channel. You can ask questions and provide feedback there. Slack (en) We will also post notices there if there are updates or downtime.","title":"Have questions? Or feedback?"},{"location":"Help/#video-tutorials","text":"After you have joined our Slack community, go and check out the following tutorials: Platform official Community driven content","title":"Video tutorials"},{"location":"Help/#github","text":"Want to know even more about our platform? Find everything about it on our GitHub page. Advanced Analytics Workspace on GitHub","title":"GitHub"},{"location":"welcome-message/","text":"\ud83e\uddd9\ud83d\udd2e Welcome to Advanced Analytics Workspace (AAW) \u00b6 Please find below additional information, videos and links to help better understand how to get started with Advanced Analytics Workspace (AAW). Advanced Analytics Workspace (AAW) is our open source platform for data science and machine learning (ML) for advanced practitioners to get their work done in an unrestricted environment made by data scientists for data scientists. With AAW, you can customize your notebook deployments to suit your data science needs. We also have a small number of expertly crafted images made by our expert data science team. AAW is based on the Kubeflow project which is an open source comprehensive solution for deploying and managing end-to-end ML workflows. Kubeflow is designed to make deployments of ML workflows on Kubernetes simple, portable and scalable. \ud83d\udd14 Important! Users external to Statistics Canada will require a cloud account granted access by the business sponsor. \ud83d\udd14 Important! Users internal to Statistics Canada can get started right away without any additional sign up procedures, just head to https://kubeflow.aaw.cloud.statcan.ca/ . \ud83d\udd17 Helpful Links \u00b6 \ud83d\udece\ufe0f AAW Services \u00b6 \ud83c\udf00 AAW Portal Homepage Internal Only https://www.statcan.gc.ca/data-analytics-service/aaw Internal/External https://analytics-platform.statcan.gc.ca/covid19 \ud83e\udd16 Kubeflow Dashboard https://kubeflow.aaw.cloud.statcan.ca/ \ud83d\udca1 Help \u00b6 \ud83d\udcd7 AAW Portal Documentation https://statcan.github.io/daaas/ \ud83d\udcd8 Kubeflow Documentation https://www.kubeflow.org/docs/ \ud83e\udd1d Slack Support Channel https://statcan-aaw.slack.com \ud83e\udded Getting Started \u00b6 In order to access the AAW services, you will need to: Login to Kubeflow with your StatCan guest cloud account. You will be prompted to authenticate the account. Select Notebook Servers. Click the \"\u2795 New Server\" button. \ud83e\uddf0 Tools Offered \u00b6 AAW is a flexible platform for data analysis and machine learning, featuring: - \ud83d\udcdc Languages - \ud83d\udc0d Python - \ud83d\udcc8 R - \ud83d\udc69\u200d\ud83d\udd2c Julia - \ud83e\uddee Development environments - VS Code - R Studio - Jupyter Notebooks - \ud83d\udc27 Linux virtual desktops for additional tools (\ud83e\uddeb OpenM++, \ud83c\udf0f QGIS etc.) \ud83d\udc31 Demos \u00b6 If you would like a quick Onboarding Demo session or require any help or have any questions, please do not hesitate to reach out through our \ud83e\udd1d Slack Support Channel . FAQ \u00b6 Frequently Asked Questions are located here . Thank you!","title":"Welcome message"},{"location":"welcome-message/#welcome-to-advanced-analytics-workspace-aaw","text":"Please find below additional information, videos and links to help better understand how to get started with Advanced Analytics Workspace (AAW). Advanced Analytics Workspace (AAW) is our open source platform for data science and machine learning (ML) for advanced practitioners to get their work done in an unrestricted environment made by data scientists for data scientists. With AAW, you can customize your notebook deployments to suit your data science needs. We also have a small number of expertly crafted images made by our expert data science team. AAW is based on the Kubeflow project which is an open source comprehensive solution for deploying and managing end-to-end ML workflows. Kubeflow is designed to make deployments of ML workflows on Kubernetes simple, portable and scalable. \ud83d\udd14 Important! Users external to Statistics Canada will require a cloud account granted access by the business sponsor. \ud83d\udd14 Important! Users internal to Statistics Canada can get started right away without any additional sign up procedures, just head to https://kubeflow.aaw.cloud.statcan.ca/ .","title":"\ud83e\uddd9\ud83d\udd2e Welcome to Advanced Analytics Workspace (AAW)"},{"location":"welcome-message/#helpful-links","text":"","title":"\ud83d\udd17 Helpful Links"},{"location":"welcome-message/#aaw-services","text":"\ud83c\udf00 AAW Portal Homepage Internal Only https://www.statcan.gc.ca/data-analytics-service/aaw Internal/External https://analytics-platform.statcan.gc.ca/covid19 \ud83e\udd16 Kubeflow Dashboard https://kubeflow.aaw.cloud.statcan.ca/","title":"\ud83d\udece\ufe0f AAW Services"},{"location":"welcome-message/#help","text":"\ud83d\udcd7 AAW Portal Documentation https://statcan.github.io/daaas/ \ud83d\udcd8 Kubeflow Documentation https://www.kubeflow.org/docs/ \ud83e\udd1d Slack Support Channel https://statcan-aaw.slack.com","title":"\ud83d\udca1 Help"},{"location":"welcome-message/#getting-started","text":"In order to access the AAW services, you will need to: Login to Kubeflow with your StatCan guest cloud account. You will be prompted to authenticate the account. Select Notebook Servers. Click the \"\u2795 New Server\" button.","title":"\ud83e\udded Getting Started"},{"location":"welcome-message/#tools-offered","text":"AAW is a flexible platform for data analysis and machine learning, featuring: - \ud83d\udcdc Languages - \ud83d\udc0d Python - \ud83d\udcc8 R - \ud83d\udc69\u200d\ud83d\udd2c Julia - \ud83e\uddee Development environments - VS Code - R Studio - Jupyter Notebooks - \ud83d\udc27 Linux virtual desktops for additional tools (\ud83e\uddeb OpenM++, \ud83c\udf0f QGIS etc.)","title":"\ud83e\uddf0 Tools Offered"},{"location":"welcome-message/#demos","text":"If you would like a quick Onboarding Demo session or require any help or have any questions, please do not hesitate to reach out through our \ud83e\udd1d Slack Support Channel .","title":"\ud83d\udc31 Demos"},{"location":"welcome-message/#faq","text":"Frequently Asked Questions are located here . Thank you!","title":"FAQ"},{"location":"1-Experiments/Jupyter/","text":"Overview \u00b6 Jupyter: friendly R and Python experience \u00b6 Jupyter gives you notebooks to write your code and make visualizations. You can quickly iterate, visualize, and share your analyses. Because it's running on a server (that you set up in the Kubeflow section) you can do really big analyses on centralized hardware, adding as much horsepower as you need! And because it's on the cloud, you can share it with your colleagues too. Explore your data \u00b6 Jupyter comes with a number of features (and we can add more) Integrated visuals within your notebook Data volume for storing your data You can share your workspace with colleagues. IDE in the browser \u00b6 Create for exploring, and also great for writing code Linting and a debugger Git integration Built in Terminal Light/Dark theme (change settings at the top) More information on Jupyter here Setup \u00b6 Get started with the examples \u00b6 When you started your server, it got loaded with a bunch of example notebooks. Double click to open the jupyter-notebooks folder. Great notebooks to start with are R/01-R-Notebook-Demo.ipynb , or the notebooks in scikitlearn . pytorch and tensorflow are great if you are familiar with machine learning. The mapreduce-pipeline and ai-pipeline are more advanced. Some notebooks only work in certain server versions For instance, gdal is only in the geomatics image. So if you use another image then a notebook using gdal might not work. Adding software \u00b6 You do not have sudo in Jupyter, but you can use conda install --use-local your_package_name or pip install --user your_package_name Don't forget to restart your Jupyter kernel afterwards, to make new packages available. Make sure to restart the Jupyter kernel after installing new software If you install software in a terminal, but your Jupyter kernel was already running, then it will not be updated. Is there something that you can't install? If you need something installed, reach us or open a GitHub issue . We can add it to the default software. Once you've got the basics ... \u00b6 Getting Data in and out of Jupyter \u00b6 You can upload and download data to/from JupyterHub directly in the menu. There is an upload button at the top, and you can right-click most files or folders to download them. Shareable \"Bucket\" storage \u00b6 There is also a mounted minio folder in your home directory, which holds files in MinIO . Refer to the Storage section for details.","title":"Jupyter"},{"location":"1-Experiments/Jupyter/#overview","text":"","title":"Overview"},{"location":"1-Experiments/Jupyter/#jupyter-friendly-r-and-python-experience","text":"Jupyter gives you notebooks to write your code and make visualizations. You can quickly iterate, visualize, and share your analyses. Because it's running on a server (that you set up in the Kubeflow section) you can do really big analyses on centralized hardware, adding as much horsepower as you need! And because it's on the cloud, you can share it with your colleagues too.","title":"Jupyter: friendly R and Python experience"},{"location":"1-Experiments/Jupyter/#explore-your-data","text":"Jupyter comes with a number of features (and we can add more) Integrated visuals within your notebook Data volume for storing your data You can share your workspace with colleagues.","title":"Explore your data"},{"location":"1-Experiments/Jupyter/#ide-in-the-browser","text":"Create for exploring, and also great for writing code Linting and a debugger Git integration Built in Terminal Light/Dark theme (change settings at the top) More information on Jupyter here","title":"IDE in the browser"},{"location":"1-Experiments/Jupyter/#setup","text":"","title":"Setup"},{"location":"1-Experiments/Jupyter/#get-started-with-the-examples","text":"When you started your server, it got loaded with a bunch of example notebooks. Double click to open the jupyter-notebooks folder. Great notebooks to start with are R/01-R-Notebook-Demo.ipynb , or the notebooks in scikitlearn . pytorch and tensorflow are great if you are familiar with machine learning. The mapreduce-pipeline and ai-pipeline are more advanced. Some notebooks only work in certain server versions For instance, gdal is only in the geomatics image. So if you use another image then a notebook using gdal might not work.","title":"Get started with the examples"},{"location":"1-Experiments/Jupyter/#adding-software","text":"You do not have sudo in Jupyter, but you can use conda install --use-local your_package_name or pip install --user your_package_name Don't forget to restart your Jupyter kernel afterwards, to make new packages available. Make sure to restart the Jupyter kernel after installing new software If you install software in a terminal, but your Jupyter kernel was already running, then it will not be updated. Is there something that you can't install? If you need something installed, reach us or open a GitHub issue . We can add it to the default software.","title":"Adding software"},{"location":"1-Experiments/Jupyter/#once-youve-got-the-basics","text":"","title":"Once you've got the basics ..."},{"location":"1-Experiments/Jupyter/#getting-data-in-and-out-of-jupyter","text":"You can upload and download data to/from JupyterHub directly in the menu. There is an upload button at the top, and you can right-click most files or folders to download them.","title":"Getting Data in and out of Jupyter"},{"location":"1-Experiments/Jupyter/#shareable-bucket-storage","text":"There is also a mounted minio folder in your home directory, which holds files in MinIO . Refer to the Storage section for details.","title":"Shareable \"Bucket\" storage"},{"location":"1-Experiments/Kubeflow/","text":"Overview \u00b6 What does Kubeflow do? \u00b6 Kubeflow runs your workspaces . You can have notebook servers (called Jupyter Servers), and in them you can create analyses in R and Python with interactive visuals. You can save and upload data, download it, and create shared workspaces for your team. Let's get started! Video Tutorial \u00b6 This video is not up to date, some things have changed since. Setup \u00b6 Log into Kubeflow \u00b6 Log into the Azure Portal using your Cloud Credentials You have to login to the Azure Portal using your StatCan cloud credentials . first.lastname@cloud.statcan.ca or StatCan credentials first.lastname@statcan.gc.ca . You can do that using the Azure Portal . Log into Kubeflow Navigate to the Notebook Servers tab Then click + New Server Server Name and Namespace \u00b6 You will get a template to create your notebook server. Note: the name of your server can consist of only lowercase letters, numbers, and hyphens. No spaces, and no underscores. You will need to specify a namespace. By default you will have a default namespace for your account, but for projects you may need to select the namespace created specifically for that project. Otherwise the notebook server you create may not have access rights to resources required for the project. Image \u00b6 You will need to choose an image. There are JupyterLab, RStudio, Ubuntu remote desktop, and SAS images available. The SAS image is only available for StatCan employees (due to license limitations), the others are available for everyone. Select the drop down menu to select additional options within these (for instance, CPU, PyTorch, and TensorFlow images for JupyterLab). Check the name of the images and choose one that matches what you want to do. Don't know which one to choose? Check out your options here . CPU and Memory \u00b6 At the time of writing (December 23, 2021) there are two types of computers in the cluster CPU: D16s v3 (16 CPU cores, 64 GiB memory; for user use 15 CPU cores and 48 GiB memory are available; 1 CPU core and 16 GiB memory reserved for system use). GPU: NC6s_v3 (6 CPU cores, 112 GiB memory, 1 GPU; for user use 96 GiB memory are available; 16 GiB memory reserved for system use). The available GPU is the NVIDIA Tesla V100 GPU with specs here . When creating a notebook server, the system will limit you to the maximum specifications above. For CPU notebook servers, you can specify the exact amount of CPU and memory that you require. This allows you to meet your compute needs while minimising cost. For a GPU notebook server, you will always get the full server (6 CPU cores, 96 GiB accessible memory, and 1 GPU). See below section on GPUs for information on how to select a GPU server. In the advanced options, you can select a higher limit than the number of CPU cores and RAM requested. The amount requested is the amount guaranteed to be available for your notebook server and you will always pay for at least this much. If the limit is higher than the amount requested, if additional RAM and CPU cores are available on that shared server in the cluster your notebook server can use them as needed. One use case for this is jobs that usually need only one CPU core but can benefit from multithreading to speed up certain operations. By requesting one CPU core but a higher limit, you can pay much less for the notebook server while allowing it to use spare unused CPU cores as needed to speed up computations. GPUs \u00b6 If you want a GPU server, select 1 as the number of GPUs and NVIDIA as the GPU vendor (the create button will be greyed out until the GPU vendor is selected if you have a GPU specified). Multi-GPU servers are currently supported on the AAW system only on a special on-request basis, please contact the AAW maintainers if you would like a multi-GPU server. As mentioned before, if you select a GPU server you will automatically get 6 CPU cores and 112 GiB of memory. Use GPU machines responsibly GPU machines are significantly more expensive than CPU machines, so use them responsibly. Workspace Volume \u00b6 You will need a workspace volume, which is where the home folder will be mounted. There are various configuration options available: You can either reuse an existing workspace volume from before, or create a new one. You can specify the size of the workspace volume, from 4 GiB to 32 GiB. Check for old volumes by looking at the Existing option When you create your server you have the option of reusing an old volume or creating a new one. You probably want to reuse your old volume. Data Volumes \u00b6 You can also create data volumes that can be used to store additional data. Multiple data volumes can be created. Click the add new volume button to create a new volume and specify its configuration. Click the attach existing volume button to mount an existing data volume to the notebook server. There are the following configuration parameters for data volumes: Name : Name of the volume. Size in GiB : From 4 GiB to 512 GiB. Mount path : Path where the data volume can be accessed on the notebook server, by default /home/jovyan/vol-1 , /home/jovyan/vol-2 , etc. (incrementing counter per data volume mounted). When mounting an existing data volume, the name option becomes a drop-down list of the existing data volumes. Only a volume not currently mounted to an existing notebook server can be used. The mount path option remains user-configurable with the same defaults as creating a new volume. The garbage can icon on the right can be used to delete an existing or accidentally created data volume. Configurations \u00b6 There are currently three checkbox options available here: Mount MinIO storage to ~/minio (experimental) : This should make MinIO repositories accessible as subfolders / files of the minio/ folder. This is still experimental and may not work properly currently. Run a Protected B notebook : Enable this if the server you create needs access to any Protected B resources. Protected B notebook servers run with many security restrictions and have access to separate MinIO instances specifically designed for Protected B data. Miscellaneous Settings \u00b6 The following can be customized here: Enable Shared Memory : This is required if you use PyTorch with multiple data loaders, which otherwise will generate an error. If using PyTorch make sure this is enabled, otherwise it does not matter unless you have another application that requires shared memory. System Language : Can specify English or French here. And... Create!!! \u00b6 If you're satisfied with the settings, you can now create the server! It may take a few minutes to spin up depending on the resources you asked for. GPUs take longer. Your server is running If all goes well, your server should be running!!! You will now have the option to connect, and try out Jupyter! Once you've got the basics ... \u00b6 Share your workspace \u00b6 In Kubeflow every user has a namespace that contains their work (their notebook servers, pipelines, disks, etc.). Your namespace belongs to you, but can be shared if you want to collaborate with others. For more details on collaboration on the platform, see Collaboration .","title":"Kubeflow"},{"location":"1-Experiments/Kubeflow/#overview","text":"","title":"Overview"},{"location":"1-Experiments/Kubeflow/#what-does-kubeflow-do","text":"Kubeflow runs your workspaces . You can have notebook servers (called Jupyter Servers), and in them you can create analyses in R and Python with interactive visuals. You can save and upload data, download it, and create shared workspaces for your team. Let's get started!","title":"What does Kubeflow do?"},{"location":"1-Experiments/Kubeflow/#video-tutorial","text":"This video is not up to date, some things have changed since.","title":"Video Tutorial"},{"location":"1-Experiments/Kubeflow/#setup","text":"","title":"Setup"},{"location":"1-Experiments/Kubeflow/#log-into-kubeflow","text":"Log into the Azure Portal using your Cloud Credentials You have to login to the Azure Portal using your StatCan cloud credentials . first.lastname@cloud.statcan.ca or StatCan credentials first.lastname@statcan.gc.ca . You can do that using the Azure Portal . Log into Kubeflow Navigate to the Notebook Servers tab Then click + New Server","title":"Log into Kubeflow"},{"location":"1-Experiments/Kubeflow/#server-name-and-namespace","text":"You will get a template to create your notebook server. Note: the name of your server can consist of only lowercase letters, numbers, and hyphens. No spaces, and no underscores. You will need to specify a namespace. By default you will have a default namespace for your account, but for projects you may need to select the namespace created specifically for that project. Otherwise the notebook server you create may not have access rights to resources required for the project.","title":"Server Name and Namespace"},{"location":"1-Experiments/Kubeflow/#image","text":"You will need to choose an image. There are JupyterLab, RStudio, Ubuntu remote desktop, and SAS images available. The SAS image is only available for StatCan employees (due to license limitations), the others are available for everyone. Select the drop down menu to select additional options within these (for instance, CPU, PyTorch, and TensorFlow images for JupyterLab). Check the name of the images and choose one that matches what you want to do. Don't know which one to choose? Check out your options here .","title":"Image"},{"location":"1-Experiments/Kubeflow/#cpu-and-memory","text":"At the time of writing (December 23, 2021) there are two types of computers in the cluster CPU: D16s v3 (16 CPU cores, 64 GiB memory; for user use 15 CPU cores and 48 GiB memory are available; 1 CPU core and 16 GiB memory reserved for system use). GPU: NC6s_v3 (6 CPU cores, 112 GiB memory, 1 GPU; for user use 96 GiB memory are available; 16 GiB memory reserved for system use). The available GPU is the NVIDIA Tesla V100 GPU with specs here . When creating a notebook server, the system will limit you to the maximum specifications above. For CPU notebook servers, you can specify the exact amount of CPU and memory that you require. This allows you to meet your compute needs while minimising cost. For a GPU notebook server, you will always get the full server (6 CPU cores, 96 GiB accessible memory, and 1 GPU). See below section on GPUs for information on how to select a GPU server. In the advanced options, you can select a higher limit than the number of CPU cores and RAM requested. The amount requested is the amount guaranteed to be available for your notebook server and you will always pay for at least this much. If the limit is higher than the amount requested, if additional RAM and CPU cores are available on that shared server in the cluster your notebook server can use them as needed. One use case for this is jobs that usually need only one CPU core but can benefit from multithreading to speed up certain operations. By requesting one CPU core but a higher limit, you can pay much less for the notebook server while allowing it to use spare unused CPU cores as needed to speed up computations.","title":"CPU and Memory"},{"location":"1-Experiments/Kubeflow/#gpus","text":"If you want a GPU server, select 1 as the number of GPUs and NVIDIA as the GPU vendor (the create button will be greyed out until the GPU vendor is selected if you have a GPU specified). Multi-GPU servers are currently supported on the AAW system only on a special on-request basis, please contact the AAW maintainers if you would like a multi-GPU server. As mentioned before, if you select a GPU server you will automatically get 6 CPU cores and 112 GiB of memory. Use GPU machines responsibly GPU machines are significantly more expensive than CPU machines, so use them responsibly.","title":"GPUs"},{"location":"1-Experiments/Kubeflow/#workspace-volume","text":"You will need a workspace volume, which is where the home folder will be mounted. There are various configuration options available: You can either reuse an existing workspace volume from before, or create a new one. You can specify the size of the workspace volume, from 4 GiB to 32 GiB. Check for old volumes by looking at the Existing option When you create your server you have the option of reusing an old volume or creating a new one. You probably want to reuse your old volume.","title":"Workspace Volume"},{"location":"1-Experiments/Kubeflow/#data-volumes","text":"You can also create data volumes that can be used to store additional data. Multiple data volumes can be created. Click the add new volume button to create a new volume and specify its configuration. Click the attach existing volume button to mount an existing data volume to the notebook server. There are the following configuration parameters for data volumes: Name : Name of the volume. Size in GiB : From 4 GiB to 512 GiB. Mount path : Path where the data volume can be accessed on the notebook server, by default /home/jovyan/vol-1 , /home/jovyan/vol-2 , etc. (incrementing counter per data volume mounted). When mounting an existing data volume, the name option becomes a drop-down list of the existing data volumes. Only a volume not currently mounted to an existing notebook server can be used. The mount path option remains user-configurable with the same defaults as creating a new volume. The garbage can icon on the right can be used to delete an existing or accidentally created data volume.","title":"Data Volumes"},{"location":"1-Experiments/Kubeflow/#configurations","text":"There are currently three checkbox options available here: Mount MinIO storage to ~/minio (experimental) : This should make MinIO repositories accessible as subfolders / files of the minio/ folder. This is still experimental and may not work properly currently. Run a Protected B notebook : Enable this if the server you create needs access to any Protected B resources. Protected B notebook servers run with many security restrictions and have access to separate MinIO instances specifically designed for Protected B data.","title":"Configurations"},{"location":"1-Experiments/Kubeflow/#miscellaneous-settings","text":"The following can be customized here: Enable Shared Memory : This is required if you use PyTorch with multiple data loaders, which otherwise will generate an error. If using PyTorch make sure this is enabled, otherwise it does not matter unless you have another application that requires shared memory. System Language : Can specify English or French here.","title":"Miscellaneous Settings"},{"location":"1-Experiments/Kubeflow/#and-create","text":"If you're satisfied with the settings, you can now create the server! It may take a few minutes to spin up depending on the resources you asked for. GPUs take longer. Your server is running If all goes well, your server should be running!!! You will now have the option to connect, and try out Jupyter!","title":"And... Create!!!"},{"location":"1-Experiments/Kubeflow/#once-youve-got-the-basics","text":"","title":"Once you've got the basics ..."},{"location":"1-Experiments/Kubeflow/#share-your-workspace","text":"In Kubeflow every user has a namespace that contains their work (their notebook servers, pipelines, disks, etc.). Your namespace belongs to you, but can be shared if you want to collaborate with others. For more details on collaboration on the platform, see Collaboration .","title":"Share your workspace"},{"location":"1-Experiments/MLflow/","text":"Overview \u00b6 !!! danger \"MLflow has been removed from the AAW. If you need it, contact the development team\" MLflow is an open source platform for managing the Machine Learning lifecycle. It is a \"Model Registry\" for storing your machine learning models and associated metrics. You can use the web interface to examine your models, and you can use its REST API to register your models from Python, using the mlflow pip package .","title":"Overview"},{"location":"1-Experiments/MLflow/#overview","text":"!!! danger \"MLflow has been removed from the AAW. If you need it, contact the development team\" MLflow is an open source platform for managing the Machine Learning lifecycle. It is a \"Model Registry\" for storing your machine learning models and associated metrics. You can use the web interface to examine your models, and you can use its REST API to register your models from Python, using the mlflow pip package .","title":"Overview"},{"location":"1-Experiments/RStudio/","text":"Overview \u00b6 RStudio is an integrated development environment (IDE) for R. It includes a console, editor, and tools for plotting, history, debugging and workspace management. Video Tutorial \u00b6 Setup \u00b6 You can use the rstudio image to get an RStudio environment! When you create your notebook, choose RStudio from the list of available images. You can install R or python packages with conda or install.packages() . Once you've got the basics ... \u00b6 R-Shiny \u00b6 You can use Shiny , too! Shiny is an open source R package that provides a web framework for building web applications using R. Shiny helps you turn your analyses into interactive web applications.","title":"RStudio"},{"location":"1-Experiments/RStudio/#overview","text":"RStudio is an integrated development environment (IDE) for R. It includes a console, editor, and tools for plotting, history, debugging and workspace management.","title":"Overview"},{"location":"1-Experiments/RStudio/#video-tutorial","text":"","title":"Video Tutorial"},{"location":"1-Experiments/RStudio/#setup","text":"You can use the rstudio image to get an RStudio environment! When you create your notebook, choose RStudio from the list of available images. You can install R or python packages with conda or install.packages() .","title":"Setup"},{"location":"1-Experiments/RStudio/#once-youve-got-the-basics","text":"","title":"Once you've got the basics ..."},{"location":"1-Experiments/RStudio/#r-shiny","text":"You can use Shiny , too! Shiny is an open source R package that provides a web framework for building web applications using R. Shiny helps you turn your analyses into interactive web applications.","title":"R-Shiny"},{"location":"1-Experiments/Remote-Desktop/","text":"Overview \u00b6 What is Remote Desktop? \u00b6 Remote Desktop provides an in-browser GUI Ubuntu desktop experience as well as quick access to supporting tools. The operating system is Ubuntu 18.04 with the XFCE desktop environment. Geomatics \u00b6 Our version of Remote Desktop is built on an R Geospatial image. Customization \u00b6 pip , conda , npm and yarn are available to install various packages. Setup \u00b6 Accessing the Remote Desktop \u00b6 To launch the Remote Desktop or any of its supporting tools, create a Notebook Server in Kubeflow and select the remote desktop option. Once it has been created, click Connect to be redirected to the Remote Desktop. Remote Desktop brings you to the Desktop GUI through a noVNC session. Click on the > on the left side of the screen to expand a panel with options such as fullscreen and clipboard access. Accessing the Clipboard \u00b6 This is done via the second button from the top of the panel on the left. It brings up a text box which we can modify to change the contents of the clipboard or copy stuff from the clipboard of the remote desktop. For example, suppose we want to execute the command head -c 20 /dev/urandom | md5sum and copy-paste the result into a text file on our computer used to connect to the remote desktop. We first open the clipboard from the panel on the left and paste in that command into the text box: To close the clipboard window over the remote desktop, simply click the clipboard button again. We then right click on a terminal window to paste in that command and press enter to execute the command. At that point we select the MD5 result, right click, and click copy: If we open the clipboard from the panel on the left again, it will now have the new contents: The clipboard window will even update in-place if we leave it open the whole time and we simply select new material on the remote desktop and press copy again. We can simply copy what we have in that text box and paste it into any other software running on the computer used to connect. In-browser Tools \u00b6 VS Code \u00b6 Visual Studio Code is a lightweight but powerful source code editor. It comes with built-in support for JavaScript, TypeScript and Node.js and has a rich ecosystem of extensions for several languages (such as C++, C#, Java, Python, PHP, Go). Footnotes \u00b6 Remote Desktop is based on ml-tooling/ml-workspace .","title":"Remote Desktop"},{"location":"1-Experiments/Remote-Desktop/#overview","text":"","title":"Overview"},{"location":"1-Experiments/Remote-Desktop/#what-is-remote-desktop","text":"Remote Desktop provides an in-browser GUI Ubuntu desktop experience as well as quick access to supporting tools. The operating system is Ubuntu 18.04 with the XFCE desktop environment.","title":"What is Remote Desktop?"},{"location":"1-Experiments/Remote-Desktop/#geomatics","text":"Our version of Remote Desktop is built on an R Geospatial image.","title":"Geomatics"},{"location":"1-Experiments/Remote-Desktop/#customization","text":"pip , conda , npm and yarn are available to install various packages.","title":"Customization"},{"location":"1-Experiments/Remote-Desktop/#setup","text":"","title":"Setup"},{"location":"1-Experiments/Remote-Desktop/#accessing-the-remote-desktop","text":"To launch the Remote Desktop or any of its supporting tools, create a Notebook Server in Kubeflow and select the remote desktop option. Once it has been created, click Connect to be redirected to the Remote Desktop. Remote Desktop brings you to the Desktop GUI through a noVNC session. Click on the > on the left side of the screen to expand a panel with options such as fullscreen and clipboard access.","title":"Accessing the Remote Desktop"},{"location":"1-Experiments/Remote-Desktop/#accessing-the-clipboard","text":"This is done via the second button from the top of the panel on the left. It brings up a text box which we can modify to change the contents of the clipboard or copy stuff from the clipboard of the remote desktop. For example, suppose we want to execute the command head -c 20 /dev/urandom | md5sum and copy-paste the result into a text file on our computer used to connect to the remote desktop. We first open the clipboard from the panel on the left and paste in that command into the text box: To close the clipboard window over the remote desktop, simply click the clipboard button again. We then right click on a terminal window to paste in that command and press enter to execute the command. At that point we select the MD5 result, right click, and click copy: If we open the clipboard from the panel on the left again, it will now have the new contents: The clipboard window will even update in-place if we leave it open the whole time and we simply select new material on the remote desktop and press copy again. We can simply copy what we have in that text box and paste it into any other software running on the computer used to connect.","title":"Accessing the Clipboard"},{"location":"1-Experiments/Remote-Desktop/#in-browser-tools","text":"","title":"In-browser Tools"},{"location":"1-Experiments/Remote-Desktop/#vs-code","text":"Visual Studio Code is a lightweight but powerful source code editor. It comes with built-in support for JavaScript, TypeScript and Node.js and has a rich ecosystem of extensions for several languages (such as C++, C#, Java, Python, PHP, Go).","title":"VS Code"},{"location":"1-Experiments/Remote-Desktop/#footnotes","text":"Remote Desktop is based on ml-tooling/ml-workspace .","title":"Footnotes"},{"location":"1-Experiments/Selecting-an-Image/","text":"Selecting an Image for your Notebook Server \u00b6 Depending on your project or use case of the Notebook Server, some images may be more suitable than others. The following will go through the main features of each to help you pick the most appropriate image for you. When selecting an image, you have 3 main options: Jupyter Notebook (CPU, TensorFlow, PyTorch) RStudio Remote Desktop (r, geomatics) Jupyter Notebooks \u00b6 Jupyter Notebooks are used to create and share interactive documents that contain a mix of live code, visualizations, and text. These can be written in Python , Julia , or R . Common uses include: data transformation, numerical simulation, statistical modelling, machine learning and more. The jupyter notebooks are great launchpads for analytics including machine learning. The jupyterlab-cpu image gives a good core experience for python, including common packages such as numpy , pandas and scikit-learn . If you're interested specifically in using TensorFlow or PyTorch , we also have jupyterlab-tensorflow and jupyterlab-pytorch which come with those tools pre-installed. For the jupyterlab-pytorch image, the PyTorch packages (torch, torchvision, and torchaudio) are installed in the torch conda environment. You must activate this environment to use PyTorch. For the jupyterlab-cpu , jupyterlab-tensorflow , and jupyterlab-pytorch images, in the default shell the conda activate command may not work. This is due to the environment not being initialized properly. In this case run bash , you should see the AAW logo and a few instructions appear. After this conda activate should work properly. If you see the AAW logo on startup it means the environment is correctly initialized and conda activate should work properly. A fix for this bug is in the works, once this is fixed this paragraph will be removed. Each image comes pre-loaded with VS Code in the browser if you prefer a full IDE experience. RStudio \u00b6 RStudio gives you an integrated development environment specifically for R . If you're coding in R , this is typically the Notebook Server to use. Use the rstudio image to get an RStudio environment. Remote-Desktop \u00b6 For a full Ubuntu desktop experience, use the remote desktop image. It comes pre-loaded with Python, R and Geomatics tooling, but are delivered in a typical desktop experience that also comes with Firefox, VS Code, and open office tools. The operating system is Ubuntu 18.04 with the XFCE desktop environment.","title":"Selecting an Image for your Notebook Server"},{"location":"1-Experiments/Selecting-an-Image/#selecting-an-image-for-your-notebook-server","text":"Depending on your project or use case of the Notebook Server, some images may be more suitable than others. The following will go through the main features of each to help you pick the most appropriate image for you. When selecting an image, you have 3 main options: Jupyter Notebook (CPU, TensorFlow, PyTorch) RStudio Remote Desktop (r, geomatics)","title":"Selecting an Image for your Notebook Server"},{"location":"1-Experiments/Selecting-an-Image/#jupyter-notebooks","text":"Jupyter Notebooks are used to create and share interactive documents that contain a mix of live code, visualizations, and text. These can be written in Python , Julia , or R . Common uses include: data transformation, numerical simulation, statistical modelling, machine learning and more. The jupyter notebooks are great launchpads for analytics including machine learning. The jupyterlab-cpu image gives a good core experience for python, including common packages such as numpy , pandas and scikit-learn . If you're interested specifically in using TensorFlow or PyTorch , we also have jupyterlab-tensorflow and jupyterlab-pytorch which come with those tools pre-installed. For the jupyterlab-pytorch image, the PyTorch packages (torch, torchvision, and torchaudio) are installed in the torch conda environment. You must activate this environment to use PyTorch. For the jupyterlab-cpu , jupyterlab-tensorflow , and jupyterlab-pytorch images, in the default shell the conda activate command may not work. This is due to the environment not being initialized properly. In this case run bash , you should see the AAW logo and a few instructions appear. After this conda activate should work properly. If you see the AAW logo on startup it means the environment is correctly initialized and conda activate should work properly. A fix for this bug is in the works, once this is fixed this paragraph will be removed. Each image comes pre-loaded with VS Code in the browser if you prefer a full IDE experience.","title":"Jupyter Notebooks"},{"location":"1-Experiments/Selecting-an-Image/#rstudio","text":"RStudio gives you an integrated development environment specifically for R . If you're coding in R , this is typically the Notebook Server to use. Use the rstudio image to get an RStudio environment.","title":"RStudio"},{"location":"1-Experiments/Selecting-an-Image/#remote-desktop","text":"For a full Ubuntu desktop experience, use the remote desktop image. It comes pre-loaded with Python, R and Geomatics tooling, but are delivered in a typical desktop experience that also comes with Firefox, VS Code, and open office tools. The operating system is Ubuntu 18.04 with the XFCE desktop environment.","title":"Remote-Desktop"},{"location":"2-Publishing/Accessing-Published-Content/","text":"","title":"Accessing Published Content"},{"location":"2-Publishing/Custom/","text":"Overview \u00b6 Custom Web Apps \u00b6 We can deploy anything as long as it's open source and we can put it in a Docker container. For instance, Node.js apps, Flask or Dash apps. Etc. See the source code for this app We just push these kinds of applications through GitHub into the server. The source for the above app is StatCan/covid19 Setup \u00b6 How to get your app hosted \u00b6 If you already have a web app in a git repository then, as soon as it's containerized, we can fork the Git repository into the StatCan GitHub repository and point a URL to it. To update it, you'll just interact with the StatCan GitHub repository with Pull Requests. Contact us if you have questions.","title":"Custom"},{"location":"2-Publishing/Custom/#overview","text":"","title":"Overview"},{"location":"2-Publishing/Custom/#custom-web-apps","text":"We can deploy anything as long as it's open source and we can put it in a Docker container. For instance, Node.js apps, Flask or Dash apps. Etc. See the source code for this app We just push these kinds of applications through GitHub into the server. The source for the above app is StatCan/covid19","title":"Custom Web Apps"},{"location":"2-Publishing/Custom/#setup","text":"","title":"Setup"},{"location":"2-Publishing/Custom/#how-to-get-your-app-hosted","text":"If you already have a web app in a git repository then, as soon as it's containerized, we can fork the Git repository into the StatCan GitHub repository and point a URL to it. To update it, you'll just interact with the StatCan GitHub repository with Pull Requests. Contact us if you have questions.","title":"How to get your app hosted"},{"location":"2-Publishing/Dash/","text":"Overview \u00b6 Dash is a great tool used by many for data analysis, data exploration, visualization, modelling, instrument control, and reporting. The following example demonstrates a highly reactive and customised Dash app with little code. Running your Notebook Server and accessing the port When running any tool from your Jupyter Notebook that posts a website to a port, you will not be able to simply access it from http://localhost:5000/ as normally suggested in the output upon running the web-app. To access the web server you will need to use the base URL. In your notebook terminal run: python echo https://kubeflow.covid.cloud.statcan.ca${JUPYTER_SERVER_URL:19}proxy/5000/ Data Visualization with Dash \u00b6 Dash makes it simple to build an interactive GUI around your data analysis code. This is an example of a Layout With Figure and Slider from Dash . Getting Started \u00b6 Open a terminal window in your Jupyter notebook and run the following commands: # required installations if not already installed pip3 install dash == 1.16.3 pip3 install pandas Create a file called app.py with the following content: # app.py #!/usr/bin/env python3 import dash import dash_core_components as dcc import dash_html_components as html from dash.dependencies import Input , Output import plotly.express as px import pandas as pd df = pd . read_csv ( 'https://raw.githubusercontent.com/plotly/datasets/master/gapminderDataFiveYear.csv' ) external_stylesheets = [ 'https://codepen.io/chriddyp/pen/bWLwgP.css' ] app = dash . Dash ( __name__ , external_stylesheets = external_stylesheets ) app . layout = html . Div ([ dcc . Graph ( id = 'graph-with-slider' ), dcc . Slider ( id = 'year-slider' , min = df [ 'year' ] . min (), max = df [ 'year' ] . max (), value = df [ 'year' ] . min (), marks = { str ( year ): str ( year ) for year in df [ 'year' ] . unique ()}, step = None ) ]) @app . callback ( Output ( 'graph-with-slider' , 'figure' ), [ Input ( 'year-slider' , 'value' )]) def update_figure ( selected_year ): filtered_df = df [ df . year == selected_year ] fig = px . scatter ( filtered_df , x = \"gdpPercap\" , y = \"lifeExp\" , size = \"pop\" , color = \"continent\" , hover_name = \"country\" , log_x = True , size_max = 55 ) fig . update_layout ( transition_duration = 500 ) return fig if __name__ == '__main__' : app . run_server ( debug = True ) Run your app \u00b6 python app . py # or you can use: export FLASK_APP = app . py flask run","title":"Dash"},{"location":"2-Publishing/Dash/#overview","text":"Dash is a great tool used by many for data analysis, data exploration, visualization, modelling, instrument control, and reporting. The following example demonstrates a highly reactive and customised Dash app with little code. Running your Notebook Server and accessing the port When running any tool from your Jupyter Notebook that posts a website to a port, you will not be able to simply access it from http://localhost:5000/ as normally suggested in the output upon running the web-app. To access the web server you will need to use the base URL. In your notebook terminal run: python echo https://kubeflow.covid.cloud.statcan.ca${JUPYTER_SERVER_URL:19}proxy/5000/","title":"Overview"},{"location":"2-Publishing/Dash/#data-visualization-with-dash","text":"Dash makes it simple to build an interactive GUI around your data analysis code. This is an example of a Layout With Figure and Slider from Dash .","title":"Data Visualization with Dash"},{"location":"2-Publishing/Dash/#getting-started","text":"Open a terminal window in your Jupyter notebook and run the following commands: # required installations if not already installed pip3 install dash == 1.16.3 pip3 install pandas Create a file called app.py with the following content: # app.py #!/usr/bin/env python3 import dash import dash_core_components as dcc import dash_html_components as html from dash.dependencies import Input , Output import plotly.express as px import pandas as pd df = pd . read_csv ( 'https://raw.githubusercontent.com/plotly/datasets/master/gapminderDataFiveYear.csv' ) external_stylesheets = [ 'https://codepen.io/chriddyp/pen/bWLwgP.css' ] app = dash . Dash ( __name__ , external_stylesheets = external_stylesheets ) app . layout = html . Div ([ dcc . Graph ( id = 'graph-with-slider' ), dcc . Slider ( id = 'year-slider' , min = df [ 'year' ] . min (), max = df [ 'year' ] . max (), value = df [ 'year' ] . min (), marks = { str ( year ): str ( year ) for year in df [ 'year' ] . unique ()}, step = None ) ]) @app . callback ( Output ( 'graph-with-slider' , 'figure' ), [ Input ( 'year-slider' , 'value' )]) def update_figure ( selected_year ): filtered_df = df [ df . year == selected_year ] fig = px . scatter ( filtered_df , x = \"gdpPercap\" , y = \"lifeExp\" , size = \"pop\" , color = \"continent\" , hover_name = \"country\" , log_x = True , size_max = 55 ) fig . update_layout ( transition_duration = 500 ) return fig if __name__ == '__main__' : app . run_server ( debug = True )","title":"Getting Started"},{"location":"2-Publishing/Dash/#run-your-app","text":"python app . py # or you can use: export FLASK_APP = app . py flask run","title":"Run your app"},{"location":"2-Publishing/Datasette/","text":"Overview \u00b6 Datasette is an instant JSON API for your SQLite databases allowing you to explore the DB and run SQL queries in a more interactive way. You can find a list of example datasettes here . The Datasette Ecosystem There are all sorts of tools for converting data to and from sqlite here . For example, you can load shapefiles into sqlite, or create Vega plots from a sqlite database. SQLite works well with R , Python , and many other tools. Example Datasette \u00b6 Below are some screenshots from the global-power-plants Datasette, you can preview and explore the data in the browser, either with clicks or SQL queries. You can even explore maps within the tool! Video Tutorial \u00b6 Getting Started \u00b6 Installing Datasette \u00b6 In your Jupyter Notebook, open a terminal window and run the command pip3 install datasette . Starting Datasette \u00b6 To view your own database in your Jupyter Notebook, create a file called start.sh in your project directory and copy the below code into it. Make the file executable using chmod +x start.sh . Run the file with ./start.sh . Access the web server using the base URL with the port number you are using in the below file. start.sh #!/bin/bash # This script just starts Datasette with the correct URL, so # that you can use it within kubeflow. # Get an example database wget https://github.com/StatCan/aaw-contrib-r-notebooks/raw/master/database-connections/latin_phrases.db # If you have your own database, you can change this line! DATABASE = latin_phrases.db export BASE_URL = \"https://kubeflow.covid.cloud.statcan.ca ${ JUPYTER_SERVER_URL : 19 } proxy/8001/\" echo \"Base url: ${ BASE_URL } \" datasette $DATABASE --cors --config max_returned_rows:100000 --config sql_time_limit_ms:5500 --config base_url: ${ BASE_URL } Check out this video tutorial One user of the platform used Datasette along with a javascript dashboard. See this video for a demo. Running your Notebook Server and accessing the port When running any tool from your Jupyter Notebook that posts a website to a port, you will not be able to simply access it from http://localhost:5000/ as normally suggested in the output upon running the web-app. To access the web server you will need to use the base URL. In your notebook terminal, run: python echo https://kubeflow.covid.cloud.statcan.ca${JUPYTER_SERVER_URL:19}proxy/5000/","title":"Datasette"},{"location":"2-Publishing/Datasette/#overview","text":"Datasette is an instant JSON API for your SQLite databases allowing you to explore the DB and run SQL queries in a more interactive way. You can find a list of example datasettes here . The Datasette Ecosystem There are all sorts of tools for converting data to and from sqlite here . For example, you can load shapefiles into sqlite, or create Vega plots from a sqlite database. SQLite works well with R , Python , and many other tools.","title":"Overview"},{"location":"2-Publishing/Datasette/#example-datasette","text":"Below are some screenshots from the global-power-plants Datasette, you can preview and explore the data in the browser, either with clicks or SQL queries. You can even explore maps within the tool!","title":"Example Datasette"},{"location":"2-Publishing/Datasette/#video-tutorial","text":"","title":"Video Tutorial"},{"location":"2-Publishing/Datasette/#getting-started","text":"","title":"Getting Started"},{"location":"2-Publishing/Datasette/#installing-datasette","text":"In your Jupyter Notebook, open a terminal window and run the command pip3 install datasette .","title":"Installing Datasette"},{"location":"2-Publishing/Datasette/#starting-datasette","text":"To view your own database in your Jupyter Notebook, create a file called start.sh in your project directory and copy the below code into it. Make the file executable using chmod +x start.sh . Run the file with ./start.sh . Access the web server using the base URL with the port number you are using in the below file. start.sh #!/bin/bash # This script just starts Datasette with the correct URL, so # that you can use it within kubeflow. # Get an example database wget https://github.com/StatCan/aaw-contrib-r-notebooks/raw/master/database-connections/latin_phrases.db # If you have your own database, you can change this line! DATABASE = latin_phrases.db export BASE_URL = \"https://kubeflow.covid.cloud.statcan.ca ${ JUPYTER_SERVER_URL : 19 } proxy/8001/\" echo \"Base url: ${ BASE_URL } \" datasette $DATABASE --cors --config max_returned_rows:100000 --config sql_time_limit_ms:5500 --config base_url: ${ BASE_URL } Check out this video tutorial One user of the platform used Datasette along with a javascript dashboard. See this video for a demo. Running your Notebook Server and accessing the port When running any tool from your Jupyter Notebook that posts a website to a port, you will not be able to simply access it from http://localhost:5000/ as normally suggested in the output upon running the web-app. To access the web server you will need to use the base URL. In your notebook terminal, run: python echo https://kubeflow.covid.cloud.statcan.ca${JUPYTER_SERVER_URL:19}proxy/5000/","title":"Starting Datasette"},{"location":"2-Publishing/PowerBI/","text":"Overview \u00b6 Loading data into Power BI \u00b6 We do not offer a Power BI server, but you can pull your data into Power BI from our Storage system, and use the data as a pandas data frame. Setup \u00b6 What you'll need \u00b6 A computer with Power BI, and Python 3.6 Your MinIO ACCESS_KEY and SECRET_KEY on hand. (See Storage ) Set up Power BI \u00b6 Open up your Power BI system, and open up this Power BI quick start in your favourite text editor. You'll have to make sure that pandas , boto3 , and numpy are installed, and that you're using the right Conda virtual environment (if applicable). You'll then need to make sure that Power BI is using the correct Python environment. This is modified from the options menu, and the exact path is specified in the quick start guide. Edit your python script \u00b6 Then, edit your Python script to use your MinIO ACCESS_KEY and SECRET_KEY , and then click \"Get Data\" and copy it in as a Python Script.","title":"Power BI"},{"location":"2-Publishing/PowerBI/#overview","text":"","title":"Overview"},{"location":"2-Publishing/PowerBI/#loading-data-into-power-bi","text":"We do not offer a Power BI server, but you can pull your data into Power BI from our Storage system, and use the data as a pandas data frame.","title":"Loading data into Power BI"},{"location":"2-Publishing/PowerBI/#setup","text":"","title":"Setup"},{"location":"2-Publishing/PowerBI/#what-youll-need","text":"A computer with Power BI, and Python 3.6 Your MinIO ACCESS_KEY and SECRET_KEY on hand. (See Storage )","title":"What you'll need"},{"location":"2-Publishing/PowerBI/#set-up-power-bi","text":"Open up your Power BI system, and open up this Power BI quick start in your favourite text editor. You'll have to make sure that pandas , boto3 , and numpy are installed, and that you're using the right Conda virtual environment (if applicable). You'll then need to make sure that Power BI is using the correct Python environment. This is modified from the options menu, and the exact path is specified in the quick start guide.","title":"Set up Power BI"},{"location":"2-Publishing/PowerBI/#edit-your-python-script","text":"Then, edit your Python script to use your MinIO ACCESS_KEY and SECRET_KEY , and then click \"Get Data\" and copy it in as a Python Script.","title":"Edit your python script"},{"location":"2-Publishing/R-Shiny/","text":"Overview \u00b6 R-Shiny is an R package that makes it easy to build interactive web apps in R. We handle the R-Shiny server, and it's super easy to get your dashboard onto the platform. Setup \u00b6 Just send a pull request! \u00b6 All you have to do is send a pull request to our R-Dashboards repository . Include your repository in a folder with the name you want (for example, \"air-quality-dashboard\"). Then we will approve it and it will come online. If you need extra R libraries to be installed, send your list to the R-Shiny repository by creating a GitHub Issue and we will add the dependencies. See the above dashboard here The above dashboard is in GitHub. Take a look at the source , and see the dashboard live . Once you've got the basics ... \u00b6 Embedding dashboards into your websites \u00b6 Embedding dashboards in other sites We have not had a chance to look at this or prototype it yet, but if you have a use-case, feel free to reach out to engineering. We will work with you to figure something out.","title":"R Shiny"},{"location":"2-Publishing/R-Shiny/#overview","text":"R-Shiny is an R package that makes it easy to build interactive web apps in R. We handle the R-Shiny server, and it's super easy to get your dashboard onto the platform.","title":"Overview"},{"location":"2-Publishing/R-Shiny/#setup","text":"","title":"Setup"},{"location":"2-Publishing/R-Shiny/#just-send-a-pull-request","text":"All you have to do is send a pull request to our R-Dashboards repository . Include your repository in a folder with the name you want (for example, \"air-quality-dashboard\"). Then we will approve it and it will come online. If you need extra R libraries to be installed, send your list to the R-Shiny repository by creating a GitHub Issue and we will add the dependencies. See the above dashboard here The above dashboard is in GitHub. Take a look at the source , and see the dashboard live .","title":"Just send a pull request!"},{"location":"2-Publishing/R-Shiny/#once-youve-got-the-basics","text":"","title":"Once you've got the basics ..."},{"location":"2-Publishing/R-Shiny/#embedding-dashboards-into-your-websites","text":"Embedding dashboards in other sites We have not had a chance to look at this or prototype it yet, but if you have a use-case, feel free to reach out to engineering. We will work with you to figure something out.","title":"Embedding dashboards into your websites"},{"location":"3-Pipelines/Kubeflow-Pipelines/","text":"Overview \u00b6 Kubeflow pipelines are in the process of being removed from AAW. No new development should use Kubeflow pipelines. If you have questions about this removal, please speak with the AAW maintainers. Kubeflow Pipelines is a platform for building machine learning workflows for deployment in a Kubernetes environment. It enables authoring pipelines that encapsulate analytical workflows (transforming data, training models, building visuals, etc.). These pipelines can be shared, reused, and scheduled, and are built to run on compute provided via Kubernetes. Here is an example of a pipeline with many sample steps feeding into a single average step. This image comes from the Kubeflow Pipelines UI. In the context of the Advanced Analytics Workspace, Kubeflow Pipelines are interacted with through: The Kubeflow UI , where from the Pipelines menu you can upload pipelines, view the pipelines you have and their results, etc. The Kubeflow Pipelines python SDK , accessible through the Jupyter Notebook Servers , where you can define your components and pipelines, submit them to run now, or even save them for later. More examples in the notebooks More comprehensive pipeline examples specifically made for this platform are available on GitHub (and in every Notebook Server at /jupyter-notebooks ). You can also check out public sources . See the official Kubeflow docs for a more detailed explanation of Kubeflow Pipelines. What are pipelines and how do they work? \u00b6 A pipeline in Kubeflow Pipelines consists of one or more pipeline components chained together to form a workflow. The components are like functions, describing the individual steps in your workflow (such as pulling columns from a data store, transforming data, or training a model). The pipeline is the logic that glues components together, such as: Run Component-A Pass the output from Component-A to Component-B and Component-C ... In the above image, the logic would be running many sample steps followed by a single average step. At their core, each component has: A standalone application, packaged as a Docker image , for doing the actual work. The code in the Docker image could be a shell script, Python script, or anything else you can run from a Linux terminal, and generally will have a command line interface for data exchange (accessible through docker run ) A YAML file that describes how Kubeflow Pipelines runs this code (what Docker image should be run, what command line arguments does it accept, what output does it generate) Each component should be single purpose , modular , and reusable . Setup \u00b6 Define and run your first pipeline using the Python SDK \u00b6 While pipelines and components are defined in Kubeflow Pipelines by YAML files that use Docker images, that does not mean we have to work directly with either YAML files or Docker images. The Kubeflow Pipelines SDK provides a way for us to define our pipeline and components directly in Python code, where the SDK then translates our Python code to YAML files for us. For our first example, let's define a simple pipeline using only the Python SDK. The purpose of this section is to give a high level view of component and pipeline authoring, not a deep dive. More detailed looks into defining your own components , passing data between components , and returning data from your pipeline are explained in more detail in further sections. The demo pipeline we define will do the following: Accept five numbers as arguments Average of the first three numbers Average of the last two numbers Average of the results of (2) and (3) To do this, we will first define our component . Our average component will call a Docker image that does the following: Accepts one or more numbers as command line arguments Returns the average of these numbers by writing them to an output file in the container (by default, to out.txt ) This Docker image is already built for us and stored in our container registry here: k8scc01covidacr.azurecr.io/kfp-components/average:v1 . Don't worry if you don't know Docker - since the image is built already, we only have to tell Kubeflow Pipelines where it is. ??? info \"Full details of the average component's Docker image are in GitHub \" This image effectively runs the following code (slightly cleaned up for brevity). By making average.py accept an arbitrary set of numbers as inputs, we can use the same average component for all steps in our pipeline : import argparse def parse_args (): parser = argparse . ArgumentParser ( description = \"Returns the average of one or \" \"more numbers as a JSON file\" ) parser . add_argument ( \"numbers\" , type = float , nargs = \"+\" , help = \"One or more numbers\" ) parser . add_argument ( \"--output_file\" , type = str , default = \"out.txt\" , help = \"Filename \" \"to write output number to\" ) return parser . parse_args () if __name__ == '__main__' : args = parse_args () numbers = args . numbers output_file = args . output_file print ( f \"Averaging numbers: { numbers } \" ) avg = sum ( numbers ) / len ( numbers ) print ( f \"Result = { avg } \" ) print ( f \"Writing output to { output_file } \" ) with open ( output_file , 'w' ) as fout : fout . write ( str ( avg )) print ( \"Done\" ) To make our average image into a Kubeflow Pipelines component , we make a kfp.dsl.ContainerOp in Python that defines how Kubeflow Pipelines interacts with our container, specifying: The Docker image location to use How to pass arguments to the running container What outputs to expect from the container We could use ContainerOp directly, but since we'll use average a few times we instead create a factory function we can reuse: from kfp import dsl def average_op ( * numbers ): \"\"\" Factory for average ContainerOps Accepts an arbitrary number of input numbers, returning a ContainerOp that passes those numbers to the underlying Docker image for averaging Returns output collected from ./out.txt from inside the container \"\"\" # Input validation if len ( numbers ) < 1 : raise ValueError ( \"Must specify at least one number to take the average of\" ) return dsl . ContainerOp ( name = \"average\" , # What will show up on the pipeline viewer image = \"k8scc01covidacr.azurecr.io/kfp-components/average:v1\" , # The image that KFP runs to do the work arguments = numbers , # Passes each number as a separate command line argument # Note that these arguments get serialized to strings file_outputs = { 'data' : './out.txt' }, # Expect an output file called out.txt to be generated # KFP can read this file and bring it back automatically ) To define our pipeline, we create a Python function decorated by the @dsl.pipeline decorator. We invoke our average_op factory to use our average container. We pass each average some inputs, and even use their outputs by accessing avg_*.output . @dsl . pipeline ( name = \"my pipeline's name\" ) def my_pipeline ( a , b , c , d , e ): \"\"\" Averaging pipeline which accepts five numbers and does some averaging operations on them \"\"\" # Compute averages for two groups avg_1 = average_op ( a , b , c ) avg_2 = average_op ( d , e ) # Use the results from _1 and _2 to compute an overall average average_result_overall = average_op ( avg_1 . output , avg_2 . output ) Finally, while we've defined our pipeline in Python, Kubeflow Pipelines itself needs everything defined as a YAML file. This final step uses the Kubeflow Pipelines Python SDK to translate our pipeline function into a YAML file that describes exactly how Kubeflow Pipelines can interact with our component. Unzip it and take a look for yourself! from kfp import compiler pipeline_yaml = 'pipeline.yaml.zip' compiler . Compiler () . compile ( my_pipeline , pipeline_yaml ) print ( f \"Exported pipeline definition to { pipeline_yaml } \" ) Kubeflow Pipelines is a lazy beast It is useful to keep in mind what computation is happening when you run this python code versus what happens when you submit the pipeline to Kubeflow Pipelines. Although it seems like everything is happening in the moment, try adding print(avg_1.output) to the above pipeline and see what happens when you compile your pipeline. The Python SDK we're using is for authoring pipelines, not for running them, so results from components will never be available when you run this Python code. The is discussed more below in Understanding what computation occurs when . To actually run our pipeline, we define an experiment: experiment_name = \"averaging-pipeline\" import kfp client = kfp . Client () exp = client . create_experiment ( name = experiment_name ) pl_params = { 'a' : 5 , 'b' : 5 , 'c' : 8 , 'd' : 10 , 'e' : 18 , } And then run an instance of our pipeline with the arguments we want: import time run = client . run_pipeline ( exp . id , # Run inside the above experiment experiment_name + '-' + time . strftime ( \"%Y%m %d -%H%M%S\" ), # Give our job a name with a timestamp so its unique pipeline_yaml , # Pass the .yaml.zip we created above. This defines the pipeline params = pl_params # Pass our parameters we want to run the pipeline with ) This can all be seen in the Kubeflow Pipelines UI : Later when we want to reuse the pipeline, we can pass different arguments and do it all again. !!! info \"We create our experiment, upload our pipeline, and run from Python in this example, but we could also do all this through the Kubeflow Pipelines UI above. Understanding what computation occurs when \u00b6 The above example uses Python code to define: The interface between Kubeflow Pipelines and our Docker containers doing the work (by defining ContainerOp 's) The logic of our pipeline (by defining my_pipeline ). But when we run compiler.Compiler().compile() and client.run_pipeline() , what actually happens? It is important to remember that everything we run in Python here is specifying the pipeline and its components in order to write YAML definitions, it is not doing the work of the pipeline. When running compiler.Compiler().compile() , we are not running our pipeline in the typical sense. Instead, KFP uses my_pipeline to build a YAML version of it. When we compile , the KFP SDK is passing placeholder arguments to my_pipeline and tracing where they (and any other runtime data) go, such as any output a component produces. When compile encounters a ContainerOp , nothing runs now - instead it takes note that a container will be there in future and remembers what data it will consume/generate. This can be seen by modifying and recompiling our pipeline: @dsl . pipeline ( name = \"my pipeline's name\" ) def my_pipeline ( a , b , c , d , e ): \"\"\" Averaging pipeline which accepts five numbers and does some averaging operations on them \"\"\" # NEW CODE x = 1 + 1 print ( f \"The value of x is { x } \" ) print ( f \"The value of a is { a } \" ) # Compute averages for two groups avg_1 = average_op ( a , b , c ) avg_2 = average_op ( d , e ) # NEW CODE print ( f \"The value of avg_1.output is { avg_1 . output } \" ) # Use the results from _1 and _2 to compute an overall average average_result_overall = average_op ( avg_1 . output , avg_2 . output ) And when we compile , we see print statements: The value of x is 2 The value of a is {{ pipelineparam : op = ; name = a }} The value of avg_1.output is {{ pipelineparam : op = averge ; name = data }} In the first print statement everything is normal. In the second and third print statements, however, we see string placeholders rather than actual output. So while compile does \"execute\" my_pipeline , the KFP-specific parts of the code don't actually generate results. This can also be seen in the YAML file that compile generates, for example looking at the portion defining our average_result_overall component: - name : average-3 container : args : [ \"{{inputs.parameters.average-data}}\" , \"{{inputs.parameters.average-2-data}}\" , ] image : k8scc01covidacr.azurecr.io/kfp-components/average:v1 inputs : parameters : - { name : average-2-data } - { name : average-data } outputs : artifacts : - { name : average-3-data , path : ./out.txt } metadata : labels : { pipelines.kubeflow.org/pipeline-sdk-type : kfp } In this YAML we see the input parameters passed are placeholders for data from previous components rather than their actual value. This is because while KFP knows a result from average-data and average-2-data will be passed to average, but the value of that result is not available until the pipeline is actually run. Component naming within the YAML file Because we made an average_op factory function with name='average' above, our YAML file has component names that automatically increment to avoid recreating the same name twice. We could have been fancier with our factory functions to more directly control our names, giving an argument like name='average_first_input_args' , or could even have explicitly defined the name in our pipeline by using avg_1 = average_op(a, b, c).set_display_name(\"Average 1\") . As one more example, let's try two more pipelines. One has a for loop inside which prints \"Woohoo!\" a fixed number of times. whereas the other does the same but loops n times (where n is a pipeline parameter): !!! info \"Pipeline parameters are described more below, but they work like parameters for functions. Pipelines can accept data (numbers, string URL's to large files in MinIO, etc.) as arguments, allowing a single generic pipeline to work in many situations.\" @dsl . pipeline ( name = \"my pipeline's name\" ) def another_pipeline (): \"\"\" Prints to the screen 10 times \"\"\" for i in range ( 10 ): print ( \"Woohoo!\" ) # And just so we've got some component going too... avg = average_op ( n ) compiler . Compiler () . compile ( another_pipeline , \"another.yaml.zip\" ) @dsl . pipeline ( name = \"my pipeline's name\" ) def another_another_pipeline ( n ): \"\"\" Prints to the screen n times \"\"\" for i in range ( n ): print ( \"Woohoo!\" ) # And just so we've got some component going too... avg = average_op ( n ) compiler . Compiler () . compile ( another_another_pipeline , \"another.yaml.zip\" ) The first works as you'd expect, but the second raises the exception: TypeError: 'PipelineParam' object cannot be interpreted as an integer Why? Because when authoring the pipeline n is a placeholder and has no value. KFP cannot define a pipeline from this because it does not know how many times to loop. We'd hit similar problems if using if statements. There are some ways around this, but they're left to the reader to explore through the Kubeflow Pipelines docs . Why does pipeline authoring behave this way? Because pipelines (and components ) are meant to be reusable definitions of logic that are defined in static YAML files, with all dynamic decision making done inside components. This can make them a little awkward to define, but also helps them be more reusable. Once you've got the basics ... \u00b6 Data Exchange \u00b6 Passing data into, within, and from a pipeline \u00b6 In the first example above, we pass: Numbers into our pipeline Numbers between components within our pipeline A number back to the user at the end But as discussed above, pipeline arguments and component results are just placeholder objects \u2013 so how does KFP know our values are numeric? The answer is: it doesn't. In fact, it didn't even treat them as numbers above. Instead it treated them as strings. It is just that our pipeline components worked just as well with \"5\" as they would have with 5 . A safe default assumption is that all data exchange happens as a string. When we passed a, b, ... into the pipeline, those numbers were implicitly stringified because they eventually become command line arguments for our Docker container. When we read the result of avg_1 from its out.txt , that result was read as a string. By calling average_op(avg_1.output, avg_2.output) , we ask KFP to pass the string output from avg_1 and avg_2 to a new average_op . It just so happened that, since average_op passes each string as a command line argument to our Docker image, it didn't really matter they were strings. You can still use non-string data types, but you need to pass them as serialized versions. So if we wanted our avg_1 component to return both the numbers passed to it and the average returned as a dictionary, for example: { 'numbers' : [ 5 , 5 , 8 ], 'result' : 6.0 , } We could modify our average.py in the Docker image write our dictionary of numbers and result to out.txt as JSON. But then when we pass the result to make average_result_overall , that component needs to deserialize the above JSON and pull the data from it that it needs. And because these results are not available when authoring the pipeline, something like this does not work: def my_pipeline ( a , b , c , d , e ): \"\"\" Averaging pipeline which accepts five numbers and does some averaging operations on them \"\"\" # Compute averages for two groups avg_1 = average_op_that_returns_json ( a , b , c ) avg_2 = average_op_that_returns_json ( d , e ) # THIS DOES NOT WORK! import json avg_1_result = json . loads ( avg_1 . output )[ 'result' ] avg_2_result = json . loads ( avg_2 . output )[ 'result' ] # Use the results from _1 and _2 to compute an overall average average_result_overall = average_op ( avg_1 . output , avg_2 . output ) At compile time, avg_1.output is just a placeholder and can't be treated like the JSON it will eventually become. To do something like this, we need to interpret the JSON string within a container. Passing Secrets \u00b6 Pipelines often need sensitive information (passwords, API keys, etc.) to operate. To keep these secure, these cannot be passed to a Kubeflow Pipeline using a pipeline argument, environment variable, or as a hard coded value in python code, as they will be exposed as plain text for others to read. To address this issue, we use Kubernetes secrets as a way to securely store and pass sensitive information. Each secret is a key-value store containing some number of key-value pairs. The secrets can be passed by reference to the pipeline. Secrets are only accessible within their own namespace Secrets can only be accessed within the namespace they are created in. If you need to use a secret in another namespace, you need to add it there manually. Create a key-value store \u00b6 The example below creates a key-value store called elastic-credentials which contains two key-value pairs: \"username\": \"USERNAME\", \"password\": \"PASSWORD\" kubectl create secret generic elastic-credentials \\ --from-literal = username = \"YOUR_USERNAME\" \\ --from-literal = password = \"YOUR_PASSWORD\" Get an existing key-value store \u00b6 # The secrets will be base64 encoded. kubectl get secret elastic-credentials Mounting Kubernetes Secrets to Environment Variables in Container Operations \u00b6 Once the secrets are defined in the project namespace, you can mount specific secrets as environment variables in your container using the Kubeflow SDK. Example This example is based off of a snippet from the Python KFP source code . This example shows how (1) an Elasticsearch username(2) an Elasticsearch password, and (3) a GitLab deploy token are passed to the container operation as environment variables. # Names of k8s secret key-value stores ES_CREDENTIALS_STORE = \"elastic-credentials\" GITLAB_CREDENTIALS_STORE = \"gitlab-credentials\" # k8s secrets key names ES_USER_KEY = \"username\" ES_PASSWORD_KEY = \"password\" GITLAB_DEPLOY_TOKEN_KEY = 'token' # Names of environment variables that secrets should be mounted to in the # container ES_USER_ENV = \"ES_USER\" ES_PASS_ENV = \"ES_PASS\" GITLAB_DEPLOY_TOKEN_ENV = \"GITLAB_DEPLOY_TOKEN\" # ... container_operation = dsl . ContainerOp ( name = 'some-op' , image = 'some-image' , ) . add_env_variable ( k8s_client . V1EnvVar ( name = ES_USER_ENV , value_from = k8s_client . V1EnvVarSource ( secret_key_ref = k8s_client . V1SecretKeySelector ( name = ES_CREDENTIALS_STORE , key = ES_USER_KEY ) ) ) ) \\ . add_env_variable ( k8s_client . V1EnvVar ( name = ES_PASS_ENV , value_from = k8s_client . V1EnvVarSource ( secret_key_ref = k8s_client . V1SecretKeySelector ( name = ES_CREDENTIALS_STORE , key = ES_PASSWORD_KEY ) ) ) ) \\ . add_env_variable ( k8s_client . V1EnvVar ( name = GITLAB_DEPLOY_TOKEN_ENV , value_from = k8s_client . V1EnvVarSource ( secret_key_ref = k8s_client . V1SecretKeySelector ( name = GITLAB_CREDENTIALS_STORE , key = GITLAB_DEPLOY_TOKEN_KEY ) ) ) ) Parameterizing pipelines \u00b6 Whenever possible, create pipelines in a generic way: define parameters that might change as pipeline inputs instead of writing values directly in your Python code. For example, if you want a pipeline to process data from minimal-tenant/john-smith/data1.csv , don't hard code that path - instead, accept it as a pipeline parameter. This way you can call the same pipeline repeatedly by passing it the data location as an argument. You can see this approach in our example notebooks , where we accept MinIO credentials and the location to store our results as pipeline parameters. Passing complex/large data to/from a pipeline \u00b6 Although small data can often be stringified, passing by string is not suitable for complex data (large parquet files, images, etc.). It is common to use blob storage (for example: MinIO ) or other outside storage methods to persist data between components or even for later use. A typical pattern would be: Upload large/complex input data to blob storage (e.g. training data, a saved model, etc.) Pass the location of this data into the pipeline as parameters, and make your pipeline/components fetch the data as required For each component in a pipeline, specify where they place outputs in the same way For each component also return the path where it has stored its data (in this case, the string we passed it in the above bullet). This feels redundant, but it is a common pattern that lets you chain operations together Here is a schematic example of this pattern: def my_blobby_pipeline ( path_to_numbers_1 , path_to_numbers_2 , path_for_output ): \"\"\" Averaging pipeline which accepts two groups of numbers and does some averaging operations on them \"\"\" # Compute averages for two groups avg_1 = average_op_that_takes_path_to_blob ( path_to_numbers = path_to_numbers_1 , output_location = path_for_output + \"/avg_1\" ) avg_2 = average_op_that_takes_path_to_blob ( numbers = path_to_numbers_2 , output_location = path_for_output + \"/avg_2\" ) # Note that this assumes the average_op can take multiple paths to numbers. You could also have an # aggregation component that combines avg_1 and avg_2 into a single file of numbers paths_to_numbers = [ avg_1 . output , avg_2 . output ] average_result_overall = average_op ( path_to_numbers = paths_to_numbers , output_location = path_for_output + \"/average_result_overall\" ) Within this platform, the primary method for persisting large files is through MinIO as described in our Storage documentation . Examples of this are also described in our example notebooks (also found in jupyter-notebooks/self-serve-storage/ on any notebook server). Typical development patterns \u00b6 End-to-end pipeline development \u00b6 A typical pattern for building pipelines in Kubeflow Pipelines is: Define components for each of your tasks Compose your components in a @dsl.pipeline decorated function compile() your pipeline, upload your YAML files, and run This pattern lets you define portable components that can be individually tested before combining them into a full pipeline. Depending on the type and complexity of task, there are different methods for building the components. Methods for authoring components \u00b6 Fundamentally, every component in Kubeflow Pipelines runs a container. Kubeflow Pipelines offers several methods to define these components with different levels of flexibility and complexity. User-defined container components \u00b6 You can define tasks through custom Docker images. The design pattern for this is: Define (update) code for your task and commit to Git Build an image from your task (through manual command or CI pipeline) Test running this Docker image locally (and iterate if needed) Push the image to a container registry (usually Docker hub, but it will be Azure Container Registry in our case on the Advanced Analytics Workspace) Update the Kubeflow Pipeline to point to the new image (via dsl.ContainerOp like above) and test the pipeline This lets you run anything you can put into a Docker image as a task in Kubeflow Pipelines. You can manage and test your images and have complete control over how they run and what dependencies use. The docker run interface for each container becomes the API that Kubeflow Pipelines dsl.ContainerOp interacts with \u2013 running the containers is effectively like running them locally using a terminal. Anything you can make into a container with that interface can be run in Kubeflow Pipelines. !!! danger \"...however, for security reasons the platform currently does not allow users to build/run custom Docker images. This is planned for the future, but in interim see Lightweight components for a way to develop pipelines without custom images\" Lightweight Python components \u00b6 While full custom containers offer great flexibility, sometimes they're heavier than needed. The Kubeflow Pipelines SDK also allows for Lightweight Python Components , which are components that can be built straight from Python without building new container images for each change. These components are great for fast iteration during development, as well as for simple tasks that can be written and managed easily. This is an example of a lightweight pipeline with a single component that concatenates strings: import kfp from kfp import dsl from kfp.components import func_to_container_op def concat_string ( a , b ) -> str : return f \"( { a } | { b } )\" concat_string_component = func_to_container_op ( concat_string , base_image = \"python:3.8.3-buster\" ) @dsl . pipeline ( name = \"My lightweight pipeline\" , ) def pipeline ( str1 , str2 , str3 ): # Note that we use the concat_string_component, not the # original concat_string() function concat_result_1 = concat_string_component ( str1 , str2 ) # By using cancat_result_1's output, we define the dependency of # concat_result_2 on concat_result_1 concat_result_2 = concat_string_component ( concat_result_1 . output , str3 ) We see that our concat_string component is defined directly in Python rather than from a Docker image. In the end, our function still runs in a container, but we don't have to built it ourselves: func_to_container_op() runs our Python code inside the provided base image ( python:3.8.3-buster ). This lets use avoid building every time we change our code. The base image can be anything accessible by Kubeflow, which includes all images in the Azure Container Registry and any whitelisted images from Docker hub. Lightweight components have a number of advantages but also some drawbacks See this description of their basic characteristics, as well as this example which uses them in a more complex pipeline A convenient base image to use is the the image your notebook server is running By using the same image as your notebook server, you ensure Kubeflow Pipelines has the same packages available to it as the notebook where you do your analysis. This can help avoid errors from importing packages specific to your environment. You can find that link from the notebook server page as shown below, but make sure you prepend the registry URL (so the below image would have base_image=k8scc01covidacr.azurecr.io/machine-learning-notebook-cpu:562fa4a2899eeb9ae345c51c2491447ec31a87d7 ). Note that while using a fully featured base image for iteration is fine, it's good practice to keep production pipelines lean and only supply the necessary software. That way you reduce the startup time for each step in your pipeline. Defining components directly in YAML \u00b6 Components can be defined directly with a YAML file, where the designer can run terminal commands from a given Docker image. This can be a great way to make non-Python pipeline components from existing containers. As with all components, we can pass both arguments and data files into/out of the component. For example: name : Concat Strings inputs : - { name : Input text 1 , type : String , description : \"Some text to echo into a terminal and tee to a file\" , } - { name : Input text 2 , type : String , description : \"Some text to echo into a terminal and tee to a file\" , } outputs : - { name : Output filename , type : String } implementation : container : image : bash:5 command : - bash - -ex - -c - | echo \"$0 | $1\" | tee $2 - { inputValue : Input text 1 } - { inputValue : Input text 2 } - { outputPath : Output filename } This example concatenates two strings like our lightweight example above. We then define a component in python from this YAML: from kfp.components import load_component_from_file echo_and_tee = load_component_from_file ( 'path/to/echo_and_tee.yaml' ) @dsl . pipeline def my_pipeline (): echo_and_tee_task_1 = echo_and_tee ( \"My text to echo\" ) # A second use that consumes the return of the first one echo_and_tee_task_2 = echo_and_tee ( echo_and_tee_task_1 . output ) See this example for more details on using existing components. Reusing existing components \u00b6 Similar to well abstracted functions, well abstracted components can reduce the amount of code you have to write for any given project. For example, rather than teaching your machine learning train_model component to also save the resulting model to MinIO, you can instead have train_model return the model and then Kubeflow Pipelines can pass the model to a reusable copy_to_minio component. This reuse pattern applies to components defined through any means (containers, lightweight, or YAML). Take a look at our example notebook , which reuses provided components for simple file IO tasks.","title":"Kubeflow Pipelines"},{"location":"3-Pipelines/Kubeflow-Pipelines/#overview","text":"Kubeflow pipelines are in the process of being removed from AAW. No new development should use Kubeflow pipelines. If you have questions about this removal, please speak with the AAW maintainers. Kubeflow Pipelines is a platform for building machine learning workflows for deployment in a Kubernetes environment. It enables authoring pipelines that encapsulate analytical workflows (transforming data, training models, building visuals, etc.). These pipelines can be shared, reused, and scheduled, and are built to run on compute provided via Kubernetes. Here is an example of a pipeline with many sample steps feeding into a single average step. This image comes from the Kubeflow Pipelines UI. In the context of the Advanced Analytics Workspace, Kubeflow Pipelines are interacted with through: The Kubeflow UI , where from the Pipelines menu you can upload pipelines, view the pipelines you have and their results, etc. The Kubeflow Pipelines python SDK , accessible through the Jupyter Notebook Servers , where you can define your components and pipelines, submit them to run now, or even save them for later. More examples in the notebooks More comprehensive pipeline examples specifically made for this platform are available on GitHub (and in every Notebook Server at /jupyter-notebooks ). You can also check out public sources . See the official Kubeflow docs for a more detailed explanation of Kubeflow Pipelines.","title":"Overview"},{"location":"3-Pipelines/Kubeflow-Pipelines/#what-are-pipelines-and-how-do-they-work","text":"A pipeline in Kubeflow Pipelines consists of one or more pipeline components chained together to form a workflow. The components are like functions, describing the individual steps in your workflow (such as pulling columns from a data store, transforming data, or training a model). The pipeline is the logic that glues components together, such as: Run Component-A Pass the output from Component-A to Component-B and Component-C ... In the above image, the logic would be running many sample steps followed by a single average step. At their core, each component has: A standalone application, packaged as a Docker image , for doing the actual work. The code in the Docker image could be a shell script, Python script, or anything else you can run from a Linux terminal, and generally will have a command line interface for data exchange (accessible through docker run ) A YAML file that describes how Kubeflow Pipelines runs this code (what Docker image should be run, what command line arguments does it accept, what output does it generate) Each component should be single purpose , modular , and reusable .","title":"What are pipelines and how do they work?"},{"location":"3-Pipelines/Kubeflow-Pipelines/#setup","text":"","title":"Setup"},{"location":"3-Pipelines/Kubeflow-Pipelines/#define-and-run-your-first-pipeline-using-the-python-sdk","text":"While pipelines and components are defined in Kubeflow Pipelines by YAML files that use Docker images, that does not mean we have to work directly with either YAML files or Docker images. The Kubeflow Pipelines SDK provides a way for us to define our pipeline and components directly in Python code, where the SDK then translates our Python code to YAML files for us. For our first example, let's define a simple pipeline using only the Python SDK. The purpose of this section is to give a high level view of component and pipeline authoring, not a deep dive. More detailed looks into defining your own components , passing data between components , and returning data from your pipeline are explained in more detail in further sections. The demo pipeline we define will do the following: Accept five numbers as arguments Average of the first three numbers Average of the last two numbers Average of the results of (2) and (3) To do this, we will first define our component . Our average component will call a Docker image that does the following: Accepts one or more numbers as command line arguments Returns the average of these numbers by writing them to an output file in the container (by default, to out.txt ) This Docker image is already built for us and stored in our container registry here: k8scc01covidacr.azurecr.io/kfp-components/average:v1 . Don't worry if you don't know Docker - since the image is built already, we only have to tell Kubeflow Pipelines where it is. ??? info \"Full details of the average component's Docker image are in GitHub \" This image effectively runs the following code (slightly cleaned up for brevity). By making average.py accept an arbitrary set of numbers as inputs, we can use the same average component for all steps in our pipeline : import argparse def parse_args (): parser = argparse . ArgumentParser ( description = \"Returns the average of one or \" \"more numbers as a JSON file\" ) parser . add_argument ( \"numbers\" , type = float , nargs = \"+\" , help = \"One or more numbers\" ) parser . add_argument ( \"--output_file\" , type = str , default = \"out.txt\" , help = \"Filename \" \"to write output number to\" ) return parser . parse_args () if __name__ == '__main__' : args = parse_args () numbers = args . numbers output_file = args . output_file print ( f \"Averaging numbers: { numbers } \" ) avg = sum ( numbers ) / len ( numbers ) print ( f \"Result = { avg } \" ) print ( f \"Writing output to { output_file } \" ) with open ( output_file , 'w' ) as fout : fout . write ( str ( avg )) print ( \"Done\" ) To make our average image into a Kubeflow Pipelines component , we make a kfp.dsl.ContainerOp in Python that defines how Kubeflow Pipelines interacts with our container, specifying: The Docker image location to use How to pass arguments to the running container What outputs to expect from the container We could use ContainerOp directly, but since we'll use average a few times we instead create a factory function we can reuse: from kfp import dsl def average_op ( * numbers ): \"\"\" Factory for average ContainerOps Accepts an arbitrary number of input numbers, returning a ContainerOp that passes those numbers to the underlying Docker image for averaging Returns output collected from ./out.txt from inside the container \"\"\" # Input validation if len ( numbers ) < 1 : raise ValueError ( \"Must specify at least one number to take the average of\" ) return dsl . ContainerOp ( name = \"average\" , # What will show up on the pipeline viewer image = \"k8scc01covidacr.azurecr.io/kfp-components/average:v1\" , # The image that KFP runs to do the work arguments = numbers , # Passes each number as a separate command line argument # Note that these arguments get serialized to strings file_outputs = { 'data' : './out.txt' }, # Expect an output file called out.txt to be generated # KFP can read this file and bring it back automatically ) To define our pipeline, we create a Python function decorated by the @dsl.pipeline decorator. We invoke our average_op factory to use our average container. We pass each average some inputs, and even use their outputs by accessing avg_*.output . @dsl . pipeline ( name = \"my pipeline's name\" ) def my_pipeline ( a , b , c , d , e ): \"\"\" Averaging pipeline which accepts five numbers and does some averaging operations on them \"\"\" # Compute averages for two groups avg_1 = average_op ( a , b , c ) avg_2 = average_op ( d , e ) # Use the results from _1 and _2 to compute an overall average average_result_overall = average_op ( avg_1 . output , avg_2 . output ) Finally, while we've defined our pipeline in Python, Kubeflow Pipelines itself needs everything defined as a YAML file. This final step uses the Kubeflow Pipelines Python SDK to translate our pipeline function into a YAML file that describes exactly how Kubeflow Pipelines can interact with our component. Unzip it and take a look for yourself! from kfp import compiler pipeline_yaml = 'pipeline.yaml.zip' compiler . Compiler () . compile ( my_pipeline , pipeline_yaml ) print ( f \"Exported pipeline definition to { pipeline_yaml } \" ) Kubeflow Pipelines is a lazy beast It is useful to keep in mind what computation is happening when you run this python code versus what happens when you submit the pipeline to Kubeflow Pipelines. Although it seems like everything is happening in the moment, try adding print(avg_1.output) to the above pipeline and see what happens when you compile your pipeline. The Python SDK we're using is for authoring pipelines, not for running them, so results from components will never be available when you run this Python code. The is discussed more below in Understanding what computation occurs when . To actually run our pipeline, we define an experiment: experiment_name = \"averaging-pipeline\" import kfp client = kfp . Client () exp = client . create_experiment ( name = experiment_name ) pl_params = { 'a' : 5 , 'b' : 5 , 'c' : 8 , 'd' : 10 , 'e' : 18 , } And then run an instance of our pipeline with the arguments we want: import time run = client . run_pipeline ( exp . id , # Run inside the above experiment experiment_name + '-' + time . strftime ( \"%Y%m %d -%H%M%S\" ), # Give our job a name with a timestamp so its unique pipeline_yaml , # Pass the .yaml.zip we created above. This defines the pipeline params = pl_params # Pass our parameters we want to run the pipeline with ) This can all be seen in the Kubeflow Pipelines UI : Later when we want to reuse the pipeline, we can pass different arguments and do it all again. !!! info \"We create our experiment, upload our pipeline, and run from Python in this example, but we could also do all this through the Kubeflow Pipelines UI above.","title":"Define and run your first pipeline using the Python SDK"},{"location":"3-Pipelines/Kubeflow-Pipelines/#understanding-what-computation-occurs-when","text":"The above example uses Python code to define: The interface between Kubeflow Pipelines and our Docker containers doing the work (by defining ContainerOp 's) The logic of our pipeline (by defining my_pipeline ). But when we run compiler.Compiler().compile() and client.run_pipeline() , what actually happens? It is important to remember that everything we run in Python here is specifying the pipeline and its components in order to write YAML definitions, it is not doing the work of the pipeline. When running compiler.Compiler().compile() , we are not running our pipeline in the typical sense. Instead, KFP uses my_pipeline to build a YAML version of it. When we compile , the KFP SDK is passing placeholder arguments to my_pipeline and tracing where they (and any other runtime data) go, such as any output a component produces. When compile encounters a ContainerOp , nothing runs now - instead it takes note that a container will be there in future and remembers what data it will consume/generate. This can be seen by modifying and recompiling our pipeline: @dsl . pipeline ( name = \"my pipeline's name\" ) def my_pipeline ( a , b , c , d , e ): \"\"\" Averaging pipeline which accepts five numbers and does some averaging operations on them \"\"\" # NEW CODE x = 1 + 1 print ( f \"The value of x is { x } \" ) print ( f \"The value of a is { a } \" ) # Compute averages for two groups avg_1 = average_op ( a , b , c ) avg_2 = average_op ( d , e ) # NEW CODE print ( f \"The value of avg_1.output is { avg_1 . output } \" ) # Use the results from _1 and _2 to compute an overall average average_result_overall = average_op ( avg_1 . output , avg_2 . output ) And when we compile , we see print statements: The value of x is 2 The value of a is {{ pipelineparam : op = ; name = a }} The value of avg_1.output is {{ pipelineparam : op = averge ; name = data }} In the first print statement everything is normal. In the second and third print statements, however, we see string placeholders rather than actual output. So while compile does \"execute\" my_pipeline , the KFP-specific parts of the code don't actually generate results. This can also be seen in the YAML file that compile generates, for example looking at the portion defining our average_result_overall component: - name : average-3 container : args : [ \"{{inputs.parameters.average-data}}\" , \"{{inputs.parameters.average-2-data}}\" , ] image : k8scc01covidacr.azurecr.io/kfp-components/average:v1 inputs : parameters : - { name : average-2-data } - { name : average-data } outputs : artifacts : - { name : average-3-data , path : ./out.txt } metadata : labels : { pipelines.kubeflow.org/pipeline-sdk-type : kfp } In this YAML we see the input parameters passed are placeholders for data from previous components rather than their actual value. This is because while KFP knows a result from average-data and average-2-data will be passed to average, but the value of that result is not available until the pipeline is actually run. Component naming within the YAML file Because we made an average_op factory function with name='average' above, our YAML file has component names that automatically increment to avoid recreating the same name twice. We could have been fancier with our factory functions to more directly control our names, giving an argument like name='average_first_input_args' , or could even have explicitly defined the name in our pipeline by using avg_1 = average_op(a, b, c).set_display_name(\"Average 1\") . As one more example, let's try two more pipelines. One has a for loop inside which prints \"Woohoo!\" a fixed number of times. whereas the other does the same but loops n times (where n is a pipeline parameter): !!! info \"Pipeline parameters are described more below, but they work like parameters for functions. Pipelines can accept data (numbers, string URL's to large files in MinIO, etc.) as arguments, allowing a single generic pipeline to work in many situations.\" @dsl . pipeline ( name = \"my pipeline's name\" ) def another_pipeline (): \"\"\" Prints to the screen 10 times \"\"\" for i in range ( 10 ): print ( \"Woohoo!\" ) # And just so we've got some component going too... avg = average_op ( n ) compiler . Compiler () . compile ( another_pipeline , \"another.yaml.zip\" ) @dsl . pipeline ( name = \"my pipeline's name\" ) def another_another_pipeline ( n ): \"\"\" Prints to the screen n times \"\"\" for i in range ( n ): print ( \"Woohoo!\" ) # And just so we've got some component going too... avg = average_op ( n ) compiler . Compiler () . compile ( another_another_pipeline , \"another.yaml.zip\" ) The first works as you'd expect, but the second raises the exception: TypeError: 'PipelineParam' object cannot be interpreted as an integer Why? Because when authoring the pipeline n is a placeholder and has no value. KFP cannot define a pipeline from this because it does not know how many times to loop. We'd hit similar problems if using if statements. There are some ways around this, but they're left to the reader to explore through the Kubeflow Pipelines docs . Why does pipeline authoring behave this way? Because pipelines (and components ) are meant to be reusable definitions of logic that are defined in static YAML files, with all dynamic decision making done inside components. This can make them a little awkward to define, but also helps them be more reusable.","title":"Understanding what computation occurs when"},{"location":"3-Pipelines/Kubeflow-Pipelines/#once-youve-got-the-basics","text":"","title":"Once you've got the basics ..."},{"location":"3-Pipelines/Kubeflow-Pipelines/#data-exchange","text":"","title":"Data Exchange"},{"location":"3-Pipelines/Kubeflow-Pipelines/#passing-data-into-within-and-from-a-pipeline","text":"In the first example above, we pass: Numbers into our pipeline Numbers between components within our pipeline A number back to the user at the end But as discussed above, pipeline arguments and component results are just placeholder objects \u2013 so how does KFP know our values are numeric? The answer is: it doesn't. In fact, it didn't even treat them as numbers above. Instead it treated them as strings. It is just that our pipeline components worked just as well with \"5\" as they would have with 5 . A safe default assumption is that all data exchange happens as a string. When we passed a, b, ... into the pipeline, those numbers were implicitly stringified because they eventually become command line arguments for our Docker container. When we read the result of avg_1 from its out.txt , that result was read as a string. By calling average_op(avg_1.output, avg_2.output) , we ask KFP to pass the string output from avg_1 and avg_2 to a new average_op . It just so happened that, since average_op passes each string as a command line argument to our Docker image, it didn't really matter they were strings. You can still use non-string data types, but you need to pass them as serialized versions. So if we wanted our avg_1 component to return both the numbers passed to it and the average returned as a dictionary, for example: { 'numbers' : [ 5 , 5 , 8 ], 'result' : 6.0 , } We could modify our average.py in the Docker image write our dictionary of numbers and result to out.txt as JSON. But then when we pass the result to make average_result_overall , that component needs to deserialize the above JSON and pull the data from it that it needs. And because these results are not available when authoring the pipeline, something like this does not work: def my_pipeline ( a , b , c , d , e ): \"\"\" Averaging pipeline which accepts five numbers and does some averaging operations on them \"\"\" # Compute averages for two groups avg_1 = average_op_that_returns_json ( a , b , c ) avg_2 = average_op_that_returns_json ( d , e ) # THIS DOES NOT WORK! import json avg_1_result = json . loads ( avg_1 . output )[ 'result' ] avg_2_result = json . loads ( avg_2 . output )[ 'result' ] # Use the results from _1 and _2 to compute an overall average average_result_overall = average_op ( avg_1 . output , avg_2 . output ) At compile time, avg_1.output is just a placeholder and can't be treated like the JSON it will eventually become. To do something like this, we need to interpret the JSON string within a container.","title":"Passing data into, within, and from a pipeline"},{"location":"3-Pipelines/Kubeflow-Pipelines/#passing-secrets","text":"Pipelines often need sensitive information (passwords, API keys, etc.) to operate. To keep these secure, these cannot be passed to a Kubeflow Pipeline using a pipeline argument, environment variable, or as a hard coded value in python code, as they will be exposed as plain text for others to read. To address this issue, we use Kubernetes secrets as a way to securely store and pass sensitive information. Each secret is a key-value store containing some number of key-value pairs. The secrets can be passed by reference to the pipeline. Secrets are only accessible within their own namespace Secrets can only be accessed within the namespace they are created in. If you need to use a secret in another namespace, you need to add it there manually.","title":"Passing Secrets"},{"location":"3-Pipelines/Kubeflow-Pipelines/#create-a-key-value-store","text":"The example below creates a key-value store called elastic-credentials which contains two key-value pairs: \"username\": \"USERNAME\", \"password\": \"PASSWORD\" kubectl create secret generic elastic-credentials \\ --from-literal = username = \"YOUR_USERNAME\" \\ --from-literal = password = \"YOUR_PASSWORD\"","title":"Create a key-value store"},{"location":"3-Pipelines/Kubeflow-Pipelines/#get-an-existing-key-value-store","text":"# The secrets will be base64 encoded. kubectl get secret elastic-credentials","title":"Get an existing key-value store"},{"location":"3-Pipelines/Kubeflow-Pipelines/#mounting-kubernetes-secrets-to-environment-variables-in-container-operations","text":"Once the secrets are defined in the project namespace, you can mount specific secrets as environment variables in your container using the Kubeflow SDK. Example This example is based off of a snippet from the Python KFP source code . This example shows how (1) an Elasticsearch username(2) an Elasticsearch password, and (3) a GitLab deploy token are passed to the container operation as environment variables. # Names of k8s secret key-value stores ES_CREDENTIALS_STORE = \"elastic-credentials\" GITLAB_CREDENTIALS_STORE = \"gitlab-credentials\" # k8s secrets key names ES_USER_KEY = \"username\" ES_PASSWORD_KEY = \"password\" GITLAB_DEPLOY_TOKEN_KEY = 'token' # Names of environment variables that secrets should be mounted to in the # container ES_USER_ENV = \"ES_USER\" ES_PASS_ENV = \"ES_PASS\" GITLAB_DEPLOY_TOKEN_ENV = \"GITLAB_DEPLOY_TOKEN\" # ... container_operation = dsl . ContainerOp ( name = 'some-op' , image = 'some-image' , ) . add_env_variable ( k8s_client . V1EnvVar ( name = ES_USER_ENV , value_from = k8s_client . V1EnvVarSource ( secret_key_ref = k8s_client . V1SecretKeySelector ( name = ES_CREDENTIALS_STORE , key = ES_USER_KEY ) ) ) ) \\ . add_env_variable ( k8s_client . V1EnvVar ( name = ES_PASS_ENV , value_from = k8s_client . V1EnvVarSource ( secret_key_ref = k8s_client . V1SecretKeySelector ( name = ES_CREDENTIALS_STORE , key = ES_PASSWORD_KEY ) ) ) ) \\ . add_env_variable ( k8s_client . V1EnvVar ( name = GITLAB_DEPLOY_TOKEN_ENV , value_from = k8s_client . V1EnvVarSource ( secret_key_ref = k8s_client . V1SecretKeySelector ( name = GITLAB_CREDENTIALS_STORE , key = GITLAB_DEPLOY_TOKEN_KEY ) ) ) )","title":"Mounting Kubernetes Secrets to Environment Variables in Container Operations"},{"location":"3-Pipelines/Kubeflow-Pipelines/#parameterizing-pipelines","text":"Whenever possible, create pipelines in a generic way: define parameters that might change as pipeline inputs instead of writing values directly in your Python code. For example, if you want a pipeline to process data from minimal-tenant/john-smith/data1.csv , don't hard code that path - instead, accept it as a pipeline parameter. This way you can call the same pipeline repeatedly by passing it the data location as an argument. You can see this approach in our example notebooks , where we accept MinIO credentials and the location to store our results as pipeline parameters.","title":"Parameterizing pipelines"},{"location":"3-Pipelines/Kubeflow-Pipelines/#passing-complexlarge-data-tofrom-a-pipeline","text":"Although small data can often be stringified, passing by string is not suitable for complex data (large parquet files, images, etc.). It is common to use blob storage (for example: MinIO ) or other outside storage methods to persist data between components or even for later use. A typical pattern would be: Upload large/complex input data to blob storage (e.g. training data, a saved model, etc.) Pass the location of this data into the pipeline as parameters, and make your pipeline/components fetch the data as required For each component in a pipeline, specify where they place outputs in the same way For each component also return the path where it has stored its data (in this case, the string we passed it in the above bullet). This feels redundant, but it is a common pattern that lets you chain operations together Here is a schematic example of this pattern: def my_blobby_pipeline ( path_to_numbers_1 , path_to_numbers_2 , path_for_output ): \"\"\" Averaging pipeline which accepts two groups of numbers and does some averaging operations on them \"\"\" # Compute averages for two groups avg_1 = average_op_that_takes_path_to_blob ( path_to_numbers = path_to_numbers_1 , output_location = path_for_output + \"/avg_1\" ) avg_2 = average_op_that_takes_path_to_blob ( numbers = path_to_numbers_2 , output_location = path_for_output + \"/avg_2\" ) # Note that this assumes the average_op can take multiple paths to numbers. You could also have an # aggregation component that combines avg_1 and avg_2 into a single file of numbers paths_to_numbers = [ avg_1 . output , avg_2 . output ] average_result_overall = average_op ( path_to_numbers = paths_to_numbers , output_location = path_for_output + \"/average_result_overall\" ) Within this platform, the primary method for persisting large files is through MinIO as described in our Storage documentation . Examples of this are also described in our example notebooks (also found in jupyter-notebooks/self-serve-storage/ on any notebook server).","title":"Passing complex/large data to/from a pipeline"},{"location":"3-Pipelines/Kubeflow-Pipelines/#typical-development-patterns","text":"","title":"Typical development patterns"},{"location":"3-Pipelines/Kubeflow-Pipelines/#end-to-end-pipeline-development","text":"A typical pattern for building pipelines in Kubeflow Pipelines is: Define components for each of your tasks Compose your components in a @dsl.pipeline decorated function compile() your pipeline, upload your YAML files, and run This pattern lets you define portable components that can be individually tested before combining them into a full pipeline. Depending on the type and complexity of task, there are different methods for building the components.","title":"End-to-end pipeline development"},{"location":"3-Pipelines/Kubeflow-Pipelines/#methods-for-authoring-components","text":"Fundamentally, every component in Kubeflow Pipelines runs a container. Kubeflow Pipelines offers several methods to define these components with different levels of flexibility and complexity.","title":"Methods for authoring components"},{"location":"3-Pipelines/Kubeflow-Pipelines/#user-defined-container-components","text":"You can define tasks through custom Docker images. The design pattern for this is: Define (update) code for your task and commit to Git Build an image from your task (through manual command or CI pipeline) Test running this Docker image locally (and iterate if needed) Push the image to a container registry (usually Docker hub, but it will be Azure Container Registry in our case on the Advanced Analytics Workspace) Update the Kubeflow Pipeline to point to the new image (via dsl.ContainerOp like above) and test the pipeline This lets you run anything you can put into a Docker image as a task in Kubeflow Pipelines. You can manage and test your images and have complete control over how they run and what dependencies use. The docker run interface for each container becomes the API that Kubeflow Pipelines dsl.ContainerOp interacts with \u2013 running the containers is effectively like running them locally using a terminal. Anything you can make into a container with that interface can be run in Kubeflow Pipelines. !!! danger \"...however, for security reasons the platform currently does not allow users to build/run custom Docker images. This is planned for the future, but in interim see Lightweight components for a way to develop pipelines without custom images\"","title":"User-defined container components"},{"location":"3-Pipelines/Kubeflow-Pipelines/#lightweight-python-components","text":"While full custom containers offer great flexibility, sometimes they're heavier than needed. The Kubeflow Pipelines SDK also allows for Lightweight Python Components , which are components that can be built straight from Python without building new container images for each change. These components are great for fast iteration during development, as well as for simple tasks that can be written and managed easily. This is an example of a lightweight pipeline with a single component that concatenates strings: import kfp from kfp import dsl from kfp.components import func_to_container_op def concat_string ( a , b ) -> str : return f \"( { a } | { b } )\" concat_string_component = func_to_container_op ( concat_string , base_image = \"python:3.8.3-buster\" ) @dsl . pipeline ( name = \"My lightweight pipeline\" , ) def pipeline ( str1 , str2 , str3 ): # Note that we use the concat_string_component, not the # original concat_string() function concat_result_1 = concat_string_component ( str1 , str2 ) # By using cancat_result_1's output, we define the dependency of # concat_result_2 on concat_result_1 concat_result_2 = concat_string_component ( concat_result_1 . output , str3 ) We see that our concat_string component is defined directly in Python rather than from a Docker image. In the end, our function still runs in a container, but we don't have to built it ourselves: func_to_container_op() runs our Python code inside the provided base image ( python:3.8.3-buster ). This lets use avoid building every time we change our code. The base image can be anything accessible by Kubeflow, which includes all images in the Azure Container Registry and any whitelisted images from Docker hub. Lightweight components have a number of advantages but also some drawbacks See this description of their basic characteristics, as well as this example which uses them in a more complex pipeline A convenient base image to use is the the image your notebook server is running By using the same image as your notebook server, you ensure Kubeflow Pipelines has the same packages available to it as the notebook where you do your analysis. This can help avoid errors from importing packages specific to your environment. You can find that link from the notebook server page as shown below, but make sure you prepend the registry URL (so the below image would have base_image=k8scc01covidacr.azurecr.io/machine-learning-notebook-cpu:562fa4a2899eeb9ae345c51c2491447ec31a87d7 ). Note that while using a fully featured base image for iteration is fine, it's good practice to keep production pipelines lean and only supply the necessary software. That way you reduce the startup time for each step in your pipeline.","title":"Lightweight Python components"},{"location":"3-Pipelines/Kubeflow-Pipelines/#defining-components-directly-in-yaml","text":"Components can be defined directly with a YAML file, where the designer can run terminal commands from a given Docker image. This can be a great way to make non-Python pipeline components from existing containers. As with all components, we can pass both arguments and data files into/out of the component. For example: name : Concat Strings inputs : - { name : Input text 1 , type : String , description : \"Some text to echo into a terminal and tee to a file\" , } - { name : Input text 2 , type : String , description : \"Some text to echo into a terminal and tee to a file\" , } outputs : - { name : Output filename , type : String } implementation : container : image : bash:5 command : - bash - -ex - -c - | echo \"$0 | $1\" | tee $2 - { inputValue : Input text 1 } - { inputValue : Input text 2 } - { outputPath : Output filename } This example concatenates two strings like our lightweight example above. We then define a component in python from this YAML: from kfp.components import load_component_from_file echo_and_tee = load_component_from_file ( 'path/to/echo_and_tee.yaml' ) @dsl . pipeline def my_pipeline (): echo_and_tee_task_1 = echo_and_tee ( \"My text to echo\" ) # A second use that consumes the return of the first one echo_and_tee_task_2 = echo_and_tee ( echo_and_tee_task_1 . output ) See this example for more details on using existing components.","title":"Defining components directly in YAML"},{"location":"3-Pipelines/Kubeflow-Pipelines/#reusing-existing-components","text":"Similar to well abstracted functions, well abstracted components can reduce the amount of code you have to write for any given project. For example, rather than teaching your machine learning train_model component to also save the resulting model to MinIO, you can instead have train_model return the model and then Kubeflow Pipelines can pass the model to a reusable copy_to_minio component. This reuse pattern applies to components defined through any means (containers, lightweight, or YAML). Take a look at our example notebook , which reuses provided components for simple file IO tasks.","title":"Reusing existing components"},{"location":"3-Pipelines/PaaS/","text":"Overview \u00b6 Integrate with Platforms like Databricks and AzureML \u00b6 The AAW platform is built around the idea of integrations, and so we can integrate with many Platform as a Service (PaaS) offerings, such as Azure ML and Databricks . See some examples on our \"MLOps\" github Repo . Setup \u00b6 If you need help integrating with a platform as a service offering, we're happy to help!","title":"PaaS Integration"},{"location":"3-Pipelines/PaaS/#overview","text":"","title":"Overview"},{"location":"3-Pipelines/PaaS/#integrate-with-platforms-like-databricks-and-azureml","text":"The AAW platform is built around the idea of integrations, and so we can integrate with many Platform as a Service (PaaS) offerings, such as Azure ML and Databricks . See some examples on our \"MLOps\" github Repo .","title":"Integrate with Platforms like Databricks and AzureML"},{"location":"3-Pipelines/PaaS/#setup","text":"If you need help integrating with a platform as a service offering, we're happy to help!","title":"Setup"},{"location":"3-Pipelines/Serving/","text":"Model Serving with Seldon Core and KFServing \u00b6 \u2692 This page is under construction \u2692 The person writing this entry does not know enough about this feature to write about it, but you can ask on our Slack channel. Serverless with KNative \u00b6 Kubernetes and KNative let your services scale up and down on demand. This lets you create APIs to serve Machine Learning models, without the need to manage load balancing or scale-up. The platform can handle all of your scaling for you, so that you can focus on the program logic. \u2692 This page is under construction \u2692 The person writing this entry does not know enough about this feature to write about it, but you can ask on our Slack channel.","title":"Model Serving"},{"location":"3-Pipelines/Serving/#model-serving-with-seldon-core-and-kfserving","text":"\u2692 This page is under construction \u2692 The person writing this entry does not know enough about this feature to write about it, but you can ask on our Slack channel.","title":"Model Serving with Seldon Core and KFServing"},{"location":"3-Pipelines/Serving/#serverless-with-knative","text":"Kubernetes and KNative let your services scale up and down on demand. This lets you create APIs to serve Machine Learning models, without the need to manage load balancing or scale-up. The platform can handle all of your scaling for you, so that you can focus on the program logic. \u2692 This page is under construction \u2692 The person writing this entry does not know enough about this feature to write about it, but you can ask on our Slack channel.","title":"Serverless with KNative"},{"location":"4-Collaboration/Geospatial-Analytical-Environment/","text":"Geospatial Analytical Environment (GAE) - Cross Platform Access \u00b6 Unprotected data only; SSI coming soon: At this time, our Geospatial server can only host and provide access to non-sensitive statistical information. Getting Started \u00b6 Prerequisites An onboarded project with access to DAS GAE ArcGIS Portal An ArcGIS Portal Client Id (API Key) The ArcGIS Enterprise Portal can be accessed in either the AAW or CAE using the API, from any service which leverages the Python programming language. For example, in AAW and the use of Jupyter Notebooks within the space, or in CAE the use of Databricks , DataFactory, etc. The DAS GAE ArcGIS Enterprise Portal can be accessed directly here For help with self-registering as a DAS Geospatial Portal user Using the ArcGIS API for Python \u00b6 Connecting to ArcGIS Enterprise Portal using ArcGIS API \u00b6 Install packages: python conda install -c esri arcgis or using Artifactory python3333 conda install -c https://jfrog.aaw.cloud.statcan.ca/artifactory/api/conda/esri-remote arcgis Import the necessary libraries that you will need in the Notebook. python from arcgis.gis import GIS from arcgis.gis import Item Access the Portal Your project group will be provided with a Client ID upon onboarding. Paste the Client ID in between the quotations client_id='######' . python gis = GIS(\"https://geoanalytics.cloud.statcan.ca/portal\", client_id=' ') print(\"Successfully logged in as: \" + gis.properties.user.username) The output will redirect you to a login Portal. - Use the StatCan Azure Login option, and your Cloud ID - After successful login, you will receive a code to sign in using SAML. - Paste this code into the output. Display user information \u00b6 Using the 'me' function, we can display various information about the user logged in. me = gis . users . me username = me . username description = me . description display ( me ) Search for Content \u00b6 Search for the content you have hosted on the DAaaS Geo Portal. Using the 'me' function we can search for all of the hosted content on the account. There are multiple ways to search for content. Two different methods are outlined below. Search all of your hosted itmes in the DAaaS Geo Portal. my_content = me . items () my_content Search for specific content you own in the DAaaS Geo Portal. This is similar to the example above, however if you know the title of they layer you want to use, you can save it as a function. my_items = me . items () for items in my_items : print ( items . title , \" | \" , items . type ) if items . title == \"Flood in Sorel-Tracy\" : flood_item = items else : continue print ( flood_item ) Search all content you have access to, not just your own. flood_item = gis . content . search ( \"tags: flood\" , item_type = \"Feature Service\" ) flood_item Get Content \u00b6 We need to get the item from the DAaaS Geo Portal in order to use it in the Jupyter Notebook. This is done by providing the unique identification number of the item you want to use. Three examples are outlined below, all accessing the identical layer. item1 = gis . content . get ( my_content [ 5 ] . id ) #from searching your content above display ( item1 ) item2 = gis . content . get ( flood_item . id ) #from example above -searching for specific content display ( item2 ) item3 = gis . content . get ( 'edebfe03764b497f90cda5f0bfe727e2' ) #the actual content id number display ( item3 ) Perform Analysis \u00b6 Once the layers are brought into the Jupyter notebook, we are able to perform similar types of analysis you would expect to find in a GIS software such as ArcGIS. There are many modules containing many sub-modules of which can perform multiple types of analyses. Using the arcgis.features module, import the use_proximity submodule from arcgis.features import use_proximity . This submodule allows us to '.create_buffers' - areas of equal distance from features. Here, we specify the layer we want to use, distance, units, and output name (you may also specify other characteristics such as field, ring type, end type, and others). By specifying an output name, after running the buffer command, a new layer will be automatically uploaded into the DAaaS GEO Portal containing the new feature you just created. buffer_lyr = use_proximity . create_buffers ( item1 , distances = [ 1 ], units = \"Kilometers\" , output_name = 'item1_buffer' ) display ( item1_buffer ) Some users prefer to work with Open-Source packages. Translating from ArcGIS to Spatial Dataframes is simple. # create a Spatially Enabled DataFrame object sdf = pd . DataFrame . spatial . from_layer ( feature_layer ) Update Items \u00b6 By getting the item as we did similar to the example above, we can use the '.update' function to update exisiting item within the DAaaS GEO Portal. We can update item properties, data, thumbnails, and metadata. item1_buffer = gis . content . get ( 'c60c7e57bdb846dnbd7c8226c80414d2' ) item1_buffer . update ( item_properties = { 'title' : 'Enter Title' 'tags' : 'tag1, tag2, tag3, tag4' , 'description' : 'Enter description of item' } Visualize Your Data on an Interactive Map \u00b6 Example: MatplotLib Library In the code below, we create an ax object, which is a map style plot. We then plot our data ('Population Change') change column on the axes import matplotlib.pyplot as plt ax = sdf . boundary . plot ( figsize = ( 10 , 5 )) shape . plot ( ax = ax , column = 'Population Change' , legend = True ) plt . show () Example: ipyleaflet Library In this example we will use the library 'ipyleaflet' to create an interactive map. This map will be centered around Toronto, ON. The data being used will be outlined below. Begin by pasting conda install -c conda-forge ipyleaflet allowing you to install ipyleaflet libraries in the Python environment. Import the necessary libraries. import ipyleaflet from ipyleaflet import * Now that we have imported the ipyleaflet module, we can create a simple map by specifying the latitude and longitude of the location we want, zoom level, and basemap (more basemaps) . Extra controls have been added such as layers and scale. toronto_map = Map ( center = [ 43.69 , - 79.35 ], zoom = 11 , basemap = basemaps . Esri . WorldStreetMap ) toronto_map . add_control ( LayersControl ( position = 'topright' )) toronto_map . add_control ( ScaleControl ( position = 'bottomleft' )) toronto_map Learn More about the ArcGIS API for Python \u00b6 Full documentation for the ArGIS API can be located here Learn More about DAS Geospatial Analytical Environment (GAE) and Services \u00b6 GAE Help Guide","title":"GEA"},{"location":"4-Collaboration/Geospatial-Analytical-Environment/#geospatial-analytical-environment-gae-cross-platform-access","text":"Unprotected data only; SSI coming soon: At this time, our Geospatial server can only host and provide access to non-sensitive statistical information.","title":"Geospatial Analytical Environment (GAE) - Cross Platform Access"},{"location":"4-Collaboration/Geospatial-Analytical-Environment/#getting-started","text":"Prerequisites An onboarded project with access to DAS GAE ArcGIS Portal An ArcGIS Portal Client Id (API Key) The ArcGIS Enterprise Portal can be accessed in either the AAW or CAE using the API, from any service which leverages the Python programming language. For example, in AAW and the use of Jupyter Notebooks within the space, or in CAE the use of Databricks , DataFactory, etc. The DAS GAE ArcGIS Enterprise Portal can be accessed directly here For help with self-registering as a DAS Geospatial Portal user","title":"Getting Started"},{"location":"4-Collaboration/Geospatial-Analytical-Environment/#using-the-arcgis-api-for-python","text":"","title":"Using the ArcGIS API for Python"},{"location":"4-Collaboration/Geospatial-Analytical-Environment/#connecting-to-arcgis-enterprise-portal-using-arcgis-api","text":"Install packages: python conda install -c esri arcgis or using Artifactory python3333 conda install -c https://jfrog.aaw.cloud.statcan.ca/artifactory/api/conda/esri-remote arcgis Import the necessary libraries that you will need in the Notebook. python from arcgis.gis import GIS from arcgis.gis import Item Access the Portal Your project group will be provided with a Client ID upon onboarding. Paste the Client ID in between the quotations client_id='######' . python gis = GIS(\"https://geoanalytics.cloud.statcan.ca/portal\", client_id=' ') print(\"Successfully logged in as: \" + gis.properties.user.username) The output will redirect you to a login Portal. - Use the StatCan Azure Login option, and your Cloud ID - After successful login, you will receive a code to sign in using SAML. - Paste this code into the output.","title":"Connecting to ArcGIS Enterprise Portal using ArcGIS API"},{"location":"4-Collaboration/Geospatial-Analytical-Environment/#display-user-information","text":"Using the 'me' function, we can display various information about the user logged in. me = gis . users . me username = me . username description = me . description display ( me )","title":"Display user information"},{"location":"4-Collaboration/Geospatial-Analytical-Environment/#search-for-content","text":"Search for the content you have hosted on the DAaaS Geo Portal. Using the 'me' function we can search for all of the hosted content on the account. There are multiple ways to search for content. Two different methods are outlined below. Search all of your hosted itmes in the DAaaS Geo Portal. my_content = me . items () my_content Search for specific content you own in the DAaaS Geo Portal. This is similar to the example above, however if you know the title of they layer you want to use, you can save it as a function. my_items = me . items () for items in my_items : print ( items . title , \" | \" , items . type ) if items . title == \"Flood in Sorel-Tracy\" : flood_item = items else : continue print ( flood_item ) Search all content you have access to, not just your own. flood_item = gis . content . search ( \"tags: flood\" , item_type = \"Feature Service\" ) flood_item","title":"Search for Content"},{"location":"4-Collaboration/Geospatial-Analytical-Environment/#get-content","text":"We need to get the item from the DAaaS Geo Portal in order to use it in the Jupyter Notebook. This is done by providing the unique identification number of the item you want to use. Three examples are outlined below, all accessing the identical layer. item1 = gis . content . get ( my_content [ 5 ] . id ) #from searching your content above display ( item1 ) item2 = gis . content . get ( flood_item . id ) #from example above -searching for specific content display ( item2 ) item3 = gis . content . get ( 'edebfe03764b497f90cda5f0bfe727e2' ) #the actual content id number display ( item3 )","title":"Get Content"},{"location":"4-Collaboration/Geospatial-Analytical-Environment/#perform-analysis","text":"Once the layers are brought into the Jupyter notebook, we are able to perform similar types of analysis you would expect to find in a GIS software such as ArcGIS. There are many modules containing many sub-modules of which can perform multiple types of analyses. Using the arcgis.features module, import the use_proximity submodule from arcgis.features import use_proximity . This submodule allows us to '.create_buffers' - areas of equal distance from features. Here, we specify the layer we want to use, distance, units, and output name (you may also specify other characteristics such as field, ring type, end type, and others). By specifying an output name, after running the buffer command, a new layer will be automatically uploaded into the DAaaS GEO Portal containing the new feature you just created. buffer_lyr = use_proximity . create_buffers ( item1 , distances = [ 1 ], units = \"Kilometers\" , output_name = 'item1_buffer' ) display ( item1_buffer ) Some users prefer to work with Open-Source packages. Translating from ArcGIS to Spatial Dataframes is simple. # create a Spatially Enabled DataFrame object sdf = pd . DataFrame . spatial . from_layer ( feature_layer )","title":"Perform Analysis"},{"location":"4-Collaboration/Geospatial-Analytical-Environment/#update-items","text":"By getting the item as we did similar to the example above, we can use the '.update' function to update exisiting item within the DAaaS GEO Portal. We can update item properties, data, thumbnails, and metadata. item1_buffer = gis . content . get ( 'c60c7e57bdb846dnbd7c8226c80414d2' ) item1_buffer . update ( item_properties = { 'title' : 'Enter Title' 'tags' : 'tag1, tag2, tag3, tag4' , 'description' : 'Enter description of item' }","title":"Update Items"},{"location":"4-Collaboration/Geospatial-Analytical-Environment/#visualize-your-data-on-an-interactive-map","text":"Example: MatplotLib Library In the code below, we create an ax object, which is a map style plot. We then plot our data ('Population Change') change column on the axes import matplotlib.pyplot as plt ax = sdf . boundary . plot ( figsize = ( 10 , 5 )) shape . plot ( ax = ax , column = 'Population Change' , legend = True ) plt . show () Example: ipyleaflet Library In this example we will use the library 'ipyleaflet' to create an interactive map. This map will be centered around Toronto, ON. The data being used will be outlined below. Begin by pasting conda install -c conda-forge ipyleaflet allowing you to install ipyleaflet libraries in the Python environment. Import the necessary libraries. import ipyleaflet from ipyleaflet import * Now that we have imported the ipyleaflet module, we can create a simple map by specifying the latitude and longitude of the location we want, zoom level, and basemap (more basemaps) . Extra controls have been added such as layers and scale. toronto_map = Map ( center = [ 43.69 , - 79.35 ], zoom = 11 , basemap = basemaps . Esri . WorldStreetMap ) toronto_map . add_control ( LayersControl ( position = 'topright' )) toronto_map . add_control ( ScaleControl ( position = 'bottomleft' )) toronto_map","title":"Visualize Your Data on an Interactive Map"},{"location":"4-Collaboration/Geospatial-Analytical-Environment/#learn-more-about-the-arcgis-api-for-python","text":"Full documentation for the ArGIS API can be located here","title":"Learn More about the ArcGIS API for Python"},{"location":"4-Collaboration/Geospatial-Analytical-Environment/#learn-more-about-das-geospatial-analytical-environment-gae-and-services","text":"GAE Help Guide","title":"Learn More about DAS Geospatial Analytical Environment (GAE) and Services"},{"location":"4-Collaboration/Overview/","text":"There are many ways collaborate on the platform. Which is best for your situation depends on what you're sharing and how many people you want to share with. Scenarios roughly break down into what you want to share ( Data , Code , or Compute Environments (e.g.: sharing the same virtual machines)) and who you want to share it with ( No one , My Team , or Everyone ). This leads to the following table of options Private Team StatCan Code GitLab/GitHub or personal folder GitLab/GitHub or team folder GitLab/GitHub Data Personal folder or bucket Team folder or bucket, or shared namespace Shared Bucket Compute Personal namespace Shared namespace N/A What is the difference between a bucket and a folder? Buckets are like Network Storage. See the Storage overview for more discussion of the differences between these two ideas. Choosing the best way to share code, data, and compute all involve different factors, but you can generally mix and match (share code with your team through github, but store your data privately in a personal bucket). These cases are described more in the below sections. Share code among team members \u00b6 In most cases, it is easiest to share code using GitHub or GitLab to share code. The advantage of sharing with GitHub or GitLab is that it works with users across namespaces, and keeping code in git is a great way to manage large software projects. Don't forget to include a License! If your code is public, do not forget to keep with the Innovation Team's guidelines and use a proper License if your work is done for Statistics Canada. If you need to share code without publishing it on a repository, sharing a namespace might work as well. Share compute (namespace) in Kubeflow \u00b6 Sharing a namespace means you share everything in the namespace Kubeflow does not support granular sharing of one resource (one notebook, one MinIO bucket, etc.), but instead sharing of all resources. If you want to share a Jupyter Notebook server with someone, you must share your entire namespace and they will have access to all other resources (MinIO buckets, etc.) . In Kubeflow every user has a namespace that contains their work (their notebook servers, pipelines, disks, etc.). Your namespace belongs to you, but can be shared if you want to collaborate with others. You can also request a new namespace (either for yourself or to share with a team). One option for collaboration is to share namespaces with others. The advantage of sharing a Kubeflow namespace is that it lets you and your colleagues share the compute environment and MinIO buckets associated with the namespace. This makes it a very easy and free-form way to share. To share your namespace, see managing contributors Ask for help in production The Advanced Analytics Workspace support staff are happy to help with production oriented use cases, and we can probably save you lots of time. Don't be shy about asking us for help ! Share data \u00b6 Once you have a shared namespace, you have two shared storage approaches Storage Option Benefits Shared Jupyter Servers/Workspaces More amenable to small files, notebooks, and little experiments. Shared Buckets (see Storage ) Better suited for use in pipelines, APIs, and for large files. To learn more about the technology behind these, check out the Storage overview . Sharing with StatCan \u00b6 In addition to private buckets, or team-shared private buckets, you can also place your files in shared storage . Within all bucket storage options ( minimal , premium , pachyderm ), you have a private bucket, and a folder inside of the shared bucket. Take a look, for instance, at the link below: shared/blair-drummond/ Any logged in user can see these files and read them freely. Sharing with the world \u00b6 Ask about that one in our Slack channel . There are many ways to do this from the IT side, but it's important for it to go through proper processes, so this is not done in a \"self-serve\" way that the others are. That said, it is totally possible. Recommendation: Combine them all \u00b6 It's a great idea to always use git, and using git along with shared workspaces is a great way to combine ad hoc sharing (through files) while also keeping your code organized and tracked. Managing contributors \u00b6 You can add or remove people from a namespace you already own through the Manage Contributors menu in Kubeflow. Now you and your colleagues can share access to a server! Try it out!","title":"Overview"},{"location":"4-Collaboration/Overview/#share-code-among-team-members","text":"In most cases, it is easiest to share code using GitHub or GitLab to share code. The advantage of sharing with GitHub or GitLab is that it works with users across namespaces, and keeping code in git is a great way to manage large software projects. Don't forget to include a License! If your code is public, do not forget to keep with the Innovation Team's guidelines and use a proper License if your work is done for Statistics Canada. If you need to share code without publishing it on a repository, sharing a namespace might work as well.","title":"Share code among team members"},{"location":"4-Collaboration/Overview/#share-compute-namespace-in-kubeflow","text":"Sharing a namespace means you share everything in the namespace Kubeflow does not support granular sharing of one resource (one notebook, one MinIO bucket, etc.), but instead sharing of all resources. If you want to share a Jupyter Notebook server with someone, you must share your entire namespace and they will have access to all other resources (MinIO buckets, etc.) . In Kubeflow every user has a namespace that contains their work (their notebook servers, pipelines, disks, etc.). Your namespace belongs to you, but can be shared if you want to collaborate with others. You can also request a new namespace (either for yourself or to share with a team). One option for collaboration is to share namespaces with others. The advantage of sharing a Kubeflow namespace is that it lets you and your colleagues share the compute environment and MinIO buckets associated with the namespace. This makes it a very easy and free-form way to share. To share your namespace, see managing contributors Ask for help in production The Advanced Analytics Workspace support staff are happy to help with production oriented use cases, and we can probably save you lots of time. Don't be shy about asking us for help !","title":"Share compute (namespace) in Kubeflow"},{"location":"4-Collaboration/Overview/#share-data","text":"Once you have a shared namespace, you have two shared storage approaches Storage Option Benefits Shared Jupyter Servers/Workspaces More amenable to small files, notebooks, and little experiments. Shared Buckets (see Storage ) Better suited for use in pipelines, APIs, and for large files. To learn more about the technology behind these, check out the Storage overview .","title":"Share data"},{"location":"4-Collaboration/Overview/#sharing-with-statcan","text":"In addition to private buckets, or team-shared private buckets, you can also place your files in shared storage . Within all bucket storage options ( minimal , premium , pachyderm ), you have a private bucket, and a folder inside of the shared bucket. Take a look, for instance, at the link below: shared/blair-drummond/ Any logged in user can see these files and read them freely.","title":"Sharing with StatCan"},{"location":"4-Collaboration/Overview/#sharing-with-the-world","text":"Ask about that one in our Slack channel . There are many ways to do this from the IT side, but it's important for it to go through proper processes, so this is not done in a \"self-serve\" way that the others are. That said, it is totally possible.","title":"Sharing with the world"},{"location":"4-Collaboration/Overview/#recommendation-combine-them-all","text":"It's a great idea to always use git, and using git along with shared workspaces is a great way to combine ad hoc sharing (through files) while also keeping your code organized and tracked.","title":"Recommendation: Combine them all"},{"location":"4-Collaboration/Overview/#managing-contributors","text":"You can add or remove people from a namespace you already own through the Manage Contributors menu in Kubeflow. Now you and your colleagues can share access to a server! Try it out!","title":"Managing contributors"},{"location":"4-Collaboration/Request-a-Namespace/","text":"Overview \u00b6 By default, everyone gets their own personal namespace, firstname-lastname . If you want to collaborate with your team, you can request a new namespace to share . Setup \u00b6 Requesting a namespace \u00b6 To create a namespace for a team, go to the AAW portal. Click the \u22ee menu on the Kubeflow section of the portal . Enter the name you are requesting and submit the request. Be sure to use only lower case letters plus dashes. The namespace cannot have special characters other than hyphens The namespace name must only be lowercase letters a-z with dashes. Otherwise, the namespace will not be created. You will receive an email notification when the namespace is created. Once the shared namespace is created, you can access it the same as any other namespace you have through the Kubeflow UI, like shown below. You will then be able to share and manage to your namespace. To switch namespaces, take a look at the top of your window, just to the right of the Kubeflow Logo.","title":"Request a Namespace"},{"location":"4-Collaboration/Request-a-Namespace/#overview","text":"By default, everyone gets their own personal namespace, firstname-lastname . If you want to collaborate with your team, you can request a new namespace to share .","title":"Overview"},{"location":"4-Collaboration/Request-a-Namespace/#setup","text":"","title":"Setup"},{"location":"4-Collaboration/Request-a-Namespace/#requesting-a-namespace","text":"To create a namespace for a team, go to the AAW portal. Click the \u22ee menu on the Kubeflow section of the portal . Enter the name you are requesting and submit the request. Be sure to use only lower case letters plus dashes. The namespace cannot have special characters other than hyphens The namespace name must only be lowercase letters a-z with dashes. Otherwise, the namespace will not be created. You will receive an email notification when the namespace is created. Once the shared namespace is created, you can access it the same as any other namespace you have through the Kubeflow UI, like shown below. You will then be able to share and manage to your namespace. To switch namespaces, take a look at the top of your window, just to the right of the Kubeflow Logo.","title":"Requesting a namespace"},{"location":"5-Storage/AzureBlobStorage/","text":"Overview \u00b6 Azure Blob Storage is Microsoft's object storage solution for the cloud. Blob Storage is optimized for storing massive amounts of unstructured data. Unstructured data is data that doesn't adhere to a particular data model or definition, such as text or binary data. Azure Blob Storage Containers are good at three things: Large amounts of data - Containers can be huge: way bigger than hard drives. And they are still fast. Accessible by multiple consumers at once - You can access the same data source from multiple Notebook Servers and pipelines at the same time without needing to duplicate the data. Sharing - Project namespaces can share a container. This is great for sharing data with people outside of your workspace. Setup \u00b6 Azure Blob Storage containers and buckets mount will be replacing the Minio Buckets and Minio storage mounts Users will be responsible for migrating data from Minio Buckets to the Azure Storage folders. For larger files, users may contact AAW for assistance. Blob Container Mounted on a Notebook Server \u00b6 The Blob CSI volumes are persisted under /home/jovyan/buckets when creating a Notebook Server. Files under /buckets are backed by Blob storage. All AAW notebooks will have the /buckets mounted to the file-system, making data accessible from everywhere. Unclassified Notebook AAW folder mount \u00b6 Protected-b Notebook AAW folder mount \u00b6 These folders can be used like any other - you can copy files to/from using the file browser, write from Python/R, etc. The only difference is that the data is being stored in the Blob storage container rather than on a local disk (and is thus accessible wherever you can access your Kubeflow notebook). Container Types \u00b6 The following Blob containers are available: Accessing all Blob containers is the same. The difference between containers is the storage type behind them: aaw-unclassified: By default, use this one. Stores unclassified data. aaw-protected-b: Stores sensitive data protected-b. aaw-unclassified-ro: This classification is protected-b but read-only access. This is so users can view unclassified data within a protected-b notebook. Accessing Internal Data \u00b6 TBA: Awaiting DAS common storage connection AAW has an integration with the FAIR Data Infrastructure team that allows users to transfer unclassified and protected-b data to Azure Storage Accounts, thus allowing users to access this data from Notebook Servers. Please reach out to the FAIR Data Infrastructure team if you have a use case for this data. Pricing \u00b6 Pricing models are based on CPU and Memory usage Pricing is covered by KubeCost for user namespaces (In Kubeflow at the bottom of the Notebooks tab). In general, Blob Storage is much cheaper than Azure Manage Disks and has better I/O than managed SSD.","title":"Azure Blob Storage"},{"location":"5-Storage/AzureBlobStorage/#overview","text":"Azure Blob Storage is Microsoft's object storage solution for the cloud. Blob Storage is optimized for storing massive amounts of unstructured data. Unstructured data is data that doesn't adhere to a particular data model or definition, such as text or binary data. Azure Blob Storage Containers are good at three things: Large amounts of data - Containers can be huge: way bigger than hard drives. And they are still fast. Accessible by multiple consumers at once - You can access the same data source from multiple Notebook Servers and pipelines at the same time without needing to duplicate the data. Sharing - Project namespaces can share a container. This is great for sharing data with people outside of your workspace.","title":"Overview"},{"location":"5-Storage/AzureBlobStorage/#setup","text":"Azure Blob Storage containers and buckets mount will be replacing the Minio Buckets and Minio storage mounts Users will be responsible for migrating data from Minio Buckets to the Azure Storage folders. For larger files, users may contact AAW for assistance.","title":"Setup"},{"location":"5-Storage/AzureBlobStorage/#blob-container-mounted-on-a-notebook-server","text":"The Blob CSI volumes are persisted under /home/jovyan/buckets when creating a Notebook Server. Files under /buckets are backed by Blob storage. All AAW notebooks will have the /buckets mounted to the file-system, making data accessible from everywhere.","title":"Blob Container Mounted on a Notebook Server"},{"location":"5-Storage/AzureBlobStorage/#unclassified-notebook-aaw-folder-mount","text":"","title":"Unclassified Notebook AAW folder mount"},{"location":"5-Storage/AzureBlobStorage/#protected-b-notebook-aaw-folder-mount","text":"These folders can be used like any other - you can copy files to/from using the file browser, write from Python/R, etc. The only difference is that the data is being stored in the Blob storage container rather than on a local disk (and is thus accessible wherever you can access your Kubeflow notebook).","title":"Protected-b Notebook AAW folder mount"},{"location":"5-Storage/AzureBlobStorage/#container-types","text":"The following Blob containers are available: Accessing all Blob containers is the same. The difference between containers is the storage type behind them: aaw-unclassified: By default, use this one. Stores unclassified data. aaw-protected-b: Stores sensitive data protected-b. aaw-unclassified-ro: This classification is protected-b but read-only access. This is so users can view unclassified data within a protected-b notebook.","title":"Container Types"},{"location":"5-Storage/AzureBlobStorage/#accessing-internal-data","text":"TBA: Awaiting DAS common storage connection AAW has an integration with the FAIR Data Infrastructure team that allows users to transfer unclassified and protected-b data to Azure Storage Accounts, thus allowing users to access this data from Notebook Servers. Please reach out to the FAIR Data Infrastructure team if you have a use case for this data.","title":"Accessing Internal Data"},{"location":"5-Storage/AzureBlobStorage/#pricing","text":"Pricing models are based on CPU and Memory usage Pricing is covered by KubeCost for user namespaces (In Kubeflow at the bottom of the Notebooks tab). In general, Blob Storage is much cheaper than Azure Manage Disks and has better I/O than managed SSD.","title":"Pricing"},{"location":"5-Storage/Disks/","text":"Overview \u00b6 Disks are the familiar hard drive style file systems you're used to, provided to you from fast solid state drives (SSDs)! Setup \u00b6 When creating your notebook server, you request disks by adding Data Volumes to your notebook server (pictured below, with Type = New ). They are automatically mounted at the directory ( Mount Point ) you choose, and serve as a simple and reliable way to preserve data attached to a Notebook Server. You pay for all disks you own, whether they're attached to a Notebook Server or not As soon as you create a disk, you're paying for it until it is deleted , even if it's original Notebook Server is deleted. See Deleting Disk Storage for more info Once you've got the basics ... \u00b6 When you delete your Notebook Server, your disks are not deleted . This let's you reuse that same disk (with all its contents) on a new Notebook Server later (as shown above with Type = Existing and the Name set to the volume you want to reuse). If you're done with the disk and it's contents, delete it . Deleting Disk Storage \u00b6 To see your disks, check the Notebook Volumes section of the Notebook Server page (shown below). You can delete any unattached disk (orange icon on the left) by clicking the trash can icon. Pricing \u00b6 Pricing models are tentative and may change As of writing, pricing is covered by the platform for initial users. This guidance explains how things are expected to be priced priced in future, but this may change. When mounting a disk, you get an Azure Managed Disk . The Premium SSD Managed Disks pricing shows the cost per disk based on size. Note that you pay for the size of disk requested, not the amount of space you are currently using. Tips to minimize costs As disks can be attached to a Notebook Server and reused, a typical usage pattern could be: At 9AM, create a Notebook Server (request 2CPU/8GB RAM and a 32GB attached disk) Do work throughout the day, saving results to the attached disk At 5PM, shut down your Notebook Server to avoid paying for it overnight NOTE: The attached disk is not destroyed by this action At 9AM the next day, create a new Notebook Server and attach your existing disk Continue your work... This keeps all your work safe without paying for the computer when you're not using it","title":"Disks"},{"location":"5-Storage/Disks/#overview","text":"Disks are the familiar hard drive style file systems you're used to, provided to you from fast solid state drives (SSDs)!","title":"Overview"},{"location":"5-Storage/Disks/#setup","text":"When creating your notebook server, you request disks by adding Data Volumes to your notebook server (pictured below, with Type = New ). They are automatically mounted at the directory ( Mount Point ) you choose, and serve as a simple and reliable way to preserve data attached to a Notebook Server. You pay for all disks you own, whether they're attached to a Notebook Server or not As soon as you create a disk, you're paying for it until it is deleted , even if it's original Notebook Server is deleted. See Deleting Disk Storage for more info","title":"Setup"},{"location":"5-Storage/Disks/#once-youve-got-the-basics","text":"When you delete your Notebook Server, your disks are not deleted . This let's you reuse that same disk (with all its contents) on a new Notebook Server later (as shown above with Type = Existing and the Name set to the volume you want to reuse). If you're done with the disk and it's contents, delete it .","title":"Once you've got the basics ..."},{"location":"5-Storage/Disks/#deleting-disk-storage","text":"To see your disks, check the Notebook Volumes section of the Notebook Server page (shown below). You can delete any unattached disk (orange icon on the left) by clicking the trash can icon.","title":"Deleting Disk Storage"},{"location":"5-Storage/Disks/#pricing","text":"Pricing models are tentative and may change As of writing, pricing is covered by the platform for initial users. This guidance explains how things are expected to be priced priced in future, but this may change. When mounting a disk, you get an Azure Managed Disk . The Premium SSD Managed Disks pricing shows the cost per disk based on size. Note that you pay for the size of disk requested, not the amount of space you are currently using. Tips to minimize costs As disks can be attached to a Notebook Server and reused, a typical usage pattern could be: At 9AM, create a Notebook Server (request 2CPU/8GB RAM and a 32GB attached disk) Do work throughout the day, saving results to the attached disk At 5PM, shut down your Notebook Server to avoid paying for it overnight NOTE: The attached disk is not destroyed by this action At 9AM the next day, create a new Notebook Server and attach your existing disk Continue your work... This keeps all your work safe without paying for the computer when you're not using it","title":"Pricing"},{"location":"5-Storage/Overview/","text":"Storage \u00b6 The platform provides several types of storage: Disk (also called Volumes on the Notebook Server creation screen) Containers (Azure Blob Storage) Data Lakes (coming soon) Depending on your use case, either disk or bucket may be most suitable: Type Simultaneous Users Speed Total size Sharable with Other Users Disk One machine/notebook server at a time Fastest (throughput and latency) <=512GB total per drive No Container (via Azure Blob Storage) Simultaneous access from many machines/notebook servers at the same time Fast-ish (Fast download, modest upload, modest latency) Infinite (within reason) [Yes] If you're unsure which to choose, don't sweat it These are guidelines, not an exact science - pick what sounds best now and run with it. The best choice for a complicated usage is non-obvious and often takes hands-on experience, so just trying something will help. For most situations both options work well even if they're not perfect, and remember that data can always be copied later if you change your mind.","title":"Overview"},{"location":"5-Storage/Overview/#storage","text":"The platform provides several types of storage: Disk (also called Volumes on the Notebook Server creation screen) Containers (Azure Blob Storage) Data Lakes (coming soon) Depending on your use case, either disk or bucket may be most suitable: Type Simultaneous Users Speed Total size Sharable with Other Users Disk One machine/notebook server at a time Fastest (throughput and latency) <=512GB total per drive No Container (via Azure Blob Storage) Simultaneous access from many machines/notebook servers at the same time Fast-ish (Fast download, modest upload, modest latency) Infinite (within reason) [Yes] If you're unsure which to choose, don't sweat it These are guidelines, not an exact science - pick what sounds best now and run with it. The best choice for a complicated usage is non-obvious and often takes hands-on experience, so just trying something will help. For most situations both options work well even if they're not perfect, and remember that data can always be copied later if you change your mind.","title":"Storage"},{"location":"6-Gitlab/Gitlab/","text":"IMPORTANT NOTES \u00b6 1) Please do NOT store your token anywhere in your workspace server file system. Contributors to a namespace will have access to them. 2) If there is a contributor external to Statistics Canada in your namespace, you will lose access to cloud main gitlab access! Thankfully, using the cloud main gitlab on the AAW is just like how you would regularly use git. Step 1: Locate the Git repo you want to clone and copy the clone with HTTPS option \u00b6 If your repository is private, you will need to also do Step 4 (Creating a Personal Access Token) for this to go through. For me this was a test repo Step 2: Paste the copied link into one of your workspace servers \u00b6 Step 3: Success! \u00b6 As seen in the above screenshot I have cloned the repo! Step 4: Create a Personal Access Token for pushing (also used if pulling from a private repository) \u00b6 If you try to git push .... you will encounter an error eventually leading you to the gitlab help documentation You will need to make a Personal Access Token for this. To achieve this go in gitlab, click your profile icon and then hit Preferences and then Access Tokens Follow the prompts entering the name, the token expiration date and granting the token permissions (I granted write_repository ) Step 5: Personalize Git to be you \u00b6 Run git config user.email .... and git config user.name ... to match your gitlab identity. Step 6: Supply the Generated Token when asked for your password \u00b6 The token will by copy-able at the top once you hit Create personal access token at the bottom Once you have prepared everything it's time Step 7: See the results of your hard work in gitlab \u00b6","title":"Gitlab"},{"location":"6-Gitlab/Gitlab/#important-notes","text":"1) Please do NOT store your token anywhere in your workspace server file system. Contributors to a namespace will have access to them. 2) If there is a contributor external to Statistics Canada in your namespace, you will lose access to cloud main gitlab access! Thankfully, using the cloud main gitlab on the AAW is just like how you would regularly use git.","title":"IMPORTANT NOTES"},{"location":"6-Gitlab/Gitlab/#step-1-locate-the-git-repo-you-want-to-clone-and-copy-the-clone-with-https-option","text":"If your repository is private, you will need to also do Step 4 (Creating a Personal Access Token) for this to go through. For me this was a test repo","title":"Step 1: Locate the Git repo you want to clone and copy the clone with HTTPS option"},{"location":"6-Gitlab/Gitlab/#step-2-paste-the-copied-link-into-one-of-your-workspace-servers","text":"","title":"Step 2: Paste the copied link into one of your workspace servers"},{"location":"6-Gitlab/Gitlab/#step-3-success","text":"As seen in the above screenshot I have cloned the repo!","title":"Step 3: Success!"},{"location":"6-Gitlab/Gitlab/#step-4-create-a-personal-access-token-for-pushing-also-used-if-pulling-from-a-private-repository","text":"If you try to git push .... you will encounter an error eventually leading you to the gitlab help documentation You will need to make a Personal Access Token for this. To achieve this go in gitlab, click your profile icon and then hit Preferences and then Access Tokens Follow the prompts entering the name, the token expiration date and granting the token permissions (I granted write_repository )","title":"Step 4: Create a Personal Access Token for pushing (also used if pulling from a private repository)"},{"location":"6-Gitlab/Gitlab/#step-5-personalize-git-to-be-you","text":"Run git config user.email .... and git config user.name ... to match your gitlab identity.","title":"Step 5: Personalize Git to be you"},{"location":"6-Gitlab/Gitlab/#step-6-supply-the-generated-token-when-asked-for-your-password","text":"The token will by copy-able at the top once you hit Create personal access token at the bottom Once you have prepared everything it's time","title":"Step 6: Supply the Generated Token when asked for your password"},{"location":"6-Gitlab/Gitlab/#step-7-see-the-results-of-your-hard-work-in-gitlab","text":"","title":"Step 7: See the results of your hard work in gitlab"}]}