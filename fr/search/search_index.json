{"config":{"lang":["fr"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"D\u00e9marrer sur l'espace de travail d'analyse avanc\u00e9e","text":"<p>Le portail de l'espace de travail d'analyse avanc\u00e9e est un excellent endroit o\u00f9 explorer les ressources dont il sera question ici, et y acc\u00e9der.</p>"},{"location":"#que-cherchez-vous","title":"Que cherchez-vous ?","text":""},{"location":"#demarrer-avec-aaw","title":"D\u00e9marrer avec AAW","text":"<p> Tout commence avec Kubeflow! Commencez par le configurer.</p> <p> Vous allez avoir des questions. Rejoignez notre Canal Slack pour que nous puissions vous donner des r\u00e9ponses!</p> <p></p> <p>Cliquez sur le lien, puis choisissez \"Cr\u00e9er un compte\" dans le coin en haut \u00e0 droite.</p> <p></p> <p>Utilisez votre adresse \u00e9lectronique @statcan.gc.ca pour que votre demande soit automatiquement approuv\u00e9e.</p>"},{"location":"#experiences","title":"Exp\u00e9riences","text":""},{"location":"#traiter-les-donnees-en-utilisant-les-serveurs-bloc-notes","title":"Traiter les donn\u00e9es en utilisant les serveurs bloc-notes","text":"<p>Dans Kubeflow, les Serveurs Bloc-Notes vous permet d'obtenir un environnement de calcul interactif pour traiter les donn\u00e9es. Tous les serveurs bloc-notes ont acc\u00e8s \u00e0 un maximum de 15CPU/48GB RAM et \u00e0 un stockage \u00e0 l'\u00e9chelle de GB/TB, mais ont une interface utilisateur diff\u00e9rente selon la version que vous choisissez.</p> <ul> <li>Python, Julia et R via un Bloc-notes Jupyter.</li> <li>R via RStudio</li> </ul>"},{"location":"#bloc-notes-jupyter-pour-python-julia-ou-r","title":"Bloc-notes Jupyter pour <code>Python</code>, <code>Julia</code>, ou <code>R</code>","text":"<p>Utilisez un Bloc-notes Jupyter pour cr\u00e9er et partager des documents interactifs qui contiennent un m\u00e9lange de code en direct, de visualisations et de texte. Ceux-ci peuvent \u00eatre \u00e9crits en <code>Python</code>,<code>Julia</code> ou <code>R</code>.</p> <p></p> <p>Pour lancer un serveur bloc-notes avec une interface Jupyter, choisissez l'une des images <code>jupyterlab</code> lors de la cr\u00e9ation de votre serveur bloc-notes. L'image <code>jupyterlab</code> est \u00e9galement pr\u00e9charg\u00e9e avec VS Code dans le navigateur si vous pr\u00e9f\u00e9rez une exp\u00e9rience IDE compl\u00e8te.</p>"},{"location":"#rstudio-pour-r-et-shiny","title":"RStudio pour <code>R</code> et <code>Shiny</code>","text":"<p>RStudio vous offre un environnement de d\u00e9veloppement int\u00e9gr\u00e9 sp\u00e9cifiquement pour R. Si vous codez en R, c'est typiquement le serveur bloc-notes \u00e0 utiliser. Utilisez l'image <code>rstudio</code> pour obtenir un environnement RStudio.</p> <p></p>"},{"location":"#executer-un-bureau-virtuel","title":"Ex\u00e9cuter un bureau virtuel","text":"<p>Pour une exp\u00e9rience de bureau Ubuntu compl\u00e8te, utilisez l'un de nos serveurs bloc-notes <code>remote-desktop</code>.Vous pouvez ex\u00e9cuter un bureau Ubuntu complet, avec des applications typiques, directement dans votre navigateur, en utilisant Espace de travail ML</p>"},{"location":"#publication","title":"Publication","text":""},{"location":"#construire-et-publier-un-tableau-de-bord-interactif","title":"Construire et publier un tableau de bord interactif","text":"<p> Utilisez R-Shiny pour cr\u00e9er des applications web interactives directement \u00e0 partir de R. Vous pouvez d\u00e9ployer votre tableau de bord R-Shiny en soumettant une requ\u00eate de tirage \u00e0 notre d\u00e9p\u00f4t GitHub R-Dashboards. </p> <p>Dash est un outil de visualisation de donn\u00e9es qui vous permet de construire une interface graphique interactive autour de votre code d'analyse de donn\u00e9es.</p>"},{"location":"#explorer-vos-donnees","title":"Explorer vos donn\u00e9es","text":"<p> Utilisez Datasette , une API JSON instantan\u00e9e pour vos bases de donn\u00e9es SQLite. Ex\u00e9cutez des requ\u00eates SQL d'une mani\u00e8re plus interactive !</p>"},{"location":"#pipelines","title":"Pipelines","text":""},{"location":"#creer-et-planifier-des-pipelines-de-donneesanalyse","title":"Cr\u00e9er et planifier des pipelines de donn\u00e9es/analyse","text":"<p> Kubeflow Pipelines vous permet de mettre en place des pipelines. Chaque pipelines encapsulent des flux de production analytiques et peuvent \u00eatre mis en commun, r\u00e9utilis\u00e9s et programm\u00e9s. </p> <p></p>"},{"location":"#integration-avec-les-offres-de-plateforme-en-tant-que-service-paas","title":"Int\u00e9gration avec les offres de plateforme en tant que service (PaaS)","text":"<p>Nous pouvons nous int\u00e9grer \u00e0 de nombreuses offres de Plateforme en tant que Service (PaaS), telles que Databricks ou AzureML.</p>"},{"location":"#collaboration","title":"Collaboration","text":""},{"location":"#partage-du-code","title":"Partage du code","text":"<p> Utilise GitHub ou GitLab pour partager votre code avec les membres de votre \u00e9quipe, ou demander pour un espace de travail partag\u00e9 .</p> <p>Demander de l'aide pour la production</p> <p>Le personnel de soutien de l'espace de travail d'analyse avanc\u00e9e est heureux de vous aider pour les cas d'utilisation orient\u00e9s vers la production, et nous pouvons probablement vous faire gagner beaucoup de temps. N'h\u00e9sitez pas \u00e0 nous demander pour de l'aide!</p>"},{"location":"#comment-obtenir-des-donnees-comment-envoyer-des-donnees","title":"Comment obtenir des donn\u00e9es? Comment envoyer des donn\u00e9es?","text":"<ul> <li>Chaque espace de travail peut \u00eatre \u00e9quip\u00e9 de son propre stockage.</li> </ul> <ul> <li>Il existe \u00e9galement des compartiments de stockage pour la publication   d'ensembles de donn\u00e9es, pour usage interne ou diffusion plus large.</li> </ul> <p>Nous donnerons un aper\u00e7u des technologies ici. Des renseignements plus pr\u00e9cis sur chacune d'entre elles seront fournis dans les sections suivantes.</p> <p>Parcourir quelques ensembles de donn\u00e9es</p> <p>Parcourez quelques ensembles de donn\u00e9es ici. Ces ensembles de donn\u00e9es ont \u00e9t\u00e9 con\u00e7us pour stocker des donn\u00e9es largement partag\u00e9es. Il peut s'agir de donn\u00e9es qui ont \u00e9t\u00e9 introduites, ou de donn\u00e9es qui seront diffus\u00e9es sous forme de produit. Comme toujours, veillez \u00e0 ce qu'il ne s'agisse pas de donn\u00e9es de nature d\u00e9licate.</p>"},{"location":"Aide/","title":"Vous avez des questions ou des commentaires?","text":"<p>Joignez-vous \u00e0 nous sur le canal Slack de l'espace de travail en analytique avanc\u00e9e! Vous trouverez de nombreux utilisateurs de la plateforme qui pourront peut-\u00eatre r\u00e9pondre \u00e0 vos questions. Quelques ing\u00e9nieurs sont aussi habituellement pr\u00e9sents. Vous pouvez poser des questions et soumettre vos commentaires.</p> <ul> <li>Slack (fr)</li> </ul> <p>Nous publierons \u00e9galement des avis de mise \u00e0 jour et d'interruption sur le canal Slack.</p>"},{"location":"Aide/#videos-tutoriels","title":"Vid\u00e9os tutoriels","text":"<p>Une fois que vous vous \u00eates joint \u00e0 notre communaut\u00e9 Slack, prenez le temps de fureter nos vid\u00e9os tutoriels!</p> <ul> <li>Vid\u00e9os tutoriels officiels</li> <li>D\u00e9mo et contenue provenus de la communaut\u00e9 de l'EAA</li> </ul>"},{"location":"Aide/#github","title":"GitHub","text":"<p>Vous voulez en savoir plus \u00e0 propos de notre plateforme? Trouvez tous les d\u00e9tailles sur notre page GitHub!</p> <ul> <li>Page GitHub de l'Espace d'analyse avanc\u00e9e</li> </ul>"},{"location":"Collaboration/","title":"Collaboration en mati\u00e8re d'espace de travail en analytique avanc\u00e9e","text":"<p>Il existe de nombreuses fa\u00e7ons de collaborer sur la plateforme, et ce qui vous convient le mieux d\u00e9pend des \u00e9l\u00e9ments que vous partagez et du nombre de personnes avec qui vous souhaitez partager des \u00e9l\u00e9ments. Nous pouvons r\u00e9partir les \u00e9l\u00e9ments partageables en donn\u00e9es et en code. Nous pouvons aussi r\u00e9partir l'ensemble des groupes avec lesquels vous partagez des \u00e9l\u00e9ments en priv\u00e9, \u00e9quipe et StatCan. Cela conduit au tableau d'options suivant :</p> Priv\u00e9 \u00c9quipe StatCan Code GitLab/GitHub ou dossier personnel GitLab/GitHub ou dossier d'\u00e9quipe GitLab/GitHub Donn\u00e9es Dossier ou compartiment personnel Dossier ou compartiment d'\u00e9quipe Compartiment partag\u00e9 Quelle est la diff\u00e9rence entre un compartiment et un dossier? <p>Les compartiments s'apparentent au stockage r\u00e9seau. Voir la section sur le stockage pour obtenir des pr\u00e9cisions suppl\u00e9mentaires sur les diff\u00e9rences entre ces deux concepts.</p> <p>Les acc\u00e8s priv\u00e9 et \u00e9quipe sont configur\u00e9s au moyen des espaces de nommage. Nous commen\u00e7ons donc par parler de Kubeflow et des espaces de nommage Kubeflow.</p> <p>Ensuite, les donn\u00e9es et le code sont mieux pris en charge au moyen d'outils l\u00e9g\u00e8rement diff\u00e9rents. C'est pourquoi nous allons les aborder en deux temps. Pour les donn\u00e9es, nous parlons de compartiments et MinIO. Pour le code, nous parlons de Git, GitLab et GitHub.</p> <p>C'est sur quoi repose la structure de cette page :</p> <p>\u2013 Collaboration en \u00e9quipe (applicable au code et aux donn\u00e9es) \u2013 Partage du code \u2013 Partage des donn\u00e9es</p>"},{"location":"Collaboration/#collaboration-en-equipe","title":"Collaboration en \u00e9quipe","text":""},{"location":"Collaboration/#que-fait-kubeflow","title":"Que fait Kubeflow?","text":"<p>Kubeflow ex\u00e9cute vos espaces de travail. Vous pouvez avoir des serveurs de blocs-notes (appel\u00e9s serveurs Jupyter) et vous pouvez y cr\u00e9er des analyses en R et en Python en utilisant des \u00e9l\u00e9ments visuels interactifs. Vous pouvez enregistrer, t\u00e9l\u00e9verser et t\u00e9l\u00e9charger des donn\u00e9es. Votre \u00e9quipe peut travailler \u00e0 vos c\u00f4t\u00e9s.</p>"},{"location":"Collaboration/#demande-dun-espace-de-nommage","title":"Demande d'un espace de nommage","text":"<p>Par d\u00e9faut, chaque personne obtient son propre espace de nommage, <code>pr\u00e9nom-nom</code>. Si vous souhaitez cr\u00e9er un espace de nommage pour une \u00e9quipe, pr\u00e9sentez la demande dans le portail : Cliquez sur le \u22ee de la section Kubeflow du portail.</p> <p>L'espace de nommage ne doit comprendre aucun caract\u00e8re sp\u00e9cial autre que le trait d'union.</p> <p>Le nom doit seulement comprendre des lettres minuscules (de \u00ab a \u00bb \u00e0 \u00ab z \u00bb) et des traits d'union. Sinon, il sera impossible de cr\u00e9er l'espace de nommage.</p> <p>Vous recevrez un avis par courriel vous indiquant que l'espace de nommage est cr\u00e9\u00e9. Une fois que l'espace de nommage partag\u00e9 est cr\u00e9\u00e9, vous pouvez y acc\u00e9der comme tous les autres espaces de nommage dont vous disposez par l'entremise de l'interface utilisateur Kubeflow, comme il est indiqu\u00e9 ci-apr\u00e8s. Vous pourrez ensuite g\u00e9rer la liste des collaborateurs sous l'onglet de gestion des contributeurs de Kubeflow, o\u00f9 vous pourrez ajouter vos coll\u00e8gues \u00e0 l'espace de nommage partag\u00e9.</p> <p></p> <p>Pour changer d'espace de nommage, allez en haut de votre fen\u00eatre, juste \u00e0 droite du logo de Kubeflow.</p> <p></p>"},{"location":"Collaboration/#code-partage","title":"Code partag\u00e9","text":"<p>Les \u00e9quipes ont deux options (mais vous pouvez combiner les deux) :</p>"},{"location":"Collaboration/#partager-un-espace-de-travail-dans-kubeflow","title":"Partager un espace de travail dans Kubeflow","text":"<p>Le partage dans Kubeflow pr\u00e9sente l'avantage suivant : il est \u00e0 structure libre et il fonctionne mieux pour les fichiers <code>.ipynb</code> (blocs-notes Jupyter). Cette m\u00e9thode vous permet \u00e9galement de partager un environnement de calcul, de sorte que vous pouvez partager des ressources tr\u00e8s facilement. Lorsque vous partagez un espace de travail, vous partagez :</p> <p>\u2013 un compartiment priv\u00e9 et partag\u00e9 (<code>/team-name</code> et <code>/shared/team-name</code>); \u2013 tous les serveurs de blocs-notes dans l'espace de nommage Kubeflow.</p>"},{"location":"Collaboration/#partager-avec-git-au-moyen-de-gitlab-ou-de-github","title":"Partager avec Git, au moyen de GitLab ou de GitHub","text":"<p>Le partage au moyen de Git pr\u00e9sente l'avantage suivant : il permet de travailler avec des utilisateurs de tous les espaces de nommage. De plus, le fait de conserver le code dans Git est un excellent moyen de g\u00e9rer les grands projets de logiciels.</p> <p>N'oubliez pas d'obtenir une licence d'utilisation!</p> <p>Si votre code est public, suivez les directives de l'\u00e9quipe de l'innovation et utilisez une licence appropri\u00e9e si vous r\u00e9alisez des t\u00e2ches pour le compte de Statistique Canada.</p>"},{"location":"Collaboration/#recommandation-combinez-les-deux","title":"Recommandation : combinez les deux.","text":"<p>Il est sage de toujours utiliser Git. L'utilisation de Git de concert avec des espaces de travail partag\u00e9s est un bon moyen de combiner le partage ponctuel (par l'entremise de fichiers) tout en assurant l'organisation et le suivi de votre code.</p>"},{"location":"Collaboration/#stockage-partage","title":"Stockage partag\u00e9","text":""},{"location":"Collaboration/#partage-avec-votre-equipe","title":"Partage avec votre \u00e9quipe","text":"<p>Une fois que vous avez un espace de nommage partag\u00e9, deux m\u00e9thodes de stockage partag\u00e9 s'offrent \u00e0 vous :</p> Option de stockage Avantages Serveurs/espaces de travail Jupyter partag\u00e9s Mieux adapt\u00e9s aux petits fichiers, aux blocs-notes et aux petites exp\u00e9riences. Compartiments partag\u00e9s (voir Stockage) Mieux adapt\u00e9s \u00e0 une utilisation dans les pipelines et les interfaces API et aux fichiers volumineux. <p>Pour en savoir plus sur la technologie sous-jacente, consultez la section sur le stockage.</p>"},{"location":"Collaboration/#partage-avec-statcan","title":"Partage avec StatCan","text":"<p>En plus des compartiments priv\u00e9s et des compartiments priv\u00e9s partag\u00e9s en \u00e9quipe, vous pouvez \u00e9galement placer vos fichiers dans un espace de stockage partag\u00e9. Toutes les options de stockage en compartiments (<code>minimal</code>, <code>sup\u00e9rieur</code>, <code>\u00e9l\u00e9phantesque</code>) offrent un compartiment priv\u00e9 et un dossier \u00e0 l'int\u00e9rieur du compartiment <code>partag\u00e9</code>. Par exemple, consultez le lien ci-dessous :</p> <ul> <li><code>shared/blair-drummond/</code></li> </ul> <p>Tous les utilisateurs connect\u00e9s peuvent visualiser et consulter ces fichiers librement.</p>"},{"location":"Collaboration/#partage-avec-le-monde","title":"Partage avec le monde","text":"<p>Renseignez-vous \u00e0 ce sujet sur notre canal Slack. Il existe de nombreuses m\u00e9thodes de partage avec le monde du point de vue informatique. Cependant, comme il est important de respecter les processus appropri\u00e9s, on n'utilise pas le libre-service, comme dans les autres cas. Cela dit, c'est possible.</p>"},{"location":"message-de-bienvenue/","title":"Message de bienvenue","text":""},{"location":"message-de-bienvenue/#bienvenue-a-lespace-danalyse-avance-eaa","title":"\ud83e\uddd9\ud83d\udd2e Bienvenue \u00e0 l\u2019Espace d'analyse avanc\u00e9 (EAA)","text":"<p>Veuillez trouver ci-dessous des informations, des vid\u00e9os et des liens suppl\u00e9mentaires pour mieux comprendre comment d\u00e9marrer avec l\u2019Espace d'analyse avanc\u00e9 (EAA).</p> <p>L\u2019Espace d'analyse avanc\u00e9 (EAA) est notre plateforme open source pour la science des donn\u00e9es et l'apprentissage automatique (ML) destin\u00e9e aux praticiens avanc\u00e9s, qui peuvent ainsi accomplir leur travail dans un environnement sans restriction, con\u00e7u par des scientifiques des donn\u00e9es pour des scientifiques des donn\u00e9es. Avec EAA, vous pouvez personnaliser vos d\u00e9ploiements d'ordinateurs portables pour r\u00e9pondre \u00e0 vos besoins en mati\u00e8re de science des donn\u00e9es. Nous disposons \u00e9galement d'un petit nombre d'images r\u00e9alis\u00e9es par notre \u00e9quipe experte en science des donn\u00e9es.</p> <p>EAA est bas\u00e9 sur le projet Kubeflow qui est une solution compl\u00e8te open source pour le d\u00e9ploiement et la gestion de flux de travail ML de bout en bout. Kubeflow est con\u00e7u pour rendre les d\u00e9ploiements de flux de travail ML sur Kubernetes simples, portables et \u00e9volutifs.</p> <p>\ud83d\udd14 Important! Les utilisateurs externes \u00e0 Statistique Canada devront disposer d'un compte cloud dont l'acc\u00e8s est accord\u00e9 par le commanditaire de l'entreprise.</p> <p>\ud83d\udd14 Important! Les utilisateurs internes \u00e0 Statistique Canada peuvent commencer tout de suite sans proc\u00e9dure d'inscription suppl\u00e9mentaire, il suffit de se rendre \u00e0 l'adresse https://kubeflow.aaw.cloud.statcan.ca/.</p>"},{"location":"message-de-bienvenue/#liens-utiles","title":"\ud83d\udd17 Liens utiles","text":""},{"location":"message-de-bienvenue/#services-eaa","title":"\ud83d\udece\ufe0f Services EAA","text":"<ul> <li>\ud83c\udf00 Page d'accueil du portail EEA<ul> <li>Interne seulement https://www.statcan.gc.ca/data-analytics-service/aaw</li> <li>Interne/externe https://analytics-platform.statcan.gc.ca/covid19</li> </ul> </li> </ul> <ul> <li>\ud83e\udd16 Tableau de bord Kubeflow<ul> <li>https://kubeflow.aaw.cloud.statcan.ca/</li> </ul> </li> </ul>"},{"location":"message-de-bienvenue/#aide","title":"\ud83d\udca1 Aide","text":"<ul> <li>\ud83d\udcd7 Documentation du portail EAA<ul> <li>https://statcan.github.io/aaw/</li> </ul> </li> <li>\ud83d\udcd8 Documentation sur Kubeflow<ul> <li>https://www.kubeflow.org/docs/</li> </ul> </li> <li>\ud83e\udd1d Canal de support Slack<ul> <li>https://statcan-aaw.slack.com</li> </ul> </li> </ul>"},{"location":"message-de-bienvenue/#mise-en-route","title":"\ud83e\udded Mise en route","text":"<p>Afin d'acc\u00e9der aux services de l'EAA, vous devrez:</p> <ol> <li>Vous connecter \u00e0 Kubeflow avec votre compte cloud invit\u00e9 StatCan. Vous serez invit\u00e9 \u00e0 authentifier le compte.</li> <li>S\u00e9lectionnez Notebook Servers.</li> <li>Cliquez sur le bouton \" \u2795 Nouveau serveur \".</li> </ol>"},{"location":"message-de-bienvenue/#outils-offerts","title":"\ud83e\uddf0 Outils offerts","text":"<p>AAW est une plateforme flexible pour l'analyse de donn\u00e9es et l'apprentissage automatique, avec:</p> <ul> <li>\ud83d\udcdc Langues<ul> <li>\ud83d\udc0d Python</li> <li>\ud83d\udcc8 R</li> <li>\ud83d\udc69\ud83d\udd2c Julia</li> <li>SAS (Prochainement!)</li> </ul> </li> <li>\ud83e\uddee Environnements de d\u00e9veloppement<ul> <li>VS Code</li> <li>R Studio</li> <li>Jupyter Notebooks</li> </ul> </li> <li>\ud83d\udc27 Bureaux virtuels Linux pour des outils suppl\u00e9mentaires (\ud83e\uddeb OpenM++, \ud83c\udf0f QGIS, etc.)</li> </ul>"},{"location":"message-de-bienvenue/#demonstrations","title":"\ud83d\udc31 D\u00e9monstrations","text":"<p>Si vous souhaitez une session d'embarquement/d\u00e9mo rapide ou si vous avez besoin d'aide ou avez des questions, n'h\u00e9sitez pas \u00e0 nous contacter via notre canal de support \ud83e\udd1d Slack.</p>"},{"location":"message-de-bienvenue/#faq","title":"FAQ","text":"<ul> <li>\ud83d\udea7 Bient\u00f4t disponible !</li> </ul> <p>Merci !</p>"},{"location":"1-Experiences/Bureau-virtuel/","title":"Bureau virtuel","text":""},{"location":"1-Experiences/Bureau-virtuel/#quest-ce-quun-bureau-virtuel","title":"Qu'est-ce qu'un Bureau virtuel?","text":"<p>Le bureau \u00e0 distance offre une exp\u00e9rience de bureau Ubuntu avec une interface graphique dans le navigateur, ainsi qu'un acc\u00e8s rapide aux outils de support. Le syst\u00e8me d'exploitation est Ubuntu 18.04 avec l'environnement de bureau XFCE.</p>"},{"location":"1-Experiences/Bureau-virtuel/#versions","title":"Versions","text":"<p>Deux versions du Bureau virtuel sont disponibles. R comprend R et RStudio. Geomatics \u00e9tend R avec QGIS et diverses biblioth\u00e8ques de support. Vous pouvez personnaliser votre espace de travail en fonction de vos besoins et de vos pr\u00e9f\u00e9rences.</p>"},{"location":"1-Experiences/Bureau-virtuel/#personnalisation","title":"Personnalisation","text":"<p>pip, conda, npm et yarn sont disponibles pour installer divers paquets.</p>"},{"location":"1-Experiences/Bureau-virtuel/#acceder-au-bureau-virtuel","title":"Acc\u00e9der au Bureau virtuel","text":"<p>Pour lancer le Bureau virtuel ou l'un de ses outils de support, cr\u00e9ez un serveur bloc-notes dans Kubeflow et s\u00e9lectionnez l'une des versions disponibles dans la liste des images. Ensuite, cliquez sur <code>Connecter</code> pour acc\u00e9der au Bureau virtuel.</p> <p>Un Bureau virtuel vous permet d'acc\u00e9der \u00e0 l'interface graphique du bureau via une session noVNC. Cliquez sur le '&lt;' sur le c\u00f4t\u00e9 gauche de l'\u00e9cran pour ouvrir un panneau avec des options telles qu'un plein \u00e9cran et l'acc\u00e8s au presse-papiers.</p> <p></p>"},{"location":"1-Experiences/Bureau-virtuel/#outils-dans-le-navigateur","title":"Outils dans le Navigateur","text":""},{"location":"1-Experiences/Bureau-virtuel/#vs-code","title":"VS Code","text":"<p>Visual Studio Code est un \u00e9diteur de code source l\u00e9ger mais puissant. Il est livr\u00e9 avec un support int\u00e9gr\u00e9 pour JavaScript, TypeScript et Node.js et poss\u00e8de un riche \u00e9cosyst\u00e8me d'extensions pour plusieurs langages (tels que C++, C#, Java, Python, PHP, Go).</p> <p></p>"},{"location":"1-Experiences/Bureau-virtuel/#notes-de-bas-de-page","title":"Notes de bas de page","text":"<p>Bureau virtuel est bas\u00e9 sur ml-tooling/ml-workspace.</p>"},{"location":"1-Experiences/Jupyter/","title":"Aper\u00e7u","text":""},{"location":"1-Experiences/Jupyter/#jupyter-experience-conviviale-de-r-et-python","title":"Jupyter: Exp\u00e9rience conviviale de R et Python","text":"<p>Jupyter vous permet d'obtenir des bloc-notes pour \u00e9crire votre code et faire des visualisations. Vous pouvez rapidement it\u00e9rer, visualiser et partager vos analyses. Puisque Jupyter est ex\u00e9cut\u00e9 sur un serveur (que vous avez mis en place dans la derni\u00e8re section), il vous permet d'effectuer de tr\u00e8s grandes analyses sur un mat\u00e9riel centralis\u00e9! Ajoutez autant de puissance qu'il vous faut! Et puisque c'est dans le nuage, vous pouvez aussi le partager avec vos coll\u00e8gues.</p>"},{"location":"1-Experiences/Jupyter/#explorez-vos-donnees","title":"Explorez vos donn\u00e9es","text":"<p>Jupyter offre un certain nombre de fonctionnalit\u00e9s (et nous pouvons en ajouter d'autres)</p> <ul> <li>\u00c9l\u00e9ments visuels int\u00e9gr\u00e9s dans votre bloc-notes</li> <li>Volume de donn\u00e9es pour le stockage de vos donn\u00e9es</li> <li>Possibilit\u00e9 de partager votre espace de travail avec vos coll\u00e8gues</li> </ul> <p></p>"},{"location":"1-Experiences/Jupyter/#environnement-de-developpement-dans-le-navigateur","title":"Environnement de d\u00e9veloppement dans le navigateur","text":"<p>Cr\u00e9ez pour explorer, et aussi pour \u00e9crire du code</p> <ul> <li>Linting et d\u00e9bogage</li> <li>Int\u00e9gration Git</li> <li>Terminal int\u00e9gr\u00e9</li> <li>Th\u00e8me clair/fonc\u00e9 (changer les param\u00e8tres en haut)</li> </ul> <p></p> <p>Plus de renseignements sur Jupyter ici</p>"},{"location":"1-Experiences/Jupyter/#installation","title":"Installation","text":""},{"location":"1-Experiences/Jupyter/#commencez-par-les-exemples","title":"Commencez par les exemples","text":"<p>Lorsque vous avez d\u00e9marr\u00e9 votre serveur, il a \u00e9t\u00e9 charg\u00e9 de mod\u00e8les de bloc-notes. Parmi les bons blocs-notes pour commencer, il y a <code>R/01-R-Notebook-Demo.ipynb</code> et ceux dans <code>scikitlearn</code>. Les bloc-notes <code>pytorch</code> et <code>tensorflow</code> sont excellents si vous connaissez l'apprentissage automatique. <code>mapreduce-pipeline</code> et <code>ai-pipeline</code> sont plus avanc\u00e9s.</p> Certains bloc-notes ne fonctionnent que dans certaines versions de serveur <p>Par exemple, <code>gdal</code> ne fonctionne que dans l'image g\u00e9omatique. Donc, si vous utilisez une autre image, un bloc-notes utilisant <code>gdal</code> pourrait ne pas fonctionner.</p>"},{"location":"1-Experiences/Jupyter/#ajout-de-logiciels","title":"Ajout de logiciels","text":"<p>Vous n'avez pas <code>sudo</code> dans Jupyter, mais vous pouvez utiliser</p> <pre><code>conda install --use-local your_package_name\n</code></pre> <p>ou</p> <pre><code>pip install --user your_package_name\n</code></pre> <p>N'oubliez pas de red\u00e9marrer votre noyau Jupyter par la suite, pour acc\u00e9der \u00e0 de nouvelles trousses.</p> Assurez-vous de red\u00e9marrer le noyau Jupyter apr\u00e8s l'installation d'un nouveau logiciel <p>Si vous installez un logiciel dans un terminal, mais que votre noyau Jupyter \u00e9tait d\u00e9j\u00e0 en cours d'ex\u00e9cution, il ne sera pas mis \u00e0 jour.</p> Y a-t-il quelque chose que vous ne pouvez pas installer? <p>Si vous avez besoin d'installer quelque chose, communiquez avec nous ou ouvrir une question GitHub. Nous pouvons l'ajouter au logiciel par d\u00e9faut.</p>"},{"location":"1-Experiences/Jupyter/#une-fois-que-vous-avez-les-bases","title":"Une fois que vous avez les bases ...","text":""},{"location":"1-Experiences/Jupyter/#entrer-et-sortir-des-donnees-de-jupyter","title":"Entrer et sortir des donn\u00e9es de Jupyter","text":"<p>Vous pouvez t\u00e9l\u00e9charger et charger des donn\u00e9es vers ou depuis JupyterHub directement dans le menu. Il y a un bouton de chargement en haut, et vous pouvez cliquer avec le bouton droit de la souris sur la plupart des fichiers ou dossiers pour les t\u00e9l\u00e9charger.</p>"},{"location":"1-Experiences/Jupyter/#stockage-partage-en-compartiment","title":"Stockage partag\u00e9 en compartiment","text":"<p>Il y a aussi un dossier <code>minio</code> mont\u00e9 dans votre r\u00e9pertoire personnel qui contient les fichiers dans MinIO.</p> <p>Consultez la section sur le stockage rubrique pour plus de d\u00e9tails</p>"},{"location":"1-Experiences/Jupyter/#lanalyse-des-donnees","title":"L'analyse des donn\u00e9es","text":"<p>L'analyse des donn\u00e9es est un art sous-estim\u00e9.</p> <p>L'analyse des donn\u00e9es est le processus d'examen et d'interpr\u00e9tation de grandes quantit\u00e9s de donn\u00e9es pour extraire des informations utiles et tirer des conclusions significatives. Cela peut \u00eatre fait \u00e0 l'aide de diverses techniques et outils, tels que l'analyse statistique, l'apprentissage automatique et la visualisation. L'objectif de l'analyse des donn\u00e9es est de d\u00e9couvrir des mod\u00e8les, des tendances et des relations dans les donn\u00e9es, qui peuvent ensuite \u00eatre utilis\u00e9s pour \u00e9clairer les d\u00e9cisions et r\u00e9soudre les probl\u00e8mes. L'analyse de donn\u00e9es est utilis\u00e9e dans un large \u00e9ventail de domaines, des affaires et de la finance aux soins de sant\u00e9 et \u00e0 la science, pour aider les organisations \u00e0 prendre des d\u00e9cisions plus \u00e9clair\u00e9es sur la base de preuves et d'informations bas\u00e9es sur des donn\u00e9es.</p>"},{"location":"1-Experiences/Jupyter/#jupyterlab","title":"JupyterLab","text":"<p>Traiter les donn\u00e9es \u00e0 l'aide de R, Python ou Julia dans JupyterLab</p> <p>Le traitement des donn\u00e9es \u00e0 l'aide de R, Python ou Julia est simplifi\u00e9 gr\u00e2ce \u00e0 l'espace de travail d'analyse avanc\u00e9e. Que vous d\u00e9butiez dans l'analyse de donn\u00e9es ou que vous soyez un data scientist exp\u00e9riment\u00e9, notre plateforme prend en charge une gamme de langages de programmation pour r\u00e9pondre \u00e0 vos besoins. Vous pouvez installer et ex\u00e9cuter des packages pour R ou Python pour effectuer des t\u00e2ches de traitement de donn\u00e9es telles que le nettoyage, la transformation et la mod\u00e9lisation des donn\u00e9es. Si vous pr\u00e9f\u00e9rez Julia, notre plateforme offre \u00e9galement un support pour ce langage de programmation.</p>"},{"location":"1-Experiences/Kubeflow/","title":"Aper\u00e7u","text":""},{"location":"1-Experiences/Kubeflow/#que-fait-kubeflow","title":"Que fait Kubeflow?","text":"<p>Kubeflow ex\u00e9cute vos espaces de travail. Vous pouvez avoir des serveurs de bloc-notes (appel\u00e9s serveurs Jupyter), et vous pouvez y cr\u00e9er des analyses en R et Python avec des visuels interactifs. Vous pouvez enregistrer et charger des donn\u00e9es, t\u00e9l\u00e9charger des donn\u00e9es, et cr\u00e9er des espaces de travail partag\u00e9s pour votre \u00e9quipe.</p> <p></p> <p>Commen\u00e7ons sans plus tarder!</p>"},{"location":"1-Experiences/Kubeflow/#creer-un-serveur","title":"Cr\u00e9er un serveur","text":""},{"location":"1-Experiences/Kubeflow/#se-connecter-a-kubeflow","title":"Se connecter \u00e0 Kubeflow","text":""},{"location":"1-Experiences/Kubeflow/#didacticiel-video","title":"Didacticiel vid\u00e9o","text":"<p>Cette vid\u00e9o n'est pas \u00e0 jour, certaines choses pourraient avoir chang\u00e9 depuis.</p> <p></p>"},{"location":"1-Experiences/Kubeflow/#installation","title":"Installation","text":""},{"location":"1-Experiences/Kubeflow/#connectez-vous-a-kubeflow","title":"Connectez-vous \u00e0 Kubeflow","text":"<p>Connectez-vous au portail Azure \u00e0 l'aide de vos identifiants cloud</p> <p>Vous devez vous connecter au portail Azure  en utilisant vos informations d'identification StatCan .<code>first.lastname@cloud.statcan.ca</code> ou  en utilisant vos informations d'identification StatCan  <code>first.lastname@statcan.gc.ca</code>. Vous pouvez le faire en utilisant Portail Azure. </p> <ul> <li>Se connecter \u00e0 Kubeflow</li> </ul> <ul> <li>Acc\u00e9dez \u00e0 l'onglet Serveurs bloc-notes</li> </ul> <p></p> <ul> <li>Puis clique + Nouveau serveur</li> </ul>"},{"location":"1-Experiences/Kubeflow/#configuration-de-votre-serveur","title":"Configuration de votre serveur","text":"<ul> <li>Vous obtiendrez un mod\u00e8le pour cr\u00e9er votre serveur de bloc-notes. Remarque   : le nom de votre serveur doit \u00eatre en lettres minuscules avec des tirets.   Pas d'espaces, et non souligne.</li> </ul> <ul> <li>Vous devrez choisir une image. V\u00e9rifiez le nom des images et choisissez-en une   qui correspond \u00e0 ce que tu veux faire. (Vous ne savez pas lequel choisir ?   V\u00e9rifiez vos options ici.)</li> </ul> <ul> <li>Si vous souhaitez utiliser un GPU, v\u00e9rifiez si l'image indique <code>cpu</code> ou <code>gpu</code>.</li> </ul>"},{"location":"1-Experiences/Kubeflow/#cpu-et-memoire","title":"CPU et m\u00e9moire","text":"<ul> <li> <p>Au moment de la r\u00e9daction (23 d\u00e9cembre, 2021), il existe deux types   d'ordinateurs dans la grappe</p> <ul> <li>CPU: <code>D16s v3</code> (16 CPU , 64 G m\u00e9moire; 15 CPU et 48 G m\u00e9moire sont   disponible pour l'utilisateur, 1 CPU et 16 G m\u00e9moire sont r\u00e9serv\u00e9s pour   l'utilisation du syst\u00e8me)</li> <li>GPU: <code>NC6s_v3</code> (6 CPU , 112 G m\u00e9moire, 1 GPU; 96 G de m\u00e9moire disponible   pour l'utilisateur, 16 G sont r\u00e9serv\u00e9s pour l'utilisation du syst\u00e8me)</li> </ul> </li> </ul> <p>Lors de la cr\u00e9ation d'un serveur de bloc-notes, le syst\u00e8me vous limitera aux sp\u00e9cifications maximales ci-dessus. Pour les serveurs de bloc-notes CPU, vous pouvez sp\u00e9cifier la quantit\u00e9 exacte de CPU et de m\u00e9moire dont vous avez besoin. Cela vous permet de r\u00e9pondre \u00e0 vos besoins de calcul tout en minimisant les co\u00fbts. Pour un serveur portable GPU, vous obtiendrez toujours le serveur complet (6 c\u0153urs CPU, 96 Gio de m\u00e9moire accessible et 1 GPU).</p> <p>\u00c0 l'avenir, il se peut que des machines plus grandes soient disponibles, vous pourriez donc avoir des restrictions plus souples.</p> <p>Bogue de cr\u00e9ation de n\u0153ud lent.</p> <p>En raison d'un bug avec le pare-feu, la cr\u00e9ation d'un nouveau n\u0153ud peut \u00eatre tr\u00e8s lente dans certains cas (jusqu'\u00e0 quelques heures). Un correctif pour ce probl\u00e8me est en cours.</p> <p>Utilisez les machines GPU de mani\u00e8re responsable</p> <p>Il y a moins de machines GPU que de machines CPU, alors utilisez-les de mani\u00e8re responsable.</p>"},{"location":"1-Experiences/Kubeflow/#stockage-de-vos-donnees","title":"Stockage de vos donn\u00e9es","text":"<p>-Vous aurez envie de cr\u00e9er un volume de donn\u00e9es ! Vous pourrez enregistrer votre travail ici, et si vous \u00e9teignez votre serveur, vous pourrez simplement remonter vos anciennes donn\u00e9es en entrant le nom de votre ancien disque. Il est important que vous vous souveniez du nom du volume.</p> <p></p> <p>V\u00e9rifiez les anciens volumes en regardant l'option Existant</p> <p>Lorsque vous cr\u00e9ez votre serveur vous avez la possibilit\u00e9 de r\u00e9utiliser un ancien volume ou en cr\u00e9er un nouveau. Vous souhaitez probablement r\u00e9utiliser votre ancien volume.</p>"},{"location":"1-Experiences/Kubeflow/#et-creer","title":"Et... Cr\u00e9er!!!","text":"<ul> <li>Si vous \u00eates satisfait des param\u00e8tres, vous pouvez maintenant cr\u00e9er le serveur   ! Cela pourrait prenez quelques minutes pour d\u00e9marrer en fonction des   ressources que vous avez demand\u00e9es. (GPU prendre plus de temps.)</li> </ul> <p>Votre serveur est en cours d'ex\u00e9cution</p> <p>Si tout se passe bien, votre serveur devrait fonctionner !!! Vous aurez maintenant le possibilit\u00e9 de se connecter, et essayer Jupyter!</p>"},{"location":"1-Experiences/Kubeflow/#une-fois-que-vous-avez-les-bases","title":"Une fois que vous avez les bases...","text":""},{"location":"1-Experiences/Kubeflow/#partagez-votre-espace-de-travail","title":"Partagez votre espace de travail","text":"<p>Dans Kubeflow, chaque utilisateur dispose d'un espace de noms qui contient son travail (son serveurs de blocs-note, pipelines, disques, etc.). Votre espace de nom vous appartient, mais peut \u00eatre partag\u00e9 si vous souhaitez collaborer avec d'autres. Pour plus de d\u00e9tails sur collaboration sur la plateforme, voir Collaboration.</p>"},{"location":"1-Experiences/MLflow/","title":"Aper\u00e7u","text":"<p>!!! danger \"MLflow a \u00e9t\u00e9 retir\u00e9 du projet AAW.     Si vous en avez besoin, contactez l'\u00e9quipe de d\u00e9veloppement\"</p> <p>MLflow s'agit d'une plateforme libre pour la gestion du cycle de vie de l'apprentissage automatique. C'est un \"registre de mod\u00e8les\" pour stocker vos mod\u00e8les d'apprentissage automatique et les m\u00e9triques associ\u00e9es. Vous pouvez utiliser l'interface web pour examiner vos mod\u00e8les, et vous pouvez utiliser son API REST pour enregistrer vos mod\u00e8les depuis Python, en utilisant le [paquet mlflow pip] (https://pypi.org/project/mlflow/).</p> <p></p>"},{"location":"1-Experiences/RStudio/","title":"RStudio","text":"<p>Vous pouvez utiliser l'image <code>rstudio</code> pour obtenir un environnement RStudio!</p> <p></p> <p>Vous pouvez installer les paquets <code>R</code> ou <code>python</code> avec <code>conda</code> ou <code>install.packages()</code>.</p>"},{"location":"1-Experiences/RStudio/#r-shiny","title":"R-Shiny","text":"<p>Vous pouvez aussi utiliser <code>Shiny</code>! Et le tableau de bord appara\u00eetra dans une nouvelle fen\u00eatre.</p> <p></p>"},{"location":"1-Experiences/Selectionner-une-Image/","title":"S\u00e9lectionner une Image pour votre Serveur bloc-notes","text":"<p>Selon votre projet ou votre utilisation souhait\u00e9e du serveur bloc-notes, certaines images seront plus appropri\u00e9es que d'autres.</p> <p>Ce qui suit passera en revue quelques caract\u00e9ristiques de chaque image pour vous aider \u00e0 choisir.</p> <p>Lors de la s\u00e9lection, vous avez 3 options principales:</p> <ul> <li>Bloc-notes Jupyter (CPU, TensorFlow, PyTorch)</li> <li>RStudio</li> <li>Bureau virtuel (r, Geomatics)</li> </ul>"},{"location":"1-Experiences/Selectionner-une-Image/#jupyter","title":"Jupyter","text":"<p>Les Bloc-notes Jupyter sont utilis\u00e9s pour cr\u00e9er et partager des documents interactifs qui contiennent un m\u00e9lange de code en direct, de visualisations et de texte. Ceux-ci peuvent \u00eatre \u00e9crits en <code>Python</code>,<code>Julia</code> ou <code>R</code>.</p> <p></p> La plupart des utilisations comprennent: <p>la transformation de donn\u00e9es, la simulation num\u00e9rique, la mod\u00e9lisation statistique, l'apprentissage automatique et autres.</p> <p>Ceux-ci s'agit d'un excellent outil pour l'analyse, y compris l'apprentissage machine. L'image <code>jupyterlab-cpu</code> fournit une bonne exp\u00e9rience de base pour <code>python</code>, y compris des paquets tels que <code>numpy</code>, <code>pandas</code> et <code>scikit-learn</code>. Si vous \u00eates int\u00e9ress\u00e9 sp\u00e9cifiquement par TensorFlow ou PyTorch, nous avons \u00e9galement les images <code>jupyterlab-tensorflow</code> et<code>jupyterlab-pytorch</code> qui viennent avec ces outils pr\u00e9-install\u00e9s.</p> <p>Pour l'image <code>jupyterlab-pytorch</code>, les packages PyTorch (torch, torchvision et torchaudio) sont install\u00e9s dans l'environnement conda <code>torch</code>. Vous devez activer cet environnement pour utiliser PyTorch.</p> <p>Pour les images <code>jupyterlab-cpu</code>, <code>jupyterlab-tensorflow</code> et <code>jupyterlab-pytorch</code>, dans le shell par d\u00e9faut, la commande <code>conda activate</code> peut ne pas fonctionner. Cela est d\u00fb au fait que l'environnement n'est pas correctement initialis\u00e9. Dans ce cas, ex\u00e9cutez \u00ab\u00a0bash\u00a0\u00bb, vous devriez voir le logo AAW et quelques instructions apparaissent. Apr\u00e8s cela, \"conda activate\" devrait fonctionner correctement. Si vous voyez le logo AAW au d\u00e9marrage, cela signifie que l'environnement est correctement initialis\u00e9 et que \u00ab\u00a0conda activate\u00a0\u00bb devrait fonctionner correctement. Un correctif pour ce bogue est en pr\u00e9paration, une fois cela corrig\u00e9, ce paragraphe sera supprim\u00e9.</p> <p>Chaque image est pr\u00e9charg\u00e9e avec VS Code dans le navigateur si vous pr\u00e9f\u00e9rez une exp\u00e9rience IDE compl\u00e8te.</p>"},{"location":"1-Experiences/Selectionner-une-Image/#rstudio","title":"RStudio","text":"<p>RStudio vous offre un environnement de d\u00e9veloppement int\u00e9gr\u00e9 sp\u00e9cifiquement pour <code>R</code>. Si vous codez en <code>R</code>, il s'agit g\u00e9n\u00e9ralement du serveur bloc-notes utiliser. Utilisez l'image <code>rstudio</code> pour obtenir un environnement RStudio.</p> <p></p>"},{"location":"1-Experiences/Selectionner-une-Image/#bureau-virtuel","title":"Bureau virtuel","text":"<p>Pour une exp\u00e9rience Ubuntu compl\u00e8te, deux versions du Bureau virtuel sont disponible. Ceux-ci sont pr\u00e9charg\u00e9s avec Python et R, mais sont livr\u00e9s dans un exp\u00e9rience typique qui est \u00e9galement fournie avec Firefox, VS Code et les outils Open Office. Le syst\u00e8me d'exploitation est Ubuntu 18.04 avec l'environnement de bureau XFCE.</p> <p><code>remote-desktop-r</code> inclut R et RStudio mais si vous avez besoin d'outils de g\u00e9omatique pour R, choisissez la version <code>remote-desktop-geomatics</code> de cette image.</p> <p></p>"},{"location":"2-Publication/Dash/","title":"Aper\u00e7u","text":"<p>Dash est un excellent outil utilis\u00e9 par beaucoup pour l'analyse et l'exploration de donn\u00e9es, la visualisation, la mod\u00e9lisation, le contr\u00f4le des instruments et la cr\u00e9ation de rapports.</p> <p>L'exemple suivant illustre une application Dash hautement r\u00e9active et personnalis\u00e9e avec peu de code.</p> <p>Ex\u00e9cution de votre serveur bloc-note et acc\u00e8s au port</p> <p>Lorsque vous ex\u00e9cutez un outil de votre Jupyter Notebook qui publie un site Web sur un port,  vous ne pourrez pas simplement y acc\u00e9der depuis <code>http://localhost:5000/</code> car  normalement sugg\u00e9r\u00e9 dans la sortie lors de l'ex\u00e9cution de l'application Web.</p> <p>Pour acc\u00e9der au serveur Web, vous devrez utiliser l'URL de base. Dans ton cahier  ex\u00e9cution terminale\u00a0:</p> <pre><code>echo https://kubeflow.covid.cloud.statcan.ca${JUPYTER_SERVER_URL:19}proxy/5000/\n</code></pre>"},{"location":"2-Publication/Dash/#visualisation-des-donnees-avec-dash","title":"Visualisation des donn\u00e9es avec Dash","text":"<p>Dash simplifie la cr\u00e9ation d'une interface graphique interactive autour de votre code d'analyse de donn\u00e9es. Ceci est un exemple de mise en page avec figure et curseur de Dash.</p> <p></p>"},{"location":"2-Publication/Dash/#plotly-dash","title":"Plotly Dash","text":"<p>Publier avec des logiciels fabriqu\u00e9s au Canada.</p> <p>Plotly Dash est une biblioth\u00e8que Python populaire qui vous permet de cr\u00e9er facilement des visualisations et des tableaux de bord Web interactifs. D\u00e9velopp\u00e9 par la soci\u00e9t\u00e9 montr\u00e9alaise Plotly, Dash a acquis la r\u00e9putation d'\u00eatre un outil puissant et flexible pour cr\u00e9er des graphiques de science des donn\u00e9es personnalis\u00e9s. Avec Dash, vous pouvez tout cr\u00e9er, des simples graphiques lin\u00e9aires aux tableaux de bord complexes de plusieurs pages avec des widgets et des commandes interactifs. Parce qu'il repose sur des technologies open source telles que Flask, React et Plotly.js, Dash est hautement personnalisable et peut \u00eatre facilement int\u00e9gr\u00e9 \u00e0 d'autres outils et workflows de science des donn\u00e9es. Que vous soyez data scientist, analyste ou d\u00e9veloppeur, Dash peut vous aider \u00e0 cr\u00e9er des visualisations attrayantes et informatives qui donnent vie \u00e0 vos donn\u00e9es.</p>"},{"location":"2-Publication/Dash/#commencer","title":"Commencer","text":"<p>Ouvrez une fen\u00eatre de terminal dans votre notebook Jupyter et ex\u00e9cutez les commandes suivantes\u00a0:</p> <pre><code>#\u00a0installations requises si elles ne sont pas d\u00e9j\u00e0 install\u00e9es\npip3 install dash==1.16.3\npip3 install pandas\n</code></pre> <p>Cr\u00e9ez un fichier appel\u00e9 app.py avec le contenu suivant\u00a0:</p> <pre><code># app.py\n#!/usr/bin/env python3\nimport dash\nimport dash_core_components as dcc\nimport dash_html_components as html\nfrom dash.dependencies import Input, Output\nimport plotly.express as px\nimport pandas as pd\ndf = pd.read_csv('https://raw.githubusercontent.com/plotly/datasets/master/gapminderDataFiveYear.csv')\nexternal_stylesheets = ['https://codepen.io/chriddyp/pen/bWLwgP.css']\napp = dash.Dash(__name__, external_stylesheets=external_stylesheets)\napp.layout = html.Div([\ndcc.Graph(id='graph-with-slider'),\ndcc.Slider(\nid='year-slider',\nmin=df['year'].min(),\nmax=df['year'].max(),\nvalue=df['year'].min(),\nmarks={str(year): str(year) for year in df['year'].unique()},\nstep=None\n)\n])\n@app.callback(\nOutput('graph-with-slider', 'figure'),\n[Input('year-slider', 'value')])\ndef update_figure(selected_year):\nfiltered_df = df[df.year == selected_year]\nfig = px.scatter(filtered_df, x=\"gdpPercap\", y=\"lifeExp\",\nsize=\"pop\", color=\"continent\", hover_name=\"country\",\nlog_x=True, size_max=55)\nfig.update_layout(transition_duration=500)\nreturn fig\nif __name__ == '__main__':\napp.run_server(debug=True)\n</code></pre>"},{"location":"2-Publication/Dash/#executez-votre-application","title":"Ex\u00e9cutez votre application","text":"<pre><code>python app.py\n# ou vous pouvez utiliser\u00a0:\nexport FLASK_APP=app.py\nflask run\n</code></pre>"},{"location":"2-Publication/Datasette/","title":"Aper\u00e7u","text":"<p>Datasette est une API JSON instantan\u00e9e pour vos bases de donn\u00e9es SQLite qui permet d'explorer la BD et d'ex\u00e9cuter des requ\u00eates SQL de mani\u00e8re plus interactive.</p> <p>Vous pouvez trouver une liste d'exemples de datasettes ici.</p> <p>\u00c9cosyst\u00e8me de Datasette</p> <p>Il existe toutes sortes d'outils pour convertir des donn\u00e9es depuis et vers sqlite. ici. Par exemple, vous pouvez charger des fichiers de forme dans sqlite, ou cr\u00e9er des graphes Vega \u00e0 partir d'une base de donn\u00e9es sqlite. SQLite fonctionne bien avec <code>R</code>, <code>Python</code>, et plusieurs autres outils.</p>"},{"location":"2-Publication/Datasette/#exemple-datasette","title":"Exemple Datasette","text":"<p>Voici quelques captures d'\u00e9cran du Datasette global-power-plants, vous pouvez pr\u00e9visualiser et explorer les donn\u00e9es dans le navigateur, que ce soit par des clics ou des requ\u00eates SQL.</p> <p></p> <p>Vous pouvez m\u00eame explorer des cartes au sein de l'outil!</p> <p>Ex\u00e9cuter des requ\u00eates SQL</p>"},{"location":"2-Publication/Datasette/#didacticiel-video","title":"Didacticiel vid\u00e9o","text":""},{"location":"2-Publication/Datasette/#commencer","title":"Commencer","text":""},{"location":"2-Publication/Datasette/#installation-des-datasettes","title":"Installation des Datasettes","text":"<p>Pour visualiser votre base de donn\u00e9es dans votre bloc-notes Jupyter, cr\u00e9ez le fichier bash suivant dans votre r\u00e9pertoire de projet, rendre le fichier ex\u00e9cutable en utilisant <code>chmod +x start.sh</code> puis ex\u00e9cutez-le avec la commande <code>./start.sh</code>. Acc\u00e9dez au serveur web en utilisant le URL de base avec le num\u00e9ro de port que vous utilisez dans le fichier ci-dessous.</p> <p>start.sh</p> <pre><code>#!/bin/bash\n# This script just starts Datasette with the correct URL, so\n# that you can use it within kubeflow.\n# Get an example database\nwget https://github.com/StatCan/aaw-contrib-r-notebooks/raw/master/database-connections/latin_phrases.db\n\n# If you have your own database, you can change this line!\nDATABASE=latin_phrases.db\n\nexport BASE_URL=\"https://kubeflow.covid.cloud.statcan.ca${JUPYTER_SERVER_URL:19}proxy/8001/\"\necho \"Base url: ${BASE_URL}\"\ndatasette $DATABASE --cors --config max_returned_rows:100000 --config sql_time_limit_ms:5500 --config base_url:${BASE_URL}\n</code></pre> <p>Regardez ce tutoriel</p> <p>Un utilisateur de la plateforme a utilis\u00e9 Datasette avec un tableau de bord. Voir la vid\u00e9o pour une d\u00e9monstration.</p> <p>Ex\u00e9cuter le serveur de votre bloc-notes et acc\u00e9der au port</p> <p>Lorsque vous ex\u00e9cutez un outil depuis votre bloc-notes Jupyter qui affiche un site web sur un port, vous ne serez pas en mesure d'y acc\u00e9der simplement \u00e0 partir de <code>http://localhost:5000/</code> comme normalement sugg\u00e9r\u00e9 dans la sortie lors de l'ex\u00e9cution de l'application web.</p> <p>Pour acc\u00e9der au serveur web, vous devrez utiliser l'URL de base. Dans le terminal du bloc-note, ex\u00e9cutez:</p> <pre><code>echo https://kubeflow.covid.cloud.statcan.ca${JUPYTER_SERVER_URL:19}proxy/5000/\n</code></pre>"},{"location":"2-Publication/PowerBI/","title":"Chargement des donn\u00e9es dans Power BI","text":"<p>Nous ne proposons pas de serveur Power BI, mais vous pouvez extraire vos donn\u00e9es dans Power BI \u00e0 partir de notre syst\u00e8me de stockage et les utiliser comme une trame de donn\u00e9es <code>pandas</code>.</p> <p></p>"},{"location":"2-Publication/PowerBI/#installation","title":"Installation","text":""},{"location":"2-Publication/PowerBI/#ce-dont-vous-aurez-besoin","title":"Ce dont vous aurez besoin","text":"<ol> <li>Un ordinateur avec Power BI et Python 3.6</li> <li>Vos <code>ACCESS_KEY</code> et <code>SECRET_KEY</code> MinIO (voir Stockage)</li> </ol>"},{"location":"2-Publication/PowerBI/#connectez-vous","title":"Connectez-vous","text":""},{"location":"2-Publication/PowerBI/#configurez-power-bi","title":"Configurez Power BI","text":"<p>Ouvrez votre syst\u00e8me Power BI et ouvrez d\u00e9marrage rapide de Power BI dans votre \u00e9diteur de texte pr\u00e9f\u00e9r\u00e9.</p> <p>Assurez-vous que <code>pandas</code>, <code>boto3</code> et <code>numpy</code> sont install\u00e9s et que vous utilisez le bon environnement virtuel Conda (le cas \u00e9ch\u00e9ant).</p> <p></p> <p>Ensuite, assurez-vous que Power BI utilise le bon environnement Python. Vous pouvez modifier cet \u00e9l\u00e9ment \u00e0 partir du menu des options. Le chemin d'acc\u00e8s exact est indiqu\u00e9 dans le guide de d\u00e9marrage rapide.</p>"},{"location":"2-Publication/PowerBI/#modifiez-votre-script-python","title":"Modifiez votre script Python","text":"<p>Ensuite, modifiez votre script Python pour utiliser vos <code>ACCESS_KEY</code> et <code>SECRET_KEY</code> MinIO, puis cliquez sur \u00ab Obtenir des donn\u00e9es \u00bb et copiez-le en tant que script Python.</p> <p></p>"},{"location":"2-Publication/R-Shiny/","title":"Aper\u00e7u","text":"<p>R-Shiny est un package R qui facilite la cr\u00e9ation d'applications Web interactives dans R.</p> <p>H\u00e9bergement d'applications R Shiny</p> <p>Nous ne prenons actuellement pas en charge l'h\u00e9bergement d'applications R Shiny, mais vous pouvez les cr\u00e9er. Nous souhaitons activer l'h\u00e9bergement de l'application R Shiny \u00e0 l'avenir.</p> <p></p>"},{"location":"2-Publication/R-Shiny/#r-brillant","title":"R brillant","text":"<p>Publier des graphismes de qualit\u00e9 professionnelle</p> <p></p> <p>R Shiny est un framework d'application Web open source qui permet aux data scientists et aux analystes de cr\u00e9er des tableaux de bord et des visualisations de donn\u00e9es interactifs bas\u00e9s sur le Web \u00e0 l'aide du langage de programmation R. L'un des principaux avantages de R Shiny est qu'il offre un moyen simple de cr\u00e9er des tableaux de bord interactifs de haute qualit\u00e9 sans avoir besoin d'une expertise approfondie en d\u00e9veloppement Web. Avec R Shiny, les data scientists peuvent tirer parti de leurs comp\u00e9tences en codage R pour cr\u00e9er des applications Web dynamiques, bas\u00e9es sur les donn\u00e9es, qui peuvent \u00eatre facilement partag\u00e9es avec les acteurs.</p> <p>Un autre avantage de R Shiny est qu'il prend en charge une vari\u00e9t\u00e9 de visualisations de donn\u00e9es qui peuvent \u00eatre facilement personnalis\u00e9es pour r\u00e9pondre aux besoins du projet. Les utilisateurs peuvent cr\u00e9er une large gamme de diagrammes et de graphiques, allant de simples diagrammes \u00e0 barres et nuages de points \u00e0 des cartes thermiques et des graphiques de r\u00e9seau plus complexes. De plus, R Shiny prend en charge une vari\u00e9t\u00e9 de widgets interactifs qui permettent aux utilisateurs de manipuler et d'explorer des donn\u00e9es en temps r\u00e9el.</p> <p></p> <p>R Shiny est \u00e9galement hautement extensible et peut \u00eatre int\u00e9gr\u00e9 \u00e0 d'autres outils et plates-formes open source pour cr\u00e9er des workflows de science des donn\u00e9es de bout en bout. Avec ses fonctionnalit\u00e9s puissantes et flexibles, R Shiny est un choix populaire pour cr\u00e9er des tableaux de bord de visualisation de donn\u00e9es pour un large \u00e9ventail d'applications, de la recherche scientifique \u00e0 l'analyse commerciale. Dans l'ensemble, R Shiny offre une solution puissante, personnalisable et rentable pour cr\u00e9er des tableaux de bord interactifs et des visualisations de donn\u00e9es.</p> <p>Utilisez R-Shiny pour cr\u00e9er des applications Web interactives directement \u00e0 partir de R. Vous pouvez d\u00e9ployer votre tableau de bord R Shiny en soumettant une demande d'extraction \u00e0 notre r\u00e9f\u00e9rentiel R-Dashboards GitHub .</p>"},{"location":"2-Publication/R-Shiny/#r-editeur-dinterface-utilisateur-brillant","title":"R \u00c9diteur d'interface utilisateur brillant","text":"<p>Le script Rscript suivant installe les packages requis pour ex\u00e9cuter \"shinyuieditor\" sur l'ETAA. Il commence par installer les packages R n\u00e9cessaires et utilise <code>conda</code> pour installer <code>yarn</code>.</p> <p>Une fois l'installation termin\u00e9e, vous pouvez acc\u00e9der au code de votre application dans <code>./my-app</code></p> <p>Ex\u00e9cutez ce script depuis <code>rstudio</code>. RStudio peut demander la permission d'ouvrir une nouvelle fen\u00eatre si vous avez un bloqueur de popup.</p> setup-shinyuieditor.R<pre><code>#!/usr/bin/env Rscript\n#' Installer les packages n\u00e9cessaires\ninstall.packages(c(\n\"shiny\",\n\"plotly\",\n\"gridlayout\",\n\"bslib\",\n\"remotes\",\n\"rstudioapi\"\n))\n#' n'a pas \u00e9t\u00e9 install\u00e9 lors de l'installation ci-dessus\ninstall.packages(\"DT\")\n#' Cela installe shinyuieditor de Github\nremotes::install_github(\"rstudio/shinyuieditor\", upgrade = F)\n#' Nous avons besoin de fil donc nous allons l'installer avec conda\nsystem(\"conda install yarn\", wait = T)\n#' Ceci clone Shinyuieditor et un exemple d'application de Github\nsystem(\"git clone https://github.com/rstudio/shinyuieditor\", wait = T)\n#' Copiez l'application des vignettes vers notre r\u00e9pertoire de travail actuel\nsystem(\"cp -R ./shinyuieditor/vignettes/demo-app/ ./my-app\")\n#' D\u00e9finit le r\u00e9pertoire de travail actuel sur le r\u00e9pertoire racine de l'application\nsetwd(\"./my-app\")\n#' Yarn va mettre en place notre projet\nsystem(\"yarn install\", wait = T)\n#' Chargez et lancez shinyuieditor\nlibrary(shinyuieditor)\nshinyuieditor::launch_editor(app_loc = \"./\")\n</code></pre>"},{"location":"2-Publication/R-Shiny/#choisissez-un-modele-dapplication","title":"Choisissez un mod\u00e8le d'application","text":"<p>La premi\u00e8re chose que vous verrez est le s\u00e9lecteur de mod\u00e8le. Il existe trois options au moment d'\u00e9crire ces lignes (<code>shinyuieditor</code> est actuellement en alpha).</p> <p></p>"},{"location":"2-Publication/R-Shiny/#mode-fichier-unique-ou-multi","title":"Mode fichier unique ou multi","text":"<p>Je recommande le mode multi-fichiers, cela mettra le code back-end dans un fichier appel\u00e9 <code>server.R</code> et le front-end dans un fichier appel\u00e9 <code>ui.R</code>.</p> <p></p>"},{"location":"2-Publication/R-Shiny/#concevez-votre-application","title":"Concevez votre application","text":"<p>Vous pouvez concevoir votre application avec du code ou l'interface utilisateur graphique. Essayez de concevoir la mise en page avec l'interface graphique et de concevoir les trac\u00e9s avec du code.</p> <p></p> <p>Toutes les modifications que vous apportez dans <code>shinyuieditor</code> appara\u00eetront imm\u00e9diatement dans le code.</p> <p></p> <p>Toute modification que vous apportez au code appara\u00eetra imm\u00e9diatement dans le <code>shinyuieditor</code>.</p> <p></p>"},{"location":"2-Publication/R-Shiny/#publication-sur-letaa","title":"Publication sur l'ETAA","text":""},{"location":"2-Publication/R-Shiny/#envoyez-simplement-une-pull-request","title":"Envoyez simplement une pull request\u00a0!","text":"<p>Tout ce que vous avez \u00e0 faire est d'envoyer une demande d'extraction \u00e0 notre r\u00e9f\u00e9rentiel R-Dashboards. Incluez votre r\u00e9f\u00e9rentiel dans un dossier avec le nom que vous voulez (par exemple, \"air-quality-dashboard\"). Ensuite, nous l'approuverons et il sera mis en ligne.</p> <p>Si vous avez besoin d'installer des biblioth\u00e8ques R suppl\u00e9mentaires, envoyez votre liste au d\u00e9p\u00f4t R-Shiny en cr\u00e9ant un probl\u00e8me GitHub et nous ajouterons les d\u00e9pendances.</p> <p></p> <p>Voir le tableau de bord ci-dessus ici</p> <p>Le tableau de bord ci-dessus est dans GitHub. Jetez un \u0153il \u00e0 la source et voir le tableau de bord en direct.</p>"},{"location":"2-Publication/R-Shiny/#une-fois-que-vous-avez-les-bases","title":"Une fois que vous avez les bases...","text":""},{"location":"2-Publication/R-Shiny/#integrer-des-tableaux-de-bord-dans-vos-sites-web","title":"Int\u00e9grer des tableaux de bord dans vos sites Web","text":"<p>Int\u00e9grer des tableaux de bord dans d'autres sites</p> <p>Nous n'avons pas encore eu l'occasion de l'examiner ou de le prototyper, mais si vous avez un cas d'utilisation, n'h\u00e9sitez pas \u00e0 contacter l'ing\u00e9nierie. Nous travaillerons avec vous pour trouver quelque chose.</p>"},{"location":"2-Publication/Sur-mesure/","title":"Applications Web personnalis\u00e9es","text":"<p>Nous pouvons tout d\u00e9ployer, dans la mesure o\u00f9 il s'agit de logiciel libre, et nous pouvons le mettre dans un conteneur Docker (p. ex. applications Node.js, Flask, Dash).</p> <p></p> <p>Voir le code de source de cette application</p> <p>Nous int\u00e9grons ces types d'applications au serveur au moyen de GitHub. La source de l'application ci-dessus est ici : <code>StatCan/covid19</code>.</p>"},{"location":"2-Publication/Sur-mesure/#comment-faire-heberger-votre-application","title":"Comment faire h\u00e9berger votre application","text":"<p>Si vous avez d\u00e9j\u00e0 une application Web dans un r\u00e9pertoire Git, d\u00e8s qu'elle est plac\u00e9e dans un conteneur Docker, nous pouvons int\u00e9grer le r\u00e9pertoire Git dans le r\u00e9pertoire GitHub de StatCan et pointer une URL vers elle. Pour la mettre \u00e0 jour, il vous suffit d'interagir avec le r\u00e9pertoire GitHub de StatCan au moyen de demandes d'extraction.</p> <p>Si vous avez des questions, n'h\u00e9sitez pas \u00e0 communiquer avec nous.</p>"},{"location":"3-Pipelines/Kubeflow-Pipelines/","title":"Vue d'ensemble","text":"<p>Kubeflow Pipelines est une plateforme de cr\u00e9ation de flux de production d'apprentissage automatique pouvant \u00eatre d\u00e9ploy\u00e9s dans un environnement Kubernetes. Il permet de cr\u00e9er des pipelines qui encapsulent les flux de production analytiques (transformation de donn\u00e9es, mod\u00e8les de formation, construction d'\u00e9l\u00e9ments visuels, etc.). Ces pipelines peuvent \u00eatre mis en commun, r\u00e9utilis\u00e9s et programm\u00e9s. Ils sont cr\u00e9\u00e9s de fa\u00e7on \u00e0 \u00eatre ex\u00e9cut\u00e9s avec les calculs fournis par Kubernetes.</p> <p>Dans le contexte de l'espace de travail en analytique avanc\u00e9e, vous pouvez interagir avec les pipelines Kubeflow par l'entremise :</p> <ul> <li>de l'interface utilisateur de Kubeflow, o\u00f9, \u00e0   partir du menu Pipelines, vous pouvez t\u00e9l\u00e9charger des pipelines, visualiser   les pipelines que vous poss\u00e9dez et leurs r\u00e9sultats, etc.</li> </ul> <ul> <li>de la trousse SDK   en Python de Kubeflow Pipelines, accessible dans les serveurs de bloc-notes   Jupyter, o\u00f9 vous pouvez d\u00e9finir vos composants et pipelines, les soumettre   pour les ex\u00e9cuter imm\u00e9diatement, ou m\u00eame les enregistrer pour plus tard.</li> </ul> Des exemples suppl\u00e9mentaires dans les bloc-notes <p>Des exemples plus exhaustifs de pipelines produits express\u00e9ment pour cette plateforme sont accessibles dans GitHub (et dans chaque serveur de bloc-notes \u00e0 <code>/jupyter-notebooks</code>). Vous pouvez \u00e9galement consulter des sources publiques.</p> <p>Voyez le documentation officiel de Kubeflow pour obtenir une explication g\u00e9n\u00e9rale d\u00e9taill\u00e9e de Kubeflow.</p> <p></p>"},{"location":"3-Pipelines/Kubeflow-Pipelines/#quest-ce-quun-pipeline-et-comment-fonctionne-t-il","title":"Qu'est-ce qu'un pipeline et comment fonctionne-t-il?","text":"<p>Dans Kubeflow Pipelines, un pipeline comprend un ou plusieurs composants de pipeline encha\u00een\u00e9s pour former un flux de production. Les composants sont comme des fonctions que le pipeline connecte ensemble.</p> <p>Le pipeline d\u00e9crit l'ensemble du flux de production de ce que vous souhaitez accomplir, tandis que les composants de pipeline d\u00e9crivent chacune des \u00e9tapes distinctes de ce processus (comme le fait d'extraire des colonnes d'un stock de donn\u00e9es, de transformer des donn\u00e9es ou d'entra\u00eener un mod\u00e8le). Chaque composant doit \u00eatre modulaire et id\u00e9alement r\u00e9utilisable.</p> <p>Essentiellement, chaque composant a :</p> <ul> <li>une application autonome, pr\u00e9sent\u00e9e sous forme d'image de menu fixe   (https://docs.docker.com/get-started/), pour effectuer le travail proprement   dit .le code dans l'image de menu fixe peut \u00eatre une s\u00e9quence de commandes en   langage naturel, un script Python ou tout autre code pouvant \u00eatre ex\u00e9cut\u00e9 \u00e0   partir d'un terminal Linux</li> <li>une description de la mani\u00e8re dont Kubeflow Pipelines ex\u00e9cute le code   (l'emplacement de l'image, les arguments de la ligne de commande qu'il   accepte, les r\u00e9sultats qu'il produit), sous forme de fichier YAML.</li> </ul> <p>Un pipeline d\u00e9finit ensuite la logique de connexion des composants, par exemple :</p> <ol> <li>Ex\u00e9cuter <code>ComposantA</code></li> <li>transmettre le r\u00e9sultat du <code>ComposantA</code> au <code>ComposantB</code> et au <code>ComposantC</code></li> <li>...</li> </ol> <p>Exemple d'un pipeline</p> <p>Voici un exemple :</p> <pre><code>#!/bin/python3\ndsl.pipeline(name=\"Estimer Pi\",\n    description=\"Estimer Pi au moyen d'un mod\u00e8le Map-Reduce\")\ndef compute_pi():\n    # Cr\u00e9er un \"exemple\" d'op\u00e9ration pour chaque valeur de d\u00e9part\n    # transmise au pipeline\n    seeds = (1,2,3,4,5,6,7,8,9,10)\n    sample_ops = [sample_op(seed) for seed in seeds]\n\n    # Obtenir les r\u00e9sultats avant de les transmettre \u00e0 deux pipelines\n    # distincts. Les r\u00e9sultats sont extraits des fichiers\n    # `output_file.json` et sont disponibles \u00e0 partir des instances\n    # `sample_op` par l'entremise de l'attribut `.outputs`.\n    outputs = [s.outputs['output'] for s in sample_ops]\n\n    _generate_plot_op = generate_plot_op(outputs)\n    _average_op = average_op(outputs)\n</code></pre> <p>Vous pouvez trouver le pipeline complet dans l'exemple <code>map-reduce-pipeline</code>.</p>"},{"location":"3-Pipelines/Kubeflow-Pipelines/#definir-et-executer-votre-premier-pipeline","title":"D\u00e9finir et ex\u00e9cuter votre premier pipeline","text":"<p>Bien que les pipelines et les composants soient d\u00e9finis par des fichiers YAML, la trousse SDK en Python vous permet de les d\u00e9finir \u00e0 partir du code Python. Voici un exemple de d\u00e9finition d'un pipeline simple en utilisant la trousse SDK en Python.</p> <p>L'objectif de notre pipeline est de calculer, au moyen de cinq nombres, les valeurs suivantes :</p> <ol> <li>la moyenne des trois premiers nombres;</li> <li>la moyenne des deux derniers nombres;</li> <li>la moyenne des r\u00e9sultats de (1) et de (2).</li> </ol> <p>Pour ce faire, nous d\u00e9finissons un pipeline qui utilise notre composant moyen pour effectuer les calculs.</p> <p>Le composant moyen est d\u00e9fini par une image de menu fixe au moyen d'un script Python qui :</p> <ul> <li>accepte un ou plusieurs nombres comme arguments de ligne de commande</li> <li>renvoie la moyenne de ces nombres, enregistr\u00e9e dans le fichier <code>out.txt</code> dans   son conteneur.</li> </ul> <p>Pour indiquer \u00e0 Kubeflow Pipelines comment utiliser cette image, nous d\u00e9finissons notre composant moyen par l'entremise d'un <code>ContainerOp</code>, qui indique \u00e0 Kubeflow l'interface API de notre image. L'instance <code>ContainerOp</code> d\u00e9finit l'emplacement de l'image du menu fixe, la fa\u00e7on de lui transmettre des arguments et les r\u00e9sultats \u00e0 extraire du conteneur. Pour utiliser r\u00e9ellement ces <code>ContainerOp</code> dans notre pipeline, nous cr\u00e9ons des fonctions de fabrique comme <code>average_op</code> (car nous voudrons probablement plus d'un composant moyen).</p> <pre><code>from kfp import dsl\ndef average_op(\\*numbers):\n\"\"\" Fabrique de ContainerOp moyen\n    Accepte un nombre arbitraire de nombres d'entr\u00e9e, en renvoyant un ContainerOp\n    qui transmet ces nombres \u00e0 l'image du menu fixe sous-jacent pour faire la\n    moyenne\n    Renvoie le r\u00e9sultat recueilli \u00e0 partir du fichier ./out.txt \u00e0 l'int\u00e9rieur du\n    conteneur\n    \"\"\"\n# Validation d'entr\u00e9e\nif len(numbers) &lt; 1:\nraise ValueError(\"Doit pr\u00e9ciser au moins un nombre \u00e0 partir duquel calculer la moyenne\")\nreturn dsl.ContainerOp(\nname=\"averge\", # \u00c9l\u00e9ment affich\u00e9 dans la visionneuse de pipeline\nimage=\"k8scc01covidacr.azurecr.io/kfp-components/average:v1\", # L'image\nex\u00e9cut\u00e9e par Kubeflow Pipelines pour faire le travail arguments=numbers,\n#transmet chaque nombre comme un argument de ligne de commande (cha\u00eene) distinct\n# Le script \u00e0 l'int\u00e9rieur du conteneur enregistre le r\u00e9sultat (sous forme de cha\u00eene de caract\u00e8res) dans le fichier out.txt, que\n# Kubeflow Pipelines lit pour nous et r\u00e9cup\u00e8re sous forme de cha\u00eene.\nfile_outputs={'data': './out.txt'},\n)\n</code></pre> <p>Nous d\u00e9finissons notre pipeline comme une fonction Python qui utilise les fabriques de <code>ComponentOp</code> ci-dessus, d\u00e9cor\u00e9es par l'\u00e9l\u00e9ment d\u00e9coratif <code>@dsl.pipeline</code>. Notre pipeline utilise notre composant moyen en lui transmettant des nombres. Puis, nous utilisons les r\u00e9sultats moyens en les transmettant \u00e0 des fonctions plus tard par l'acc\u00e8s \u00e0 <code>avg\\_\\*.output</code>.</p> <pre><code>@dsl.pipeline(\nname=\"nom de mon pipeline\"\n)\ndef my_pipeline(a, b, c, d, e):\n\"\"\"\n    Calcul de moyenne de pipeline, qui accepte cinq nombres et effectue quelques calculs de moyenne sur ceux-ci\n    \"\"\"\n# Calculer les moyennes pour deux groupes\navg_1 = average_op(a, b, c)\navg_2 = average_op(d, e)\n# Utiliser les r\u00e9sultats de \\_1 et de \\_2 pour calculer une moyenne globale\naverage_result_overall = average_op(avg_1.output, avg_2.output)\n</code></pre> <p>Enfin, nous enregistrons une d\u00e9finition YAML de notre pipeline pour la transmettre plus tard \u00e0 Kubeflow Pipelines. Ce fichier YAML d\u00e9crit \u00e0 Kubeflow Pipelines exactement comment ex\u00e9cuter notre pipeline. D\u00e9compressez-le et voyez par vous-m\u00eame!</p> <pre><code>from kfp import compiler\npipeline_yaml = 'pipeline.yaml.zip'\ncompiler.Compiler().compile(\nmy_pipeline,\npipeline_yaml\n) print(f\"D\u00e9finition de pipeline export\u00e9e vers {pipeline_yaml}\")\n</code></pre> <p>??? avertissement \"Kubeflow Pipelines est une b\u00eate paresseuse\".     Il est utile de garder \u00e0 l'esprit le calcul qui se produit lorsque vous     ex\u00e9cutez ce code Python par rapport \u00e0 ce qui se passe lorsque vous soumettez     le pipeline \u00e0 Kubeflow Pipelines. Bien que tout semble se produire     instantan\u00e9ment, essayez d'ajouter <code>print(avg_1.output)</code> au pipeline     ci-dessus et voyez ce qui se passe lorsque vous compilez votre pipeline. La     trousse SDK en Python que nous utilisons sert \u00e0 cr\u00e9er des pipelines, et non     \u00e0 les ex\u00e9cuter, de sorte que les r\u00e9sultats des composants ne seront jamais     disponibles lorsque vous ex\u00e9cuterez ce code Python. Ce point est abord\u00e9 plus     en d\u00e9tail plus loin, dans la section Comprendre l'ordre des calculs.</p> <p>Pour ex\u00e9cuter notre pipeline, nous d\u00e9finissons une exp\u00e9rience :</p> <pre><code>experiment_name = \"calcul de moyenne de pipeline\"\nimport kfp\nclient = kfp.Client()\nexp = client.create_experiment(name=experiment_name)\npl_params = { 'a': 5, 'b': 5, 'c': 8, 'd': 10, 'e': 18, }\n</code></pre> <p>Voici ce qui peut \u00eatre observ\u00e9 dans l'interface utilisateur de Kubeflow Pipelines :</p> <p></p> <p>Ensuite, nous ex\u00e9cutons une instance de notre pipeline en utilisant les arguments souhait\u00e9s :</p> <pre><code>import time\nrun = client.run_pipeline(\nexp.id, # Ex\u00e9cuter dans l'exp\u00e9rience ci-dessus\nexperiment_name + '-' + time.strftime(\"%Y%m%d-%H%M%S\"), # Donner un nom et une\nheure syst\u00e8me \u00e0 notre t\u00e2che unique pipeline_yaml, # Transmettre le .yaml.zip que\nnous avons cr\u00e9\u00e9 ci-dessus. Il d\u00e9finit le pipeline params=pl_params # Transmettre\nles param\u00e8tres en fonction desquels nous souhaitons ex\u00e9cuter le pipeline\n)\n</code></pre> <p>Voici ce que l'on peut \u00e9galement voir dans l'interface utilisateur :</p> <p></p> <p>Plus tard, lorsque nous souhaiterons r\u00e9utiliser le pipeline, nous pourrons transmettre diff\u00e9rents arguments et tout recommencer (et m\u00eame le r\u00e9utiliser \u00e0 partir de l'interface utilisateur de Kubeflow). Pour mieux comprendre cet exemple, ouvrez-le dans Kubeflow et essayez-le vous-m\u00eame.</p>"},{"location":"3-Pipelines/Kubeflow-Pipelines/#composants-legers","title":"Composants l\u00e9gers","text":"<p>En construction, malheureusement!</p>"},{"location":"3-Pipelines/Kubeflow-Pipelines/#comprendre-lordre-des-calculs","title":"Comprendre l'ordre des calculs","text":"<p>En construction, malheureusement!</p>"},{"location":"3-Pipelines/Machine-Learning-Model-Cloud-Storage/","title":"Donn\u00e9es B prot\u00e9g\u00e9es par Statcan","text":"<p>Prot\u00e9g\u00e9 B</p> <p>L'ETAA est certifi\u00e9 pour l'h\u00e9bergement de donn\u00e9es prot\u00e9g\u00e9 B !</p> <p>Afin de t\u00e9l\u00e9charger des donn\u00e9es prot\u00e9g\u00e9es B sur l'ETAA, les utilisateurs devront demander l'acc\u00e8s via l'Unit\u00e9 de Succ\u00e8s \u00e0 la Client\u00e8le (USC). Les utilisateurs d'ETAA devront \u00e9galement fournir un espace de noms, obtenir un sponsor et obtenir l'approbation d'OPMIC. Une fois le processus approuv\u00e9, notre \u00e9quipe infrastructure de donn\u00e9es F.A.I.R cr\u00e9era alors un dossier sur Net A qui \u00e0 son tour donnera acc\u00e8s aux utilisateurs via le r\u00e9pertoire actif. Les donn\u00e9es pourront alors \u00eatre transf\u00e9r\u00e9es de Net A vers le cloud ETAA.</p> <p>Le stockage des mod\u00e8les d'apprentissage automatique dans un environnement de stockage cloud prot\u00e9g\u00e9 est essentiel pour garantir la s\u00e9curit\u00e9 et la confidentialit\u00e9 des donn\u00e9es sensibles. L'espace de travail d'analyse avanc\u00e9e (ETAA) fournit un environnement de stockage cloud s\u00e9curitaire et robuste qui peut \u00eatre utilis\u00e9 pour stocker des mod\u00e8les d'apprentissage automatique et d'autres actifs de donn\u00e9es.</p> <p>La plate-forme ETAA fournit un environnement de stockage cloud prot\u00e9g\u00e9 con\u00e7u pour r\u00e9pondre aux exigences les plus strictes en mati\u00e8re de s\u00e9curit\u00e9 et de confidentialit\u00e9 des donn\u00e9es. L'environnement de stockage est prot\u00e9g\u00e9 par un cryptage et des contr\u00f4les d'acc\u00e8s conformes aux normes de l'industrie, ce qui garantit que seul le personnel autoris\u00e9 peut acc\u00e9der aux donn\u00e9es sensibles. Cela prot\u00e8ge contre les acc\u00e8s non autoris\u00e9s, les violations de donn\u00e9es et autres menaces de s\u00e9curit\u00e9.</p> <p>En plus de ses fonctions de s\u00e9curit\u00e9 robustes, l'environnement de stockage en cloud ETAA est \u00e9galement hautement \u00e9volutif et flexible. Cela signifie que les data scientists et les ing\u00e9nieurs en apprentissage automatique peuvent facilement faire \u00e9voluer leurs besoins de stockage \u00e0 mesure que leurs ensembles de donn\u00e9es et la taille de leurs mod\u00e8les augmentent. Cela leur permet de stocker et de g\u00e9rer de gros volumes de donn\u00e9es et de mod\u00e8les sans avoir \u00e0 se soucier des limitations de stockage ou des goulots d'\u00e9tranglement des performances.</p> <p>Le stockage des mod\u00e8les d'apprentissage automatique dans un environnement de stockage cloud prot\u00e9g\u00e9 sur l'espace de travail d'analyse avanc\u00e9e fournit une solution s\u00e9curis\u00e9e, \u00e9volutive et flexible pour la gestion et la protection des donn\u00e9es sensibles. En tirant parti des capacit\u00e9s de stockage dans le cloud fournies par la plate-forme ETAA, les data scientists et les ing\u00e9nieurs en apprentissage automatique peuvent se concentrer sur la cr\u00e9ation et le d\u00e9ploiement de leurs mod\u00e8les en toute confiance, sachant que leurs donn\u00e9es sont prot\u00e9g\u00e9es et s\u00e9curis\u00e9es.</p>"},{"location":"3-Pipelines/Machine-Learning-Model-Cloud-Storage/#stockage-en-ligne","title":"Stockage en ligne","text":"<p>Avantages du stockage en cloud</p> <p>Le stockage en cloud offre plusieurs avantages pour la science des donn\u00e9es et l'apprentissage automatique, notamment en termes d'\u00e9volutivit\u00e9, d'accessibilit\u00e9 et de rentabilit\u00e9.</p> <p>Premi\u00e8rement, le stockage dans le cloud permet aux data scientists de stocker et de traiter de grandes quantit\u00e9s de donn\u00e9es sans avoir \u00e0 se soucier des limites du stockage local. Ceci est particuli\u00e8rement important dans le contexte de l'apprentissage automatique, o\u00f9 de grands ensembles de donn\u00e9es sont n\u00e9cessaires pour la formation et le test des mod\u00e8les. Le stockage dans le cloud permet aux data scientists d'augmenter leur capacit\u00e9 de stockage selon les besoins, sans avoir \u00e0 investir dans du mat\u00e9riel co\u00fbteux.</p> <p>Deuxi\u00e8mement, le stockage en cloud permet aux data scientists d'acc\u00e9der aux donn\u00e9es de n'importe o\u00f9, en utilisant n'importe quel appareil dot\u00e9 d'une connexion Internet. Cela permet la collaboration entre des \u00e9quipes g\u00e9ographiquement dispers\u00e9es et permet aux data scientists de travailler \u00e0 distance. De plus, le stockage dans le cloud facilite le partage de donn\u00e9es avec d'autres parties prenantes, telles que des partenaires commerciaux ou des clients. Enfin, le stockage dans le cloud est g\u00e9n\u00e9ralement plus rentable que le stockage sur site, en particulier pour les petites entreprises ou celles dont les ressources informatiques sont limit\u00e9es.</p> <p>Dans l'ensemble, le stockage en cloud est une solution fiable et pratique pour stocker et g\u00e9rer vos donn\u00e9es. Que vous ayez besoin de stocker de grandes quantit\u00e9s de donn\u00e9es ou seulement quelques fichiers, le stockage en cloud facilite la gestion de vos besoins de stockage sans les tracas des solutions de stockage traditionnelles.</p> <p>La plateforme ETAA propose plusieurs types de stockage :</p> <ul> <li>Disques (\u00e9galement appel\u00e9s Volumes sur l'\u00e9cran de cr\u00e9ation de Kubeflow Notebook Server)</li> <li>Buckets (\"Blob\" ou stockage S3, fournis via MinIO)</li> <li>Lacs de donn\u00e9es (\u00e0 venir)</li> </ul> <p>Selon votre cas d'utilisation, le disque ou le bucket peut \u00eatre le plus appropri\u00e9. Notre aper\u00e7u du stockage vous aidera \u00e0 les comparer.</p>"},{"location":"3-Pipelines/Machine-Learning-Model-Cloud-Storage/#disques","title":"Disques","text":"<p>Disks sont ajout\u00e9s \u00e0 votre serveur notebook en ajoutant des volumes de donn\u00e9es.</p>"},{"location":"3-Pipelines/Machine-Learning-Model-Cloud-Storage/#seaux","title":"Seaux","text":"<p>MinIO est un syst\u00e8me de stockage d'objets compatible S3-API qui fournit une alternative open source aux services de stockage cloud propri\u00e9taires. Bien que nous utilisions actuellement MinIO comme solution de stockage dans le cloud, nous pr\u00e9voyons de le remplacer par s3-proxy dans un proche avenir. S3-proxy est un serveur proxy inverse l\u00e9ger et open source qui vous permet d'acc\u00e9der \u00e0 des services de stockage compatibles avec Amazon S3 avec vos applications existantes. En passant \u00e0 s3-proxy, nous pourrons am\u00e9liorer les performances, la s\u00e9curit\u00e9 et l'\u00e9volutivit\u00e9 de notre stockage dans le cloud, tout en maintenant la compatibilit\u00e9 avec l'API S3.</p> <p></p> <p>MinIO est un magasin d'objets \u00e9volutif cloud natif. Nous l'utilisons pour les buckets (stockage blob ou S3).</p>"},{"location":"3-Pipelines/Machine-Learning-Model-Cloud-Storage/#lacs-de-donnees-a-venir","title":"Lacs de donn\u00e9es (\u00e0 venir)","text":"<p>Un lac de donn\u00e9es est un r\u00e9f\u00e9rentiel central qui vous permet de stocker toutes vos donn\u00e9es structur\u00e9es et non structur\u00e9es \u00e0 n'importe quelle \u00e9chelle. C'est un moyen rentable de stocker et de g\u00e9rer tous les types de donn\u00e9es, des donn\u00e9es brutes aux donn\u00e9es trait\u00e9es, et c'est un outil essentiel pour les data scientists.</p> <p>L'un des principaux avantages d'un lac de donn\u00e9es est sa flexibilit\u00e9. Il permet de stocker tous types de donn\u00e9es sans avoir besoin de d\u00e9finir un sch\u00e9ma au pr\u00e9alable, ce qui est particuli\u00e8rement utile lorsqu'il s'agit de donn\u00e9es non structur\u00e9es. Cette flexibilit\u00e9 permet aux data scientists d'explorer, d'exp\u00e9rimenter et d'extraire facilement des informations \u00e0 partir de leurs donn\u00e9es sans \u00eatre contraints par les limites d'une base de donn\u00e9es relationnelle traditionnelle.</p> <p>Les lacs de donn\u00e9es permettent \u00e9galement aux data scientists de centraliser leurs donn\u00e9es, ce qui facilite la gestion et l'analyse de gros volumes de donn\u00e9es provenant de diverses sources. Avec un lac de donn\u00e9es, les data scientists peuvent facilement ing\u00e9rer et stocker des donn\u00e9es provenant de diverses sources, telles que des bases de donn\u00e9es, le stockage dans le cloud et des API tierces. De plus, les lacs de donn\u00e9es fournissent souvent des fonctionnalit\u00e9s pour la gouvernance des donn\u00e9es, la gestion des m\u00e9tadonn\u00e9es et le contr\u00f4le d'acc\u00e8s, ce qui permet de garantir que les donn\u00e9es sont de haute qualit\u00e9 et conformes aux r\u00e9glementations en vigueur.</p> <p>De plus, les lacs de donn\u00e9es bas\u00e9s sur le cloud offrent des solutions de stockage \u00e9volutives et \u00e9conomiques qui peuvent \u00eatre facilement \u00e9tendues en un clic. \u00c0 mesure que les besoins de stockage de donn\u00e9es d'un scientifique des donn\u00e9es augmentent, il peut ajouter une capacit\u00e9 de stockage suppl\u00e9mentaire \u00e0 son lac de donn\u00e9es avec un minimum d'effort, sans se soucier de l'infrastructure sous-jacente ou de la maintenance.</p> <p>Dans l'ensemble, les lacs de donn\u00e9es sont un outil essentiel pour les data scientists, car ils offrent la flexibilit\u00e9, l'\u00e9volutivit\u00e9 et la facilit\u00e9 d'utilisation n\u00e9cessaires pour stocker et g\u00e9rer de gros volumes de donn\u00e9es, permettant aux data scientists de se concentrer sur l'extraction d'informations et de valeur \u00e0 partir des donn\u00e9es.</p>"},{"location":"3-Pipelines/Machine-Learning-Model-Serving/","title":"Introduction au mod\u00e8le de service","text":"<p>Dans le contexte des gouvernements, servir des mod\u00e8les d'apprentissage automatique signifie rendre les mod\u00e8les form\u00e9s disponibles pour \u00eatre utilis\u00e9s par d'autres applications et syst\u00e8mes. Cela peut inclure la r\u00e9alisation de pr\u00e9dictions ou de classifications bas\u00e9es sur les donn\u00e9es d'entr\u00e9e, ou la fourniture d'informations et de recommandations bas\u00e9es sur les r\u00e9sultats de l'analyse des donn\u00e9es.</p> <p>Servir des mod\u00e8les d'apprentissage automatique dans un contexte gouvernemental soul\u00e8ve des questions importantes li\u00e9es \u00e0 la confidentialit\u00e9 des donn\u00e9es. Les agences gouvernementales sont souvent responsables de la collecte et de la gestion des donn\u00e9es personnelles sensibles, telles que les dossiers m\u00e9dicaux, les donn\u00e9es financi\u00e8res et les casiers judiciaires. Lors de la diffusion de mod\u00e8les d'apprentissage automatique, il est essentiel de s'assurer que ces donn\u00e9es sont prot\u00e9g\u00e9es et que leur acc\u00e8s est strictement contr\u00f4l\u00e9.</p> <p>Pour r\u00e9pondre \u00e0 ces pr\u00e9occupations, les agences gouvernementales doivent mettre en \u0153uvre des mesures solides de confidentialit\u00e9 et de s\u00e9curit\u00e9 des donn\u00e9es lorsqu'elles servent des mod\u00e8les d'apprentissage automatique. Cela pourrait inclure le chiffrement des donn\u00e9es au repos et en transit, la mise en \u0153uvre de contr\u00f4les d'acc\u00e8s et d'authentification des utilisateurs, et la surveillance r\u00e9guli\u00e8re des violations de donn\u00e9es et des vuln\u00e9rabilit\u00e9s.</p> <p>En plus de la confidentialit\u00e9 et de la s\u00e9curit\u00e9 des donn\u00e9es, il est \u00e9galement important de prendre en compte les implications \u00e9thiques de servir des mod\u00e8les d'apprentissage automatique dans un contexte gouvernemental. Les mod\u00e8les d'apprentissage automatique peuvent \u00eatre biais\u00e9s ou discriminatoires, entra\u00eenant un traitement injuste de certains groupes de personnes. Pour att\u00e9nuer ces risques, les agences gouvernementales doivent soigneusement \u00e9valuer et surveiller leurs mod\u00e8les d'apprentissage automatique, et prendre des mesures pour lutter contre les pr\u00e9jug\u00e9s ou la discrimination qui pourraient survenir.</p> <p>Dans l'ensemble, servir des mod\u00e8les d'apprentissage automatique dans un contexte gouvernemental n\u00e9cessite un examen attentif de la confidentialit\u00e9 des donn\u00e9es, de la s\u00e9curit\u00e9 et des pr\u00e9occupations \u00e9thiques. En mettant en \u0153uvre des mesures solides pour prot\u00e9ger les donn\u00e9es personnelles et pr\u00e9venir les pr\u00e9jug\u00e9s, les agences gouvernementales peuvent tirer parti de la puissance de l'apprentissage automatique pour prendre de meilleures d\u00e9cisions et am\u00e9liorer les r\u00e9sultats pour les citoyens tout en maintenant la confiance et la transparence.</p>"},{"location":"3-Pipelines/Machine-Learning-Model-Serving/#pourquoi-servir-avec-nous","title":"Pourquoi servir avec nous ?","text":"<p>Servir des mod\u00e8les d'apprentissage automatique avec l'espace de travail d'analyse avanc\u00e9e (l'ETAA) pr\u00e9sente plusieurs avantages. Premi\u00e8rement, l'ETAA est une plate-forme d'analyse de donn\u00e9es open source qui donne acc\u00e8s \u00e0 une vari\u00e9t\u00e9 d'outils d'analyse avanc\u00e9s, notamment Python, R et SAS. Cela facilite le d\u00e9ploiement de mod\u00e8les d'apprentissage automatique et leur int\u00e9gration dans les flux de travail existants.</p> <p>Deuxi\u00e8mement, l'ETAA prend en charge plusieurs frameworks MLOps, notamment Couler, Seldon et Argo Workflows. Ces frameworks fournissent une gamme de fonctionnalit\u00e9s, notamment la gestion des versions de mod\u00e8le, le service de mod\u00e8le et la surveillance de mod\u00e8le, qui simplifient le processus de d\u00e9ploiement et de gestion des mod\u00e8les d'apprentissage automatique en production.</p> <p>Troisi\u00e8mement, l'ETAA fournit une plate-forme s\u00e9curis\u00e9e et \u00e9volutive pour servir les mod\u00e8les d'apprentissage automatique avec le statut prot\u00e9g\u00e9 B. Les mod\u00e8les peuvent \u00eatre servis \u00e0 l'aide d'environnements conteneuris\u00e9s, tels que Docker, qui offrent un niveau \u00e9lev\u00e9 d'isolement et de s\u00e9curit\u00e9. L'ETAA fournit \u00e9galement un acc\u00e8s aux ressources de cloud computing, permettant aux utilisateurs d'augmenter leur puissance de calcul selon les besoins pour g\u00e9rer des volumes \u00e9lev\u00e9s de demandes.</p> <p>Enfin, l'ETAA est une plateforme collaborative qui permet aux utilisateurs de partager du code et des donn\u00e9es avec d'autres chercheurs et analystes. Cela favorise une communaut\u00e9 d'utilisateurs qui peuvent apprendre du travail des autres et collaborer sur des projets qui n\u00e9cessitent des capacit\u00e9s d'analyse avanc\u00e9es.</p> <p>En r\u00e9sum\u00e9, servir des mod\u00e8les d'apprentissage automatique avec l'espace de travail d'analyse avanc\u00e9e donne acc\u00e8s \u00e0 des outils d'analyse avanc\u00e9s, \u00e0 plusieurs cadres MLOps, \u00e0 une plate-forme prot\u00e9g\u00e9 B s\u00e9curis\u00e9e et \u00e9volutive et \u00e0 une communaut\u00e9 collaborative d'utilisateurs, ce qui en fait une plate-forme id\u00e9ale pour les scientifiques et les analystes de donn\u00e9es qui veulent pour d\u00e9ployer et g\u00e9rer des mod\u00e8les d'apprentissage automatique en production.</p>"},{"location":"3-Pipelines/Machine-Learning-Model-Serving/#noyau-de-seldon","title":"Noyau de Seldon","text":"<p>Seldon Core est une plate-forme open source pour le d\u00e9ploiement, la mise \u00e0 l'\u00e9chelle et la surveillance des mod\u00e8les d'apprentissage automatique sur Kubernetes. Il fournit un moyen simple et efficace de d\u00e9ployer des mod\u00e8les d'apprentissage automatique en tant que microservices dans un environnement de production.</p> <p>Servir des mod\u00e8les d'apprentissage automatique \u00e0 l'aide de Seldon Core implique les \u00e9tapes suivantes :</p> <ol> <li> <p>Conditionnement du mod\u00e8le : la premi\u00e8re \u00e9tape consiste \u00e0 conditionner le mod\u00e8le d'apprentissage automatique form\u00e9 dans une image de conteneur avec toutes les d\u00e9pendances requises. Seldon Core prend en charge divers frameworks d'apprentissage automatique, notamment TensorFlow, PyTorch et Scikit-learn.</p> </li> <li> <p>D\u00e9ploiement du mod\u00e8le : une fois l'image du conteneur cr\u00e9\u00e9e, l'\u00e9tape suivante consiste \u00e0 d\u00e9ployer le mod\u00e8le sur Kubernetes \u00e0 l'aide de Seldon Core. Cela implique de d\u00e9finir le fichier de configuration de d\u00e9ploiement, qui sp\u00e9cifie les ressources requises pour le d\u00e9ploiement, telles que le nombre de r\u00e9pliques et les ressources de calcul.</p> </li> <li> <p>Service de mod\u00e8le : une fois le mod\u00e8le d\u00e9ploy\u00e9, Seldon Core expose un point de terminaison d'API REST qui peut \u00eatre utilis\u00e9 pour effectuer des pr\u00e9dictions. Les clients peuvent envoyer des requ\u00eates au point de terminaison avec des donn\u00e9es d'entr\u00e9e, et le mod\u00e8le renverra la sortie correspondante. Seldon Core prend \u00e9galement en charge divers mod\u00e8les de d\u00e9ploiement, tels que le d\u00e9ploiement Canary et les tests A/B, pour permettre une exp\u00e9rimentation et des tests faciles de diff\u00e9rents mod\u00e8les.</p> </li> <li> <p>Surveillance des mod\u00e8les : Seldon Core fournit diverses fonctionnalit\u00e9s de surveillance pour suivre les performances des mod\u00e8les d\u00e9ploy\u00e9s. Cela inclut la surveillance en temps r\u00e9el des m\u00e9triques du mod\u00e8le, telles que la latence et le d\u00e9bit, ainsi que la journalisation des donn\u00e9es de demande et de r\u00e9ponse \u00e0 des fins de d\u00e9bogage.</p> </li> </ol> <p>Seldon Core facilite la mise \u00e0 disposition de mod\u00e8les d'apprentissage automatique \u00e0 grande \u00e9chelle, avec une prise en charge de la haute disponibilit\u00e9, de l'\u00e9volutivit\u00e9 et de la tol\u00e9rance aux pannes. Il fournit \u00e9galement une int\u00e9gration avec divers outils natifs de Kubernetes, tels qu'Istio et Prometheus, pour permettre une surveillance et une observabilit\u00e9 avanc\u00e9es.</p>"},{"location":"3-Pipelines/Machine-Learning-Training-Pipelines/","title":"Former des mod\u00e8les d'apprentissage automatique sur l'ETAA","text":"<p>Info</p> <p>La formation de mod\u00e8les d'apprentissage automatique implique l'utilisation d'algorithmes pour apprendre des mod\u00e8les et des relations dans les donn\u00e9es. Ce processus implique l'identification de caract\u00e9ristiques ou de variables pertinentes pour le probl\u00e8me en question et l'utilisation de ces caract\u00e9ristiques pour faire des pr\u00e9dictions ou des classifications.</p>"},{"location":"3-Pipelines/Machine-Learning-Training-Pipelines/#pourquoi-sentrainer-avec-nous","title":"Pourquoi s'entra\u00eener avec nous ?","text":"<p>L'entra\u00eenement des mod\u00e8les d'apprentissage automatique sur l'espace de travail d'analyse avanc\u00e9e (ETAA) pr\u00e9sente plusieurs avantages.</p> <ol> <li> <p>Open Source : L'ETAA est une plate-forme de donn\u00e9es open source h\u00e9berg\u00e9e par Statistique Canada qui fournit un acc\u00e8s s\u00e9curis\u00e9 (prot\u00e9g\u00e9 B) \u00e0 une vari\u00e9t\u00e9 de sources de donn\u00e9es, y compris des donn\u00e9es de recensement, des enqu\u00eates et des dossiers administratifs. Ces donn\u00e9es peuvent \u00eatre utilis\u00e9es pour former des mod\u00e8les d'apprentissage automatique et g\u00e9n\u00e9rer des informations qui peuvent \u00e9clairer les d\u00e9cisions politiques et am\u00e9liorer les processus m\u00e9tier.</p> </li> <li> <p>Polyvalent : L'ETAA est con\u00e7u pour g\u00e9rer des ensembles de donn\u00e9es volumineux et complexes. Il donne acc\u00e8s \u00e0 une gamme d'outils d'analyse avanc\u00e9s, dans le langage de votre choix, y compris Python, R et SAS, qui peuvent \u00eatre utilis\u00e9s pour pr\u00e9traiter les donn\u00e9es, former des mod\u00e8les d'apprentissage automatique et g\u00e9n\u00e9rer des visualisations. Parce que l'ETAA exploite les technologies cloud, les utilisateurs peuvent augmenter leur puissance de calcul selon leurs besoins*. *</p> </li> <li>S\u00e9curis\u00e9 : L'ETAA est une plate-forme s\u00e9curis\u00e9e (prot\u00e9g\u00e9 B) qui respecte les normes les plus \u00e9lev\u00e9es en mati\u00e8re de confidentialit\u00e9 et de s\u00e9curit\u00e9 des donn\u00e9es. Les donn\u00e9es peuvent \u00eatre stock\u00e9es et trait\u00e9es sur la plateforme sans risque d'acc\u00e8s non autoris\u00e9 ou de violation de donn\u00e9es.</li> </ol>"},{"location":"3-Pipelines/Machine-Learning-Training-Pipelines/#mlops-et-pipelines-de-donnees","title":"MLOps et pipelines de donn\u00e9es","text":"<p>Optimiser les workflows de donn\u00e9es</p> <p>Les MLOps et les pipelines de donn\u00e9es sont des outils importants utilis\u00e9s dans le domaine de la science des donn\u00e9es pour g\u00e9rer et optimiser les workflows de donn\u00e9es.</p>"},{"location":"3-Pipelines/Machine-Learning-Training-Pipelines/#mlops","title":"MLOps","text":"<p>MLOps fait r\u00e9f\u00e9rence \u00e0 l'ensemble des pratiques et des outils utilis\u00e9s pour g\u00e9rer l'ensemble du cycle de vie d'un mod\u00e8le d'apprentissage automatique. Cela comprend tout, du d\u00e9veloppement et de la formation du mod\u00e8le \u00e0 son d\u00e9ploiement en production et \u00e0 sa maintenance dans le temps. MLOps garantit que les mod\u00e8les d'apprentissage automatique sont fiables, pr\u00e9cis et \u00e9volutifs, et qu'ils peuvent \u00eatre mis \u00e0 jour et am\u00e9lior\u00e9s selon les besoins.</p>"},{"location":"3-Pipelines/Machine-Learning-Training-Pipelines/#pipelines-de-donnees","title":"Pipelines de donn\u00e9es","text":"<p>Les pipelines de donn\u00e9es sont une s\u00e9rie d'\u00e9tapes qui permettent de d\u00e9placer des donn\u00e9es d'un syst\u00e8me ou d'une application \u00e0 un autre. Cela comprend la collecte, le nettoyage, la transformation et le stockage des donn\u00e9es, ainsi que leur r\u00e9cup\u00e9ration en cas de besoin. Les pipelines de donn\u00e9es sont importants pour garantir que les donn\u00e9es sont exactes, fiables et accessibles \u00e0 ceux qui en ont besoin.</p> <p>Automatisation et fiabilit\u00e9</p> <p>Les MLOps et les pipelines de donn\u00e9es aident les organisations \u00e0 g\u00e9rer le processus complexe consistant \u00e0 travailler avec de grandes quantit\u00e9s de donn\u00e9es et \u00e0 d\u00e9velopper des mod\u00e8les d'apprentissage automatique. En automatisant ces processus et en s'assurant que les donn\u00e9es sont exactes et fiables, les organisations peuvent \u00e9conomiser du temps et des ressources tout en prenant de meilleures d\u00e9cisions bas\u00e9es sur des informations bas\u00e9es sur les donn\u00e9es.</p>"},{"location":"3-Pipelines/Machine-Learning-Training-Pipelines/#pourquoi-des-mlops-conteneurises","title":"Pourquoi des MLOps conteneuris\u00e9s ?","text":"<p>Les avantages de l'utilisation d'une approche conteneuris\u00e9e pour la formation de mod\u00e8les d'apprentissage automatique avec Argo Workflows incluent :</p> <ol> <li> <p>Reproductibilit\u00e9 : La conteneurisation du mod\u00e8le de l'apprentissage automatique et de ses d\u00e9pendances garantit que l'environnement reste coh\u00e9rent d'une ex\u00e9cution \u00e0 l'autre, ce qui facilite la reproduction des r\u00e9sultats.</p> </li> <li> <p>\u00c9volutivit\u00e9 : Argo Workflows peut orchestrer des t\u00e2ches parall\u00e8les et des flux de travail complexes, ce qui facilite l'\u00e9volution du processus de formation selon les besoins.</p> </li> <li> <p>Portabilit\u00e9 : Les conteneurs peuvent \u00eatre ex\u00e9cut\u00e9s sur n'importe quelle plate-forme prenant en charge la conteneurisation, ce qui facilite le d\u00e9placement du processus de formation vers diff\u00e9rents environnements ou fournisseurs de cloud.</p> </li> <li> <p>Collaboration : En transf\u00e9rant le conteneur vers un registre de conteneurs, les autres utilisateurs peuvent facilement t\u00e9l\u00e9charger et utiliser le conteneur \u00e0 leurs propres fins, ce qui facilite la collaboration sur des projets de l'apprentissage automatique.</p> </li> </ol> <p>Les flux de travail Argo et la conteneurisation offrent une approche puissante et flexible pour la formation de mod\u00e8les d'apprentissage automatique. En tirant parti de ces outils, les data scientists et les ing\u00e9nieurs en apprentissage automatique peuvent cr\u00e9er, d\u00e9ployer et g\u00e9rer des workflows d'apprentissage automatique avec facilit\u00e9 et reproductibilit\u00e9.</p>"},{"location":"3-Pipelines/Machine-Learning-Training-Pipelines/#comment-former-des-modeles","title":"Comment former des mod\u00e8les","text":"<p>Il existe plusieurs fa\u00e7ons de former des mod\u00e8les d'apprentissage automatique et ce n'est pas \u00e0 nous de dire \u00e0 qui que ce soit comment le faire. Cela \u00e9tant dit, nous avons fourni ci-dessous quelques guides sur la fa\u00e7on de former des mod\u00e8les d'apprentissage automatique \u00e0 l'aide des outils disponibles sur l'ETAA. Le premier tutoriel concerne la formation d'un mod\u00e8le simple directement dans un notebook JupyterLab. Le deuxi\u00e8me tutoriel suppose que l'utilisateur est plus avanc\u00e9 et souhaite d\u00e9finir un pipeline MLOps pour former des mod\u00e8les \u00e0 l'aide d'Argo Workflows.</p>"},{"location":"3-Pipelines/Machine-Learning-Training-Pipelines/#creer-un-serveur-de-bloc-notes-sur-letaa","title":"Cr\u00e9er un serveur de bloc-notes sur l'ETAA","text":"<p>Serveurs portables</p> <p>Que vous envisagiez de travailler dans JupyterLab, R Studio ou quelque chose de plus avanc\u00e9 avec Argo Workflows, vous aurez besoin du serveur de bloc-notes appropri\u00e9. Suivez les instructions trouv\u00e9es ici pour commencer.</p>"},{"location":"3-Pipelines/Machine-Learning-Training-Pipelines/#utilisation-de-jupyterlab","title":"Utilisation de JupyterLab","text":"<p>JupyterLab est populaire</p> <p>La formation de mod\u00e8les d'apprentissage automatique avec JupyterLab est une approche populaire parmi les data scientists et les ing\u00e9nieurs en apprentissage automatique.</p> <p>Vous trouverez ici les \u00e9tapes n\u00e9cessaires pour former un mod\u00e8le d'apprentissage automatique avec JupyterLab sur l'ETAA. Parce que nous sommes un environnement multilingue, nous avons fait de notre mieux pour fournir des exemples de code dans nos langages les plus populaires, <code>Python</code>, <code>R</code> et <code>SAS</code>.</p>"},{"location":"3-Pipelines/Machine-Learning-Training-Pipelines/#1-importez-les-bibliotheques-requises","title":"1. Importez les biblioth\u00e8ques requises","text":"<p>Une fois qu'une session JupyterLab est en cours d'ex\u00e9cution, vous devez importer les biblioth\u00e8ques requises pour votre mod\u00e8le d'apprentissage automatique. Cela peut inclure des biblioth\u00e8ques telles que <code>NumPy</code>, <code>Pandas</code>, <code>Scikit-learn</code>, <code>Tensorflow</code> ou <code>PyTorch</code>. Si vous utilisez <code>R</code>, vous aurez besoin de <code>tidyverse</code>, <code>caret</code> et <code>janitor</code>.</p> PythonRSASPySAS <p>``` py title=\"libraries.py\" linenums=\"1\"</p> libraries.R<pre><code>#!/usr/bin/env Rscript\n# tidyverse pour des outils impressionnants d'analyse de donn\u00e9es et de munging\nlibrary(tidyverse)\n# concierge pour nettoyer vos donn\u00e9es\nlibrary(janitor)\n# caret pour un apprentissage automatique facile\nlibrary(caret)\n</code></pre> libraries.py<pre><code>#!/usr/bin/env python\n# la seule biblioth\u00e8que dont vous aurez besoin pour acc\u00e9der \u00e0 SAS depuis Python\nimport saspy\n</code></pre> libraries.sas<pre><code>\n</code></pre> <p>\u00c0 propos du code</p> <p>Les exemples de code que vous voyez dans ce document et tout au long de la documentation sont \u00e0 titre indicatif pour vous aider \u00e0 d\u00e9marrer vos projets. Selon la t\u00e2che ou le projet sp\u00e9cifique, d'autres biblioth\u00e8ques et \u00e9tapes peuvent \u00eatre n\u00e9cessaires.</p>"},{"location":"3-Pipelines/Machine-Learning-Training-Pipelines/#usrbinenv-python","title":"!/usr/bin/env python","text":""},{"location":"3-Pipelines/Machine-Learning-Training-Pipelines/#tensorflow-et-keras-pour-la-construction-et-la-formation-de-modeles-dapprentissage-en-profondeur","title":"tensorflow et keras pour la construction et la formation de mod\u00e8les d'apprentissage en profondeur","text":"<p>import tensorflow as tf from tensorflow import keras</p>"},{"location":"3-Pipelines/Machine-Learning-Training-Pipelines/#numpy-pour-les-calculs-numeriques","title":"numpy pour les calculs num\u00e9riques","text":"<p>import numpy as np</p>"},{"location":"3-Pipelines/Machine-Learning-Training-Pipelines/#pandas-pour-la-manipulation-et-lanalyse-des-donnees","title":"pandas pour la manipulation et l'analyse des donn\u00e9es","text":"<p>import pandas as pd</p>"},{"location":"3-Pipelines/Machine-Learning-Training-Pipelines/#matplotlib-pour-la-visualisation-des-donnees","title":"matplotlib pour la visualisation des donn\u00e9es","text":"<p>import matplotlib.pyplot as plt  ```</p>"},{"location":"3-Pipelines/Machine-Learning-Training-Pipelines/#2-charger-et-pretraiter-les-donnees","title":"2. Charger et pr\u00e9traiter les donn\u00e9es","text":"<p>Ensuite, vous devez charger et pr\u00e9traiter les donn\u00e9es que vous utiliserez pour entra\u00eener votre mod\u00e8le d'apprentissage automatique. Cela peut inclure le nettoyage des donn\u00e9es, l'extraction de caract\u00e9ristiques et la normalisation. Les \u00e9tapes de pr\u00e9traitement exactes que vous devrez effectuer d\u00e9pendront de l'ensemble de donn\u00e9es sp\u00e9cifique avec lequel vous travaillez, des exigences de votre mod\u00e8le d'apprentissage automatique et du travail \u00e0 effectuer.</p> PythonRSASPySAS load_data.py<pre><code>#!/usr/bin/env python\n# Importer les paquets n\u00e9cessaires\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n# Charger des donn\u00e9es \u00e0 partir d'un fichier CSV\ndata = pd.read_csv('data.csv')\n# Nettoyage des donn\u00e9es ! Beaucoup plus peut \u00eatre fait, c'est un principe de base\ndata = data.dropna()  # Drop rows with missing values\ndata = data.drop_duplicates()  # Drop duplicate rows\n# Extraction de caract\u00e9ristiques\nX = data[['feature1', 'feature2', 'feature3']] # S\u00e9lectionnez les fonctionnalit\u00e9s pertinentes\n# Normalisation\nscaler = StandardScaler() # Cr\u00e9e un objet scaler\nX_norm = scaler.fit_transform(X) # Normaliser les valeurs des caract\u00e9ristiques\n</code></pre> <p>``` r title=\"load_data.R\" linenums=\"1\"</p> load_data.py<pre><code>#!/usr/bin/env python\n# Importer les paquets n\u00e9cessaires\nimport saspy\n# D\u00e9marrez une session SAS et v\u00e9rifiez les informations de configuration\nsas = saspy.SASsession(cfgname='default')\n# Charger des donn\u00e9es \u00e0 partir d'un fichier CSV\ndata = sas.read_csv(\"./data.csv\")\n</code></pre> load_data.sas<pre><code>/* Lecture d'un fichier d\u00e9limit\u00e9 par des virgules avec une extension .csv */\n/* */\n/* Puisque la valeur DBMS= est CSV, vous n'avez pas besoin d'utiliser le DELIMITER= */\n/* d\u00e9claration.Par d\u00e9faut, il est suppos\u00e9 que les noms de variables sont sur la premi\u00e8re */\n/* ligne, l'instruction GETNAMES= n'est donc pas requise. */\n/* */\n/* Cr\u00e9er un fichier de test d\u00e9limit\u00e9 par des virgules \u00e0 lire en utilisant PROC IMPORT ci-dessous. */\n/* Charger les donn\u00e9es d'un fichier CSV */\nproc import\n    datafile='data.csv'\nout=data\n    dbms=csv\n    replace; \nrun;\n/* Afficher les donn\u00e9es */\nproc print ;\ncourir;\n</code></pre>"},{"location":"3-Pipelines/Machine-Learning-Training-Pipelines/#usrbinenv-rscript","title":"!/usr/bin/env Rscript","text":""},{"location":"3-Pipelines/Machine-Learning-Training-Pipelines/#importer-les-paquets-necessaires","title":"Importer les paquets n\u00e9cessaires","text":"<p>library(tidyverse) library(janitor)</p>"},{"location":"3-Pipelines/Machine-Learning-Training-Pipelines/#charger-des-donnees-a-partir-dun-fichier-csv","title":"Charger des donn\u00e9es \u00e0 partir d'un fichier CSV","text":"<p>data &lt;- read_csv(\"data.csv\")</p>"},{"location":"3-Pipelines/Machine-Learning-Training-Pipelines/#nettoyer-les-donnees-a-laide-dun-concierge","title":"Nettoyer les donn\u00e9es \u00e0 l'aide d'un concierge","text":"<p>data_cleaned &lt;- data %&gt;%</p>"},{"location":"3-Pipelines/Machine-Learning-Training-Pipelines/#supprimer-les-espaces-blancs-de-debutfin-dans-les-noms-de-colonne","title":"Supprimer les espaces blancs de d\u00e9but/fin dans les noms de colonne","text":"<p>clean_names() %&gt;%</p>"},{"location":"3-Pipelines/Machine-Learning-Training-Pipelines/#supprimer-les-lignes-avec-des-valeurs-manquantes","title":"Supprimer les lignes avec des valeurs manquantes","text":"<p>remove_empty() %&gt;%</p>"},{"location":"3-Pipelines/Machine-Learning-Training-Pipelines/#convertir-la-colonne-de-date-au-format-date","title":"Convertir la colonne de date au format Date","text":"<p>mutate(date = as.Date(date, format = \"%m/%d/%Y\")) %&gt;%</p>"},{"location":"3-Pipelines/Machine-Learning-Training-Pipelines/#supprimer-les-lignes-en-double","title":"Supprimer les lignes en double","text":"<p>distinct() %&gt;%</p>"},{"location":"3-Pipelines/Machine-Learning-Training-Pipelines/#reorganiser-les-colonnes","title":"R\u00e9organiser les colonnes","text":"<p>select(date, column2, column1, column3)  ```</p>"},{"location":"3-Pipelines/Machine-Learning-Training-Pipelines/#3-divisez-les-donnees-en-ensembles-dentrainement-et-de-test","title":"3. Divisez les donn\u00e9es en ensembles d'entra\u00eenement et de test","text":"<p>Une fois les donn\u00e9es pr\u00e9trait\u00e9es, vous devez les diviser en ensembles d'apprentissage et de test. L'ensemble de formation sera utilis\u00e9 pour former le mod\u00e8le d'apprentissage automatique, tandis que l'ensemble de test sera utilis\u00e9 pour \u00e9valuer ses performances.</p> PythonRSASPySAS train_test.py<pre><code>#!/usr/bin/env python\n# Importer les paquets n\u00e9cessaires\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n# Diviser les donn\u00e9es en ensembles d'entra\u00eenement et de test\nX_train, X_test, y_train, y_test = train_test_split(X_norm,\ndata['target'], test_size=0.2, random_state=42)\n</code></pre> train_test.R<pre><code>#!/usr/bin/env Rscript\n# Importer les paquets n\u00e9cessaires\nlibrary(caret)\n# Charger le jeu de donn\u00e9es\ndata &lt;- read.csv(\"my-dataset.csv\")\n# D\u00e9finir la graine pour la reproductibilit\u00e9\nset.seed(123)\n# Diviser l'ensemble de donn\u00e9es en train et tester \u00e0 l'aide de la fonction createDataPartition de caret\ntrain_index &lt;- createDataPartition(data$target_variable, p = 0.7, list = FALSE)\ntrain_data &lt;- data[train_index, ]\ntest_data &lt;- data[-train_index, ]\n</code></pre> train_test.py<pre><code>#!/usr/bin/env python\n</code></pre> train_test.sas<pre><code>\n</code></pre> <p>Note</p> <p>Nous divisons les donn\u00e9es en ensembles d'apprentissage et de test \u00e0 l'aide de la fonction \"train_test_split\" de \"scikit-learn\", qui divise de mani\u00e8re al\u00e9atoire les donn\u00e9es en deux ensembles en fonction de la taille de test sp\u00e9cifi\u00e9e et de la graine al\u00e9atoire.</p>"},{"location":"3-Pipelines/Machine-Learning-Training-Pipelines/#4-definir-et-entrainer-le-modele-dapprentissage-automatique","title":"4. D\u00e9finir et entra\u00eener le mod\u00e8le d'apprentissage automatique","text":"<p>Avec la division des donn\u00e9es, vous pouvez d\u00e9sormais d\u00e9finir et entra\u00eener votre mod\u00e8le d'apprentissage automatique \u00e0 l'aide de l'ensemble d'entra\u00eenement. Cela pourrait impliquer la s\u00e9lection de l'algorithme appropri\u00e9, le r\u00e9glage des hyperparam\u00e8tres et la validation crois\u00e9e.</p> PythonRSASPySAS train.py<pre><code>#!/usr/bin/env python\n# Importer les paquets n\u00e9cessaires\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\n# Charger les ensembles de donn\u00e9es\ndata = pd.read_csv(\"my-dataset.csv\")\n# Diviser l'ensemble de donn\u00e9es en train et tester\nX_train, X_test, y_train, y_test = train_test_split(data.iloc[:, :-1], data.iloc[:, -1], test_size=0.3, random_state=123)\n# Former le mod\u00e8le\nmodel = RandomForestClassifier(n_estimators=100, random_state=123)\nmodel.fit(X_train, y_train)\n# Imprimer le score de pr\u00e9cision sur les donn\u00e9es de test\nprint(\"Accuracy on test set: {:.3f}\".format(model.score(X_test, y_test)))\n</code></pre> train.R<pre><code>#!/usr/bin/env Rscript\n# Importer les paquets n\u00e9cessaires\nlibrary(caret)\n# Charger l'ensembles de donn\u00e9es\ndata &lt;- read.csv(\"my-dataset.csv\")\n# D\u00e9finir la graine pour la reproductibilit\u00e9\nset.seed(123)\n# Diviser l'ensemble de donn\u00e9es en train et tester \u00e0 l'aide de la fonction createDataPartition de caret\ntrain_index &lt;- createDataPartition(data$target_variable, p = 0.7, list = FALSE)\ntrain_data &lt;- data[train_index, ]\ntest_data &lt;- data[-train_index, ]\n# D\u00e9finir le contr\u00f4le d'entra\u00eenement\ntrain_control &lt;- trainControl(method = \"cv\", number = 5)\n# Entra\u00eenez le mod\u00e8le \u00e0 l'aide de la fonction d'entra\u00eenement de caret, (method = \"rf\" est pour la for\u00eat al\u00e9atoire)\nmodel &lt;- train(target_variable ~ ., data = train_data, method = \"rf\", trControl = train_control)\n# Imprimez l'objet mod\u00e8le pour afficher les r\u00e9sultats\nprint(model)\n</code></pre> train.py<pre><code>#!/usr/bin/env python\n# Importer les paquets n\u00e9cessaires\nimport saspy\nimport pandas as pd\n# \u00c9tablir une connexion \u00e0 une session SAS\nsas = saspy.SASsession()\n# Charger l'ensembles de donn\u00e9es\ndata = pd.read_csv(\"my-dataset.csv\")\n# T\u00e9l\u00e9charger l'ensemble de donn\u00e9es sur SAS\nsas_df = sas.df2sd(data, \"mydata\")\n# Diviser l'ensemble de donn\u00e9es en train et tester\ntrain, test = sas.surveyselect(data=sas_df,\nmethod=\"SRS\",\nseed=123,\nsamprate=0.7,\noutall=True,\nstrata=\"target_variable\",\npartind=True)\n# Former le mod\u00e8le en utilisant la proc\u00e9dure HPFOREST\nmodel = sas.hpforest(data=train,\ntarget=\"target_variable\",\ninput=\"input_variable_1-input_variable_n\",\npartition=\"rolevar\",\nrolevars={\"test\": \"0\", \"train\": \"1\"},\nnominals=[\"input_variable_1-input_variable_n\"],\nforestopts={\"ntree\": 100, \"seed\": 123})\n# Noter le mod\u00e8le sur les donn\u00e9es de test\npredictions = model.predict(newdata=test, out=pred_out)\n# Calculer le score de pr\u00e9cision sur les donn\u00e9es de test\naccuracy = sas.freq(data=predictions, tables=\"target_variable*p_target_variable\", nocum=True, nocol=True)\n# Imprimer le score de pr\u00e9cision\nprint(\"Accuracy on test set: {:.3f}\".format(accuracy.Frequency.iloc[0, 1] / accuracy.Frequency.iloc[:, 1].sum()))\n# Se d\u00e9connecter de la session SAS\nsas.disconnect()\n</code></pre> train.sas<pre><code>/* Charger le jeu de donn\u00e9es */\nproc import datafile=\"my-dataset.csv\" out=mydata dbms=csv replace;\nrun;\n/* Divise le jeu de donn\u00e9es en train et test */\nproc surveyselect data=mydata method=srs seed=123 out=selected outall\nsamprate=0.7;\nstrata target_variable;\nrun;\n/* Former le mod\u00e8le */\nproc hpforest data=selected;\nclass _all_;\ntarget target_variable / level=nominal;\npartition rolevar=target_variable(test=\"0\" train=\"1\");\ninput _all_;\nforest ntree=100 seed=123;\nrun;\n/* Noter le mod\u00e8le sur les donn\u00e9es de test */\nproc hpforest predict testdata=selected out=testout;\nrun;\n/* Affiche le score de pr\u00e9cision sur les donn\u00e9es de test */\nproc freq data=testout;\ntables target_variable*p_target_variable / nocum nocol;\nrun;\n</code></pre>"},{"location":"3-Pipelines/Machine-Learning-Training-Pipelines/#5-evaluer-le-modele","title":"5. \u00c9valuer le mod\u00e8le","text":"<p>Apr\u00e8s avoir entra\u00een\u00e9 le mod\u00e8le, vous devez \u00e9valuer ses performances sur l'ensemble de test. Cela vous donnera une id\u00e9e de la performance du mod\u00e8le sur de nouvelles donn\u00e9es in\u00e9dites.</p> PythonRSASPySAS evaluate.py evaluate.R evaluate.py evaluate.sas"},{"location":"3-Pipelines/Machine-Learning-Training-Pipelines/#6-deployer-le-modele","title":"6. D\u00e9ployer le mod\u00e8le","text":"<p>Enfin, vous pouvez d\u00e9ployer le mod\u00e8le d'apprentissage automatique form\u00e9 dans un environnement de production.</p> PythonRSASPySAS deploy.py deploy.R deploy.py deploy.sas"},{"location":"3-Pipelines/Machine-Learning-Training-Pipelines/#utilisation-des-workflows-argo","title":"Utilisation des workflows ArgoFlux de travail Argo","text":"<p>Bonnes pratiques MLOps</p> <p>Argo Workflows est un excellent outil pour tous ceux qui cherchent \u00e0 mettre en \u0153uvre des pratiques MLOps et \u00e0 rationaliser le processus de formation et de d\u00e9ploiement de mod\u00e8les d'apprentissage automatique ou d'autres t\u00e2ches de science des donn\u00e9es telles que ETL.</p> <p>Argo Workflows est un engin de workflow open source natif de conteneur pour orchestrer des t\u00e2ches parall\u00e8les sur Kubernetes. Argo Workflows est impl\u00e9ment\u00e9 en tant que Kubernetes CRD (Custom Resource Definition). Il est particuli\u00e8rement bien adapt\u00e9 pour une utilisation dans les flux de travail d'apprentissage automatique et de science des donn\u00e9es.</p> <p>Argo Workflows vous permet de</p> <ul> <li>D\u00e9finissez des flux de travail o\u00f9 chaque \u00e9tape du flux de travail est un conteneur.</li> <li>Mod\u00e9lisez les flux de travail en plusieurs \u00e9tapes sous la forme d'une s\u00e9quence de t\u00e2ches ou capturez les d\u00e9pendances entre les t\u00e2ches \u00e0 l'aide d'un graphe acyclique dirig\u00e9 (DAG).</li> <li>Ex\u00e9cutez facilement des t\u00e2ches de calcul intensives pour l'apprentissage automatique ou le traitement de donn\u00e9es en une fraction du temps \u00e0 l'aide des flux de travail Argo sur Kubernetes.</li> <li>Ex\u00e9cutez des pipelines CI/CD en mode natif sur Kubernetes sans configurer de produits de d\u00e9veloppement logiciel complexes.</li> </ul> <p>ce qui facilite la gestion de l'ensemble du pipeline d'apprentissage automatique de bout en bout. Avec Argo Workflows, vous pouvez facilement cr\u00e9er des workflows qui int\u00e8grent des t\u00e2ches telles que le pr\u00e9traitement des donn\u00e9es, la formation de mod\u00e8les et le d\u00e9ploiement de mod\u00e8les, le tout dans un environnement Kubernetes.</p> <p> </p> <p>Vous trouverez ci-dessous les \u00e9tapes pour former un mod\u00e8le d'apprentissage automatique \u00e0 l'aide d'Argo Workflows sur l'ETAA.</p>"},{"location":"3-Pipelines/Machine-Learning-Training-Pipelines/#1-ecrivez-un-script-pour-entrainer-votre-modele","title":"1. \u00c9crivez un script pour entra\u00eener votre mod\u00e8le","text":"<p>Voici un exemple de script qui entra\u00eene un mod\u00e8le de r\u00e9gression logistique sur l'ensemble de donn\u00e9es iris. N'oubliez pas de consulter le code de chaque langue ci-dessous.</p> PythonR train.py<pre><code>#!/usr/bin/env python\n# Importer les biblioth\u00e8ques n\u00e9cessaires\nimport argparse\nimport pandas as pd\nfrom sklearn.datasets import load_iris\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nimport joblib\n# Analyser les arguments d'entr\u00e9e\nparser = argparse.ArgumentParser(description=\"Train logistic regression model on iris dataset.\")\nparser.add_argument(\"--input\", default=\"iris.csv\", help=\"Path to input dataset file.\")\nparser.add_argument(\"--output\", default=\"model.pkl\", help=\"Path to output model file.\")\nargs = parser.parse_args()\n# Charger le jeu de donn\u00e9es de l'iris\ndata = load_iris()\nX, y = data.data, data.target\n# Diviser les donn\u00e9es en ensembles d'entra\u00eenement et de test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n# Former le mod\u00e8le de r\u00e9gression logistique\nclf = LogisticRegression(random_state=42)\nclf.fit(X_train, y_train)\n# \u00c9valuer le mod\u00e8le sur l'ensemble de test\ny_pred = clf.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy:\", accuracy)\n# Enregistrer le mod\u00e8le dans un fichier\njoblib.dump(clf, args.output)\n</code></pre> train.R<pre><code>#!/usr/bin/env Rscript\n# Importer les biblioth\u00e8ques n\u00e9cessaires\nlibrary(caret)\n# Analyser les arguments d'entr\u00e9e\nargs &lt;- commandArgs(trailingOnly = TRUE)\ninput_file &lt;- ifelse(length(args) &gt; 0, args[1], \"iris.csv\")\noutput_file &lt;- ifelse(length(args) &gt; 1, args[2], \"model.rds\")\n# Charger le jeu de donn\u00e9es de l'iris\ndata(iris)\nX &lt;- iris[, 1:4]\ny &lt;- iris[, 5]\n# Diviser les donn\u00e9es en ensembles d'entra\u00eenement et de test\nset.seed(42)\ntrain_index &lt;- createDataPartition(y, p = 0.8, list = FALSE)\nX_train &lt;- X[train_index, ]\ny_train &lt;- y[train_index]\nX_test &lt;- X[-train_index, ]\ny_test &lt;- y[-train_index]\n# Former le mod\u00e8le de r\u00e9gression logistique\nclf &lt;- train(x = X_train, y = y_train, method = \"glm\")\n# \u00c9valuer le mod\u00e8le sur l'ensemble de test\ny_pred &lt;- predict(clf, newdata = X_test)\naccuracy &lt;- confusionMatrix(y_pred, y_test)$overall[\"Accuracy\"]\nprint(paste0(\"Accuracy: \", accuracy))\n# Enregistrer le mod\u00e8le dans un fichier\nsaveRDS(clf, output_file)\n</code></pre>"},{"location":"3-Pipelines/Machine-Learning-Training-Pipelines/#2-ecrivez-un-dockerfile-pour-executer-votre-code","title":"2. \u00c9crivez un Dockerfile pour ex\u00e9cuter votre code","text":"<p>Vous aurez besoin d'un Dockerfile qui inclut toutes les d\u00e9pendances n\u00e9cessaires pour former votre mod\u00e8le d'apprentissage automatique. Cela pourrait inclure</p> <ul> <li>des forfaits comme<ul> <li><code>scikit-learn</code>, <code>pandas</code> ou <code>numpy</code> si vous utilisez <code>Python</code></li> <li><code>caret</code>, <code>janitor</code> et <code>tidyverse</code> si vous utilisez <code>R</code></li> </ul> </li> <li>vos propres biblioth\u00e8ques ou scripts personnalis\u00e9s</li> <li>le code de votre mod\u00e8le de l'apprentissage automatique sous la forme d'un script comme dans l'exemple ci-dessus.</li> </ul> <p>Utilisez le <code>Dockerfile</code> suivant comme point de d\u00e9part pour vos projets <code>R</code> et <code>Python</code>.</p> PythonR Dockerfile<pre><code>FROM python:3.8-slim-buster\n# Installez toutes les d\u00e9pendances n\u00e9cessaires\nRUN pip install --no-cache-dir scikit-learn pandas numpy\n\n# D\u00e9finir le r\u00e9pertoire de travail\nWORKDIR /app\n# Copier le code dans le conteneur\nCOPY train.py .\n\n# D\u00e9finir le point d'entr\u00e9e\nENTRYPOINT [\"python\", \"train.py\"]\n</code></pre> Dockerfile<pre><code>FROM rocker/r-base:latest\nRUN apt-get update &amp;&amp; apt-get install -y --no-install-recommends \\\nlibssl-dev \\\nlibcurl4-openssl-dev \\\nlibxml2-dev \\\n&amp;&amp; apt-get clean \\\n&amp;&amp; rm -rf /var/lib/apt/lists/*\n\nRUN R -e 'install.packages(c(\"caret\", \"janitor\", \"tidyverse\"))'\nCOPY train.R /app/train.R\n\nWORKDIR /app\nENTRYPOINT [\"Rscript\", \"train.R\"]\n</code></pre>"},{"location":"3-Pipelines/Machine-Learning-Training-Pipelines/#3-ecrivez-votre-workflow-en-yaml","title":"3. \u00c9crivez votre workflow en YAML","text":"<p>YAML est encore un autre langage de balisage et vous devrez \u00e9crire les \u00e9tapes de votre pipeline de formation dans un fichier YAML Argo Workflows. Ce fichier doit inclure une r\u00e9f\u00e9rence au Dockerfile que vous avez cr\u00e9\u00e9 \u00e0 l'\u00c9tape 1, ainsi que toutes les donn\u00e9es d'entr\u00e9e et de sortie avec lesquelles vous travaillerez.</p> <p>Voici un exemple de fichier YAML pour un pipeline d'apprentissage automatique simple qui forme un mod\u00e8le de r\u00e9gression logistique sur l'ensemble de donn\u00e9es iris. La seule vraie diff\u00e9rence entre les versions <code>Python</code> et <code>R</code> est la commande <code>command : [\"python\", \"train.py\"]</code> vs <code>command : [\"Rscript\", \"train.R\"]</code> et la les mod\u00e8les sont stock\u00e9s dans diff\u00e9rents formats, <code>pkl</code> pour <code>python</code> et <code>rds</code> pour <code>R</code>.</p> <p>Le fichier YAML d\u00e9finit une seule \u00e9tape appel\u00e9e \"train\" qui ex\u00e9cute un script appel\u00e9 \"train.py\" ou \"train.R\" dans l'image Docker \"machine-learning:v1\". Le script prend un fichier d'ensemble de donn\u00e9es d'entr\u00e9e, sp\u00e9cifi\u00e9 par un param\u00e8tre appel\u00e9 <code>dataset</code>, et g\u00e9n\u00e8re un fichier de mod\u00e8le entra\u00een\u00e9 vers un artefact de sortie appel\u00e9 <code>model.pkl</code> ou <code>model.rds</code> selon le langage utilis\u00e9.</p> PythonR workflow.yaml<pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\ngenerateName: ml-pipeline-\nspec:\nentrypoint: train\ntemplates:\n- name: train\ncontainer:\nimage: machine-learning:v1\ncommand: [\"python\", \"train.py\"]\nargs: [\"--input\", \"{{inputs.parameters.dataset}}\", \"--output\", \"{{outputs.artifacts.model}}\"]\ninputs:\nparameters:\n- name: dataset\ndefault: \"iris.csv\"\noutputs:\nartifacts:\n- name: model\npath: /output/model.pkl\n</code></pre> workflow.yaml<pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\ngenerateName: ml-pipeline-\nspec:\nentrypoint: train\ntemplates:\n- name: train\ncontainer:\nimage: machine-learning:v1\ncommand: [\"Rscript\", \"train.R\"]\nargs: [\"--input\", \"{{inputs.parameters.dataset}}\", \"--output\", \"{{outputs.artifacts.model}}\"]\ninputs:\nparameters:\n- name: dataset\ndefault: \"iris.csv\"\noutputs:\nartifacts:\n- name: model\npath: /output/model.rds\n</code></pre>"},{"location":"3-Pipelines/Machine-Learning-Training-Pipelines/#4-soumettez-le-workflow-a-laide-de-la-cli-argo-workflows","title":"4. Soumettez le workflow \u00e0 l'aide de la CLI Argo Workflows","text":"<p>Pour ex\u00e9cuter le flux de travail ci-dessus, vous devez d'abord envoyer le fichier Dockerfile \u00e0 notre registre de conteneurs, puis envoyer le fichier YAML \u00e0 l'aide de la commande \"argo submit\". Une fois le pipeline termin\u00e9, vous pouvez r\u00e9cup\u00e9rer le fichier de mod\u00e8le form\u00e9 en t\u00e9l\u00e9chargeant l'artefact de sortie \u00e0 partir de la commande <code>argo logs</code>.</p> Terminal<pre><code>$ argo submit workflow.yaml       # soumettre une sp\u00e9cification de workflow \u00e0 Kubernetes\n</code></pre>"},{"location":"3-Pipelines/Machine-Learning-Training-Pipelines/#5-surveiller-le-pipeline-a-laide-de-la-cli-argo-workflows","title":"5. Surveiller le pipeline \u00e0 l'aide de la CLI Argo Workflows","text":"<p>Au fur et \u00e0 mesure que le pipeline s'ex\u00e9cute, vous pouvez surveiller sa progression \u00e0 l'aide de la CLI Argo Workflows. Cela vous montrera quelles \u00e9tapes ont r\u00e9ussi et lesquelles sont toujours en cours. Vous trouverez ci-dessous quelques commandes utiles. Pour plus d'informations sur la CLI Argo Workflows, veuillez consulter la documentation officielle de la CLI Argo Workflows .</p> Terminal<pre><code>$ argo list                       # liste les workflows actuels\n$ argo get workflow-xxx           # obtenir des informations sur un workflow sp\u00e9cifique\n$ argo logs workflow-xxx          # imprimer les logs d'un workflow\n$ argo delete workflow-xxx        # supprimer le workflow\n</code></pre>"},{"location":"3-Pipelines/Machine-Learning-Training-Pipelines/#6-recuperer-le-modele-forme","title":"6. R\u00e9cup\u00e9rer le mod\u00e8le form\u00e9","text":"<p>Une fois le pipeline termin\u00e9, vous pouvez r\u00e9cup\u00e9rer les donn\u00e9es de sortie \u00e0 l'aide de la commande argo logs ou en affichant les artefacts de sortie \u00e0 l'aide de la CLI, c'est-\u00e0-dire acc\u00e9der au r\u00e9pertoire que vous avez sp\u00e9cifi\u00e9 dans votre script et localiser le fichier <code>model.pkl</code> ou <code>model. rds</code>. L'extrait de code suivant, tir\u00e9 du script de formation ci-dessus, indique au langage de programmation respectif o\u00f9 enregistrer le mod\u00e8le form\u00e9.</p> PythonR Enregistrement des donn\u00e9es de sortie<pre><code>#!/usr/bin/env python\nparser.add_argument(\"--output\", default=\"model.pkl\", help=\"Path to output model file.\")\n# Enregistrer le mod\u00e8le dans un fichier\njoblib.dump(clf, args.sortie)\n</code></pre> Enregistrement des donn\u00e9es de sortie<pre><code>#!/usr/bin/env Rscript\noutput_file &lt;- ifelse(length(args) &gt; 1, args[2], \"model.rds\")\n# Enregistrer le mod\u00e8le dans un fichier\nsaveRDS(clf, output_file)\n</code></pre>"},{"location":"3-Pipelines/Machine-Learning-Training-Pipelines/#exemples-dutilisation-de-sdk-bases-sur-argo-workflows","title":"Exemples d'utilisation de SDK bas\u00e9s sur Argo Workflows","text":"<p>Si vous pr\u00e9f\u00e9rez utiliser un cadre de niveau sup\u00e9rieur, nous avons <code>Couler</code> et <code>Hera</code>. Ces cadres rendent la cr\u00e9ation et la gestion de flux de travail complexes plus accessibles \u00e0 un public plus large.</p>"},{"location":"3-Pipelines/Machine-Learning-Training-Pipelines/#hera","title":"Hera","text":"<p>Hera vise \u00e0 simplifier le processus de cr\u00e9ation et de soumission des flux de travail en \u00e9liminant de nombreux d\u00e9tails techniques via une interface de programmation d'application simple. Il utilise \u00e9galement un ensemble coh\u00e9rent de terminologie et de concepts qui s'alignent sur Argo Workflows, ce qui permet aux utilisateurs d'apprendre et d'utiliser plus facilement les deux outils ensemble.</p>"},{"location":"3-Pipelines/Machine-Learning-Training-Pipelines/#couler","title":"Couler","text":"<p>Couler fournit une interface de programmation d'application simple et unifi\u00e9e pour d\u00e9finir les flux de travail \u00e0 l'aide d'un style de programmation imp\u00e9ratif. Il construit \u00e9galement automatiquement des graphes acycliques dirig\u00e9s (DAG) pour les flux de travail, ce qui peut aider \u00e0 simplifier le processus de cr\u00e9ation et de gestion de ceux-ci.</p> CoulerHeraYAMLSeldon ? couler.py<pre><code>#!/usr/bin/env python\n# Pr\u00e9parez votre syst\u00e8me\n!pip config --user set global.index-url https://jfrog.ETAA.cloud.statcan.ca/artifactory/api/pypi/pypi-remote/simple\n!python3 -m pip install git+https://github.com/couler-proj/couler --ignore-installed\n# D\u00e9finir la variable globale pour plus de commodit\u00e9\nNAMESPACE = \"&lt;your-namespace&gt;\"\n# Importer les paquets n\u00e9cessaires\nimport json\nimport random\nimport couler.argo as couler\nfrom couler.argo_submitter import ArgoSubmitter\n# D\u00e9finir les \u00e9tapes (fonctions) utilis\u00e9es dans le workflow\ndef random_code():\nimport random\nres = \"heads\" if random.randint(0, 1) == 0 else \"tails\"\nprint(res)\ndef flip_coin():\nreturn couler.run_script(\nimage=\"k8scc01covidacr.azurecr.io/ubuntu\",\nsource=random_code\n)\ndef heads():\nreturn couler.run_container(\nimage=\"k8scc01covidacr.azurecr.io/ubuntu\",\ncommand=[\"sh\", \"-c\", 'echo \"it was heads\"']\n)\ndef tails():\nreturn couler.run_container(\nimage=\"k8scc01covidacr.azurecr.io/ubuntu\",\ncommand=[\"sh\", \"-c\", 'echo \"it was tails\"']\n)\nresult = flip_coin()\ncouler.when(couler.equal(result, \"heads\"), lambda: heads())\ncouler.when(couler.equal(result, \"tails\"), lambda: tails())\nsubmitter = ArgoSubmitter(namespace=\"NAMESPACE\")\nresult = couler.run(submitter=submitter)\nprint(json.dumps(result, indent=2))\n</code></pre> hera.py<pre><code>#!/usr/bin/env python\n# Pr\u00e9parez votre syst\u00e8me\n!pip config --user set global.index-url https://jfrog.ETAA.cloud.statcan.ca/artifactory/api/pypi/pypi-remote/simple\n!pip install hera-workflows\n# Importer les paquets n\u00e9cessaires\nimport hera\nfrom hera import Task, Workflow\n# Configurer Hera\nhera.global_config.GlobalConfig.token = \"&lt;your-token&gt;\"\nhera.global_config.GlobalConfig.host = \"https://argo-workflows.ETAA-dev.cloud.statcan.ca:443\"\nhera.global_config.GlobalConfig.namespace = \"&lt;your-namespace&gt;\"\nhera.global_config.GlobalConfig.service_account_name = \"&lt;your-account-name&gt;\"\n# D\u00e9finir les \u00e9tapes (fonctions) utilis\u00e9es dans le workflow\ndef random_code():\nres = \"heads\" if random.randint(0, 1) == 0 else \"tails\"\nprint(res)\ndef heads():\nprint(\"it was heads\")\ndef tails():\nprint(\"it was tails\")\n# D\u00e9finir le flux de travail\nwith Workflow(\"coin-flip\") as w:\nr = Task(\"r\", random_code)\nh = Task(\"h\", heads)\nt = Task(\"t\", tails)\nh.on_other_result(r, \"heads\")\nt.on_other_result(r, \"tails\")\n# Ex\u00e9cuter le flux de travail\nw.create()\n</code></pre> workflow.yaml<pre><code>\n</code></pre> <pre><code>\n</code></pre>"},{"location":"3-Pipelines/Machine-Learning-Training-Pipelines/#ressources-supplementaires-pour-les-workflows-argo","title":"Ressources suppl\u00e9mentaires pour les workflows Argo","text":"<p>Des exemples de workflows Argo Workflows peuvent \u00eatre trouv\u00e9s dans les r\u00e9pos Github suivants :</p> <ul> <li>Documentation Argo Workflows</li> <li>[R\u00e9f\u00e9rence Argo CLI] (https://argoproj.github.io/argo-workflows/walk-through/argo-cli/)</li> </ul>"},{"location":"3-Pipelines/Machine-Learning/","title":"Mod\u00e8les d'apprentissage automatique","text":"<p>Les mod\u00e8les d'apprentissage automatique sont des algorithmes de calcul con\u00e7us pour apprendre automatiquement des mod\u00e8les et des relations \u00e0 partir de donn\u00e9es. Ces mod\u00e8les sont entra\u00een\u00e9s sur un ensemble de donn\u00e9es, qui est g\u00e9n\u00e9ralement une collection d'exemples ou d'instances, chacun d'entre eux se composant d'un ensemble de fonctionnalit\u00e9s ou de variables, ainsi que d'une variable cible ou d'une sortie.</p> <p>L'objectif d'un mod\u00e8le d'apprentissage automatique est d'identifier des mod\u00e8les et des relations au sein des donn\u00e9es qui peuvent \u00eatre utilis\u00e9s pour faire des pr\u00e9dictions ou des d\u00e9cisions concernant de nouvelles donn\u00e9es invisibles. Cela implique de d\u00e9velopper une repr\u00e9sentation math\u00e9matique de la relation entre les caract\u00e9ristiques d'entr\u00e9e et la variable de sortie, sur la base des mod\u00e8les observ\u00e9s dans les donn\u00e9es d'apprentissage. Une fois le mod\u00e8le entra\u00een\u00e9, il peut \u00eatre utilis\u00e9 pour faire des pr\u00e9dictions ou prendre des d\u00e9cisions concernant de nouvelles donn\u00e9es in\u00e9dites.</p> <p>Il existe plusieurs types de mod\u00e8les d'apprentissage automatique, chacun \u00e9tant con\u00e7u pour traiter diff\u00e9rents types de probl\u00e8mes ou de donn\u00e9es. Certains des types les plus courants de mod\u00e8les d'apprentissage automatique incluent :</p> <ol> <li> <p>Mod\u00e8les de r\u00e9gression : Les mod\u00e8les de r\u00e9gression sont utilis\u00e9s pour pr\u00e9dire des valeurs num\u00e9riques continues, telles que les cours des actions ou les prix des logements.</p> </li> <li> <p>Mod\u00e8les de classification : Les mod\u00e8les de classification sont utilis\u00e9s pour pr\u00e9dire des valeurs cat\u00e9gorielles discr\u00e8tes, par exemple si un client ach\u00e8tera ou non un produit.</p> </li> <li> <p>Mod\u00e8les de clustering : Les mod\u00e8les de clustering sont utilis\u00e9s pour identifier des groupes ou des clusters au sein d'un ensemble de donn\u00e9es en fonction des similitudes entre les instances.</p> </li> <li> <p>Mod\u00e8les de recommandation : Les mod\u00e8les de recommandation sont utilis\u00e9s pour recommander des produits ou des services aux utilisateurs en fonction de leur comportement ou de leurs pr\u00e9f\u00e9rences pass\u00e9s.</p> </li> <li> <p>R\u00e9seaux de neurones : Les r\u00e9seaux de neurones sont un type de mod\u00e8le d'apprentissage automatique con\u00e7u pour imiter la structure et la fonction du cerveau humain. Ils sont couramment utilis\u00e9s dans les applications de reconnaissance d'images, de reconnaissance vocale et de traitement du langage naturel.</p> </li> </ol> <p>Les mod\u00e8les d'apprentissage automatique peuvent \u00eatre biais\u00e9s</p> <p>Les mod\u00e8les d'apprentissage automatique sont un outil puissant pour analyser et faire des pr\u00e9dictions sur les donn\u00e9es, et ils ont un large \u00e9ventail d'applications dans des domaines tels que la finance, la sant\u00e9, le marketing, etc. Cependant, il est important de noter que les mod\u00e8les d'apprentissage automatique ne sont pas parfaits et peuvent parfois faire des erreurs ou produire des r\u00e9sultats biais\u00e9s. Par cons\u00e9quent, il est important d'\u00e9valuer et de tester soigneusement les mod\u00e8les d'apprentissage automatique avant de les utiliser dans des applications r\u00e9elles.</p>"},{"location":"3-Pipelines/Machine-Learning/#exemples","title":"Exemples","text":""},{"location":"3-Pipelines/Machine-Learning/#regression-lineaire","title":"R\u00e9gression lin\u00e9aire","text":"<p>R\u00e9gression lin\u00e9aire</p> <p>$$  \\hat{Y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 X_i + \\hat{\\epsilon}_i  $$</p> <p>O\u00f9 \\(\\hat{Y}_i\\) d\u00e9signe le \\(i\\)i\u00e8me estimateur de la vraie valeur \\(Y\\) en fonction de la \\(i\\)i\u00e8me p\u00e9riode d'apprentissage. Chaque \\(\\hat{\\beta}\\) est un param\u00e8tre \u00e0 apprendre. \\(\\hat{\\epsilon}_i\\) est la quantit\u00e9 de bruit autoris\u00e9e dans le mod\u00e8le et peut varier en fonction du nombre d'\u00e9poques d'entra\u00eenement indiqu\u00e9 par \\(i\\). Chaque \\(X_i\\) repr\u00e9sente le \\(i\\)i\u00e8me lot de donn\u00e9es d'apprentissage.</p> <p>Dans les mod\u00e8les statistiques classiques comme la r\u00e9gression lin\u00e9aire, l'objectif est de trouver une ligne qui correspond le mieux aux donn\u00e9es, nous permettant de faire des pr\u00e9dictions sur de nouveaux points de donn\u00e9es.</p> <p>\u00c0 mesure que la complexit\u00e9 du probl\u00e8me augmente, des algorithmes plus sophistiqu\u00e9s sont n\u00e9cessaires, tels que des arbres de d\u00e9cision, des machines \u00e0 vecteurs de support et des for\u00eats al\u00e9atoires. Cependant, ces m\u00e9thodes ont des limites et peuvent ne pas \u00eatre en mesure de capturer des mod\u00e8les complexes dans de grands ensembles de donn\u00e9es.</p>"},{"location":"3-Pipelines/Machine-Learning/#exemple-de-code","title":"Exemple de code","text":"PythonR linear_regression.py<pre><code>#!/usr/bin/env python\n# Charger les biblioth\u00e8ques requises\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\n# Charger le jeu de donn\u00e9es\ndata = pd.read_csv('path/to/dataset.csv')\n# Diviser les donn\u00e9es en ensembles d'entra\u00eenement et de test\nX_train, X_test, y_train, y_test = train_test_split(data.drop('target_variable', axis=1), data['target_variable'], test_size=0.2)\n# Former le mod\u00e8le de r\u00e9gression lin\u00e9aire\nlinear_model = LinearRegression()\nlinear_model.fit(X_train, y_train)\n# Faire des pr\u00e9dictions sur l'ensemble de test\ny_pred = linear_model.predict(X_test)\n# \u00c9valuer les performances du mod\u00e8le\nmse = mean_squared_error(y_test, y_pred)\nrmse = mse ** 0.5\nprint('Root Mean Squared Error:', rmse)\n</code></pre> <p>``` r title=\"linear_regression.r\" linenums=\"1\"</p>"},{"location":"3-Pipelines/Machine-Learning/#usrbinenv-rscript","title":"!/usr/bin/env Rscript","text":""},{"location":"3-Pipelines/Machine-Learning/#definir-une-graine-aleatoire-pour-la-reproductibilite","title":"D\u00e9finir une graine al\u00e9atoire pour la reproductibilit\u00e9","text":"<p>set.seed(123)</p>"},{"location":"3-Pipelines/Machine-Learning/#charger-le-jeu-de-donnees","title":"Charger le jeu de donn\u00e9es","text":"<p>data &lt;- read.csv('path/to/dataset.csv')</p>"},{"location":"3-Pipelines/Machine-Learning/#diviser-les-donnees-en-ensembles-dentrainement-et-de-test","title":"Diviser les donn\u00e9es en ensembles d'entra\u00eenement et de test","text":"<p>train_index &lt;- sample(1:nrow(data), size=0.8*nrow(data)) train_data &lt;- data[train_index,] test_data &lt;- data[-train_index,]</p>"},{"location":"3-Pipelines/Machine-Learning/#former-le-modele-de-regression-lineaire","title":"Former le mod\u00e8le de r\u00e9gression lin\u00e9aire","text":"<p>lm_model &lt;- lm(target_variable ~ ., data=train_data)</p>"},{"location":"3-Pipelines/Machine-Learning/#faire-des-predictions-sur-lensemble-de-test","title":"Faire des pr\u00e9dictions sur l'ensemble de test","text":"<p>y_pred &lt;- predict(lm_model, newdata=test_data[,-which(names(test_data)=='target_variable')])</p>"},{"location":"3-Pipelines/Machine-Learning/#evaluer-les-performances-du-modele","title":"\u00c9valuer les performances du mod\u00e8le","text":"<p>mse &lt;- mean((y_pred - test_data$target_variable)^2) rmse &lt;- sqrt(mse) print(paste('Root Mean Squared Error:', rmse))  ```</p>"},{"location":"3-Pipelines/Machine-Learning/#machine-a-vecteurs-de-support-svm","title":"Machine \u00e0 vecteurs de support (SVM)","text":"<p>SVM</p> <p>$$  \\underset{\\mathbf{w},b,\\boldsymbol{\\xi}}{\\operatorname{minimize}} \\hspace{0.2cm} \\frac{1}{2} ||\\mathbf{w}||^2 + C \\sum_{i=1}^{N} \\xi_i  $$</p> <p>$$  \\text{o\u00f9} \\hspace{0.2cm} y_i(\\mathbf{w}^T\\mathbf{x}_i + b) \\geq 1-\\xi_i \\quad \\text{and} \\quad \\hspace{0.2cm} \\xi_i \\geq 0 \\hspace{0.2cm} \\forall i \\in {1,2,...,N}  $$</p> <p>Dans cette formule, nous utilisons la formulation SVM standard o\u00f9 \\(\\mathbf{w}\\) est le vecteur de poids, \\(b\\) est le terme de biais et \\(\\boldsymbol{\\xi}\\) est le vecteur variable d'\u00e9cart. L'objectif est de minimiser la norme L2 du vecteur de poids \\(\\mathbf{w}\\), sous la contrainte que tous les exemples d'apprentissage sont class\u00e9s correctement avec une marge d'au moins 1, plus une tol\u00e9rance pour certaines violations de marge contr\u00f4l\u00e9es par le param\u00e8tre de r\u00e9gularisation \\(C\\). La variable cible \\(y_i\\) prend les valeurs 1 ou -1, repr\u00e9sentant les deux classes du probl\u00e8me de classification binaire, et \\(\\mathbf{x}_i\\) est le vecteur de caract\u00e9ristiques pour le \\(i\\)i\u00e8me exemple d'entra\u00eenement.</p> <p>Une machine \u00e0 vecteurs de support (SVM) est un algorithme d'apprentissage automatique supervis\u00e9 qui peut \u00eatre utilis\u00e9 pour la classification, la r\u00e9gression et la d\u00e9tection des valeurs aberrantes. C'est un algorithme populaire dans le domaine de l'apprentissage automatique, en particulier pour r\u00e9soudre les probl\u00e8mes de classification.</p> <p>L'id\u00e9e de base derri\u00e8re SVM est de trouver un hyperplan qui s\u00e9pare au mieux les donn\u00e9es d'entr\u00e9e en diff\u00e9rentes classes. Dans un probl\u00e8me de classification \u00e0 deux classes, l'hyperplan est une ligne qui s\u00e9pare les points de donn\u00e9es d'une classe des points de donn\u00e9es de l'autre classe. SVM essaie de trouver l'hyperplan qui maximise la marge entre les deux classes, o\u00f9 la marge est la distance entre l'hyperplan et les points de donn\u00e9es les plus proches de chaque classe.</p>"},{"location":"3-Pipelines/Machine-Learning/#exemple-de-code_1","title":"Exemple de code","text":"PythonR svm.py<pre><code>#!/usr/bin/env python\n# Charger les biblioth\u00e8ques requises\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score\n# Charger le jeu de donn\u00e9es\ndata = pd.read_csv('path/to/dataset.csv')\n# Diviser les donn\u00e9es en ensembles d'entra\u00eenement et de test\nX_train, X_test, y_train, y_test = train_test_split(data.drop('target_variable', axis=1), data['target_variable'], test_size=0.2)\n# Former le mod\u00e8le SVM\nsvm_model = SVC(kernel='linear', C=1.0, random_state=42)\nsvm_model.fit(X_train, y_train)\n# Faire des pr\u00e9dictions sur l'ensemble de test\ny_pred = svm_model.predict(X_test)\n# \u00c9valuer les performances du mod\u00e8le\naccuracy = accuracy_score(y_test, y_pred)\nprint('Accuracy:', accuracy)\n</code></pre> svm.r<pre><code>#!/usr/bin/env Rscript\n# Charger les biblioth\u00e8ques requises\nlibrary(e1071)\n# Charger le jeu de donn\u00e9es\ndata &lt;- read.csv('path/to/dataset.csv')\n# Diviser les donn\u00e9es en ensembles d'entra\u00eenement et de test\nset.seed(123)\ntrain_index &lt;- sample(1:nrow(data), size=0.8*nrow(data))\ntrain_data &lt;- data[train_index,]\ntest_data &lt;- data[-train_index,]\n# Former le mod\u00e8le SVM\nsvm_model &lt;- svm(target_variable ~ ., data=train_data, kernel='linear', cost=1)\n# Faire des pr\u00e9dictions sur l'ensemble de test\ny_pred &lt;- predict(svm_model, newdata=test_data[,-which(names(test_data)=='target_variable')])\n# \u00c9valuer les performances du mod\u00e8le\naccuracy &lt;- mean(y_pred == test_data$target_variable)\nprint(paste('Accuracy:', accuracy))\n</code></pre>"},{"location":"3-Pipelines/Machine-Learning/#foret-aleatoire","title":"For\u00eat al\u00e9atoire","text":"<p>For\u00eat al\u00e9atoire</p> <p>$$  \\hat{y} = \\frac{1}{T} \\sum_{t=1}^{T} f_t(\\mathbf{x}),  $$</p> <p>o\u00f9 \\(\\hat{y}\\) est la sortie pr\u00e9dite, \\(f_t(\\mathbf{x})\\) est la pr\u00e9diction du \\(t\\)i\u00e8me arbre de la for\u00eat pour l'entr\u00e9e \\(\\mathbf{x}\\), et $T $ est le nombre d'arbres dans la for\u00eat.</p> <p>Les for\u00eats al\u00e9atoires sont une m\u00e9thode d'apprentissage d'ensemble qui peut \u00eatre utilis\u00e9e pour les probl\u00e8mes de classification et de r\u00e9gression. Ils sont souvent utilis\u00e9s pour leur capacit\u00e9 \u00e0 g\u00e9rer des ensembles de donn\u00e9es dimensionnelles \u00e0 haute variation et des relations non lin\u00e9aires entre les entit\u00e9s et les cibles.</p> <p>Chaque arbre est entra\u00een\u00e9 sur un sous-ensemble amorc\u00e9 des donn\u00e9es d'entra\u00eenement d'origine, et \u00e0 chaque division, un sous-ensemble al\u00e9atoire de caract\u00e9ristiques est pris en compte pour d\u00e9terminer la division. La pr\u00e9diction finale est obtenue en faisant la moyenne des pr\u00e9dictions de tous les arbres de la for\u00eat.</p>"},{"location":"3-Pipelines/Machine-Learning/#exemple-de-code_2","title":"Exemple de code","text":"PythonR random_forest.py<pre><code>#!/usr/bin/env python\n# Charger les biblioth\u00e8ques requises\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\n# Charger le jeu de donn\u00e9es\ndata = pd.read_csv('path/to/dataset.csv')\n# Diviser les donn\u00e9es en ensembles d'entra\u00eenement et de test\nX_train, X_test, y_train, y_test = train_test_split(data.drop('target_variable', axis=1), data['target_variable'], test_size=0.2)\n# Former le mod\u00e8le de for\u00eat al\u00e9atoire\nrf_model = RandomForestRegressor(n_estimators=100, random_state=42)\nrf_model.fit(X_train, y_train)\n# Faire des pr\u00e9dictions sur l'ensemble de test\ny_pred = rf_model.predict(X_test)\n# \u00c9valuer les performances du mod\u00e8le\nmse = mean_squared_error(y_test, y_pred)\nrmse = mse ** 0.5\nprint('Root Mean Squared Error:', rmse)\n</code></pre> random_forest.r<pre><code>#!/usr/bin/env Rscript\n# Charger les biblioth\u00e8ques requises\nlibrary(randomForest)\n# Charger le jeu de donn\u00e9es\ndata &lt;- read.csv('path/to/dataset.csv')\n# Diviser les donn\u00e9es en ensembles d'entra\u00eenement et de test\nset.seed(123)\ntrain_index &lt;- sample(1:nrow(data), size=0.8*nrow(data))\ntrain_data &lt;- data[train_index,]\ntest_data &lt;- data[-train_index,]\n# Former le mod\u00e8le de for\u00eat al\u00e9atoire\nrf_model &lt;- randomForest(target_variable ~ ., data=train_data, ntree=100, importance=TRUE)\n# Faire des pr\u00e9dictions sur l'ensemble de test\ny_pred &lt;- predict(rf_model, newdata=test_data[,-which(names(test_data)=='target_variable')])\n# \u00c9valuer les performances du mod\u00e8le\nmse &lt;- mean((y_pred - test_data$target_variable)^2)\nrmse &lt;- sqrt(mse)\nprint(paste('Root Mean Squared Error:', rmse))\n</code></pre>"},{"location":"3-Pipelines/Machine-Learning/#lapprentissage-en-profondeur","title":"L'apprentissage en profondeur","text":"<p>Apprentissage en profondeur</p> <p>$$  \\hat{y} = f(\\mathbf{W}L f(\\mathbf{W} f(\\dots f(\\mathbf{W}1\\mathbf{x}+\\mathbf{b} _1)\\dots)+\\mathbf{b})+\\mathbf{b}_L)  $$</p> <p>o\u00f9 \\(\\mathbf{x}\\) est le vecteur d'entr\u00e9e, \\(\\mathbf{W}_i\\) et \\(\\mathbf{b}_i\\) sont respectivement la matrice de pond\u00e9ration et le vecteur de biais pour la \\(i\\)i\u00e8me couche, et $ f$ est la fonction d'activation.</p> <p>Cette formule repr\u00e9sente un r\u00e9seau de neurones \u00e0 anticipation avec des couches \\(L\\), o\u00f9 chaque couche applique une transformation lin\u00e9aire \u00e0 la sortie de la couche pr\u00e9c\u00e9dente, suivie d'une fonction d'activation non lin\u00e9aire. La sortie de la couche finale, \\(\\hat{y}\\), repr\u00e9sente la sortie pr\u00e9dite du r\u00e9seau de neurones pour l'entr\u00e9e donn\u00e9e \\(\\mathbf{x}\\).</p> <p>L'apprentissage en profondeur est un sous-ensemble de l'apprentissage automatique qui implique la formation de r\u00e9seaux de neurones avec de nombreuses couches de n\u0153uds interconnect\u00e9s. Cette approche peut g\u00e9rer des ensembles de donn\u00e9es volumineux et complexes et est utilis\u00e9e dans un large \u00e9ventail d'applications, notamment la reconnaissance d'images, le traitement du langage naturel et la reconnaissance vocale. Le processus de formation consiste \u00e0 alimenter le r\u00e9seau de neurones avec un grand ensemble de donn\u00e9es et \u00e0 ajuster les poids des connexions entre les n\u0153uds pour minimiser l'erreur entre les sorties pr\u00e9dites et les sorties r\u00e9elles. Gr\u00e2ce \u00e0 des it\u00e9rations r\u00e9p\u00e9t\u00e9es, le r\u00e9seau de neurones peut apprendre \u00e0 reconna\u00eetre des mod\u00e8les dans les donn\u00e9es et \u00e0 faire des pr\u00e9dictions pr\u00e9cises sur de nouvelles donn\u00e9es.</p>"},{"location":"3-Pipelines/Machine-Learning/#exemple-de-code_3","title":"Exemple de code","text":"PythonR deep_learning.py<pre><code>#!/usr/bin/env python\n# Charger les biblioth\u00e8ques requises\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score\n# Charger le jeu de donn\u00e9es\ndata = pd.read_csv('path/to/dataset.csv')\n# Diviser les donn\u00e9es en ensembles d'entra\u00eenement et de test\nX_train, X_test, y_train, y_test = train_test_split(data.drop('target_variable', axis=1), data['target_variable'], test_size=0.2)\n# Standardiser les caract\u00e9ristiques d'entr\u00e9e\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n# D\u00e9finir le mod\u00e8le de deep learning\nmodel = keras.Sequential([\nkeras.layers.Dense(64, activation='relu', input_shape=[X_train_scaled.shape[1]]),\nkeras.layers.Dropout(0.2),\nkeras.layers.Dense(1, activation='sigmoid')\n])\n# Compiler le mod\u00e8le\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n# Former le mod\u00e8le\nhistory = model.fit(X_train_scaled, y_train, epochs=50, batch_size=32, validation_split=0.1)\n# \u00c9valuer les performances du mod\u00e8le\ny_pred = model.predict_classes(X_test_scaled)\naccuracy = accuracy_score(y_test, y_pred)\nprint('Accuracy:', accuracy)\n</code></pre> deep_learning.r<pre><code>#!/usr/bin/env Rscript\n# Charger les biblioth\u00e8ques requises\nlibrary(keras)\nlibrary(tensorflow)\n# Charger le jeu de donn\u00e9es\ndata &lt;- iris\nx &lt;- as.matrix(data[, 1:4])\ny &lt;- to_categorical(as.numeric(data[, 5]) - 1)\n# Diviser les donn\u00e9es en ensembles d'entra\u00eenement et de test\nset.seed(123)\ntrain_index &lt;- sample(1:nrow(data), size=0.8*nrow(data))\nx_train &lt;- x[train_index,]\ny_train &lt;- y[train_index,]\nx_test &lt;- x[-train_index,]\ny_test &lt;- y[-train_index,]\n# D\u00e9finir l'architecture du r\u00e9seau neuronal\nmodel &lt;- keras_model_sequential() %&gt;%\nlayer_dense(units = 8, input_shape = c(4)) %&gt;%\nlayer_activation('relu') %&gt;%\nlayer_dense(units = 3) %&gt;%\nlayer_activation('softmax')\n# Compiler le mod\u00e8le\nmodel %&gt;% compile(\nloss = 'categorical_crossentropy',\noptimizer = 'adam',\nmetrics = c('accuracy')\n)\n# Former le mod\u00e8le\nhistory &lt;- model %&gt;% fit(\nx_train, y_train,\nepochs = 50,\nbatch_size = 10,\nvalidation_split = 0.2,\nverbose = 0\n)\n# \u00c9valuer les performances du mod\u00e8le\nmetrics &lt;- model %&gt;% evaluate(x_test, y_test)\nprint(paste('Test Loss:', metrics[1]))\nprint(paste('Test Accuracy:', metrics[2]))\n# Tracez la pr\u00e9cision de la formation et de la validation dans le temps\nplot(history$metrics$accuracy, type='l', col='blue', ylim=c(0,1), ylab='Accuracy', xlab='Epoch')\nlines(history$metrics$val_accuracy, type='l', col='red')\nlegend('bottomright', legend=c('Training', 'Validation'), col=c('blue', 'red'), lty=1)\n</code></pre>"},{"location":"3-Pipelines/Overview/","title":"Overview","text":"<p>MLOps, ou lop\u00e9rations d'apprentissage automatique, fait r\u00e9f\u00e9rence \u00e0 l'ensemble de pratiques et d'outils qui permettent aux organisations de d\u00e9velopper, d\u00e9ployer et maintenir des mod\u00e8les d'apprentissage automatique \u00e0 grande \u00e9chelle. MLOps vise \u00e0 rationaliser le processus de bout en bout de cr\u00e9ation et de d\u00e9ploiement de mod\u00e8les d'apprentissage automatique en int\u00e9grant les diff\u00e9rentes \u00e9tapes du cycle de vie de l'apprentissage automatique dans un flux de travail coh\u00e9rent et automatis\u00e9.</p> <p>MLOps implique une gamme d'activit\u00e9s diff\u00e9rentes, y compris la pr\u00e9paration et le pr\u00e9traitement des donn\u00e9es, la formation et l'optimisation des mod\u00e8les, le d\u00e9ploiement et le service des mod\u00e8les, la surveillance et la maintenance et l'am\u00e9lioration continue. Certains des composants cl\u00e9s des MLOps incluent\u00a0:</p> <ol> <li> <p>Gestion des donn\u00e9es : MLOps implique la gestion et le traitement de grandes quantit\u00e9s de donn\u00e9es pour garantir la qualit\u00e9 et la pr\u00e9cision des mod\u00e8les d'apprentissage automatique. Cela implique des activit\u00e9s telles que le nettoyage, l'int\u00e9gration et la transformation des donn\u00e9es.</p> </li> <li> <p>Entra\u00eenement et optimisation des mod\u00e8les : MLOps implique de d\u00e9velopper et de tester des mod\u00e8les d'apprentissage automatique, ainsi que de les optimiser en termes de performances et de pr\u00e9cision. Cela peut impliquer d'exp\u00e9rimenter diff\u00e9rents algorithmes, hyperparam\u00e8tres et techniques de pr\u00e9traitement des donn\u00e9es.</p> </li> <li> <p>D\u00e9ploiement de mod\u00e8les : MLOps implique le d\u00e9ploiement de mod\u00e8les d'apprentissage automatique dans des environnements de production, les rendant disponibles pour une utilisation dans des applications du monde r\u00e9el. Cela peut impliquer la mise en conteneur de mod\u00e8les pour un d\u00e9ploiement et une mise \u00e0 l'\u00e9chelle faciles, ainsi que la configuration d'API et d'autres interfaces pour la diffusion de mod\u00e8les.</p> </li> <li> <p>Surveillance et maintenance : MLOps implique la surveillance des mod\u00e8les d'apprentissage automatique en production pour s'assurer qu'ils fonctionnent comme pr\u00e9vu. Cela peut impliquer la configuration d'alertes et de notifications pour les d\u00e9faillances du mod\u00e8le, ainsi que la mise en \u0153uvre de processus pour la maintenance et les mises \u00e0 jour du mod\u00e8le.</p> </li> <li> <p>Am\u00e9lioration continue : MLOps implique l'am\u00e9lioration continue des mod\u00e8les d'apprentissage automatique au fil du temps, en fonction des commentaires des utilisateurs et de l'analyse continue des donn\u00e9es de performance. Cela peut impliquer de recycler les mod\u00e8les avec de nouvelles donn\u00e9es ou d'int\u00e9grer les commentaires des utilisateurs pour affiner les mod\u00e8les.</p> </li> </ol> <p>Afin de mettre en \u0153uvre efficacement les MLOps, les organisations doivent g\u00e9n\u00e9ralement adopter une gamme d'outils et de technologies diff\u00e9rents, notamment des plates-formes de gestion de donn\u00e9es, des cadres d'apprentissage automatique, des outils de conteneurisation et des outils de surveillance et de journalisation. Ils doivent \u00e9galement \u00e9tablir des flux de travail et des processus clairs pour g\u00e9rer les diff\u00e9rentes \u00e9tapes du cycle de vie de l'apprentissage automatique, ainsi que mettre en \u0153uvre des mesures de gouvernance et de conformit\u00e9 pour garantir la confidentialit\u00e9 et la s\u00e9curit\u00e9 des donn\u00e9es.</p> <p>En r\u00e9sum\u00e9, MLOps est un composant essentiel du cycle de vie de l'apprentissage automatique, permettant aux organisations de d\u00e9velopper, d\u00e9ployer et maintenir des mod\u00e8les d'apprentissage automatique \u00e0 grande \u00e9chelle. En adoptant des pratiques et des outils MLOps, les organisations peuvent rationaliser leurs flux de travail d'apprentissage automatique, am\u00e9liorer la pr\u00e9cision et les performances des mod\u00e8les et offrir plus de valeur aux utilisateurs et aux parties prenantes.</p>"},{"location":"3-Pipelines/PaaS-Integration/","title":"Aper\u00e7u","text":"<p>L'un des principaux avantages de la plate-forme ETAA est sa capacit\u00e9 \u00e0 s'int\u00e9grer aux plates-formes d'apprentissage automatique populaires telles que Databricks et AzureML.</p> <p>Espace de travail d'analyse avanc\u00e9e (ETAA) est une plate-forme d'analyse de donn\u00e9es open source con\u00e7ue pour \u00eatre hautement int\u00e9grable. Cela signifie qu'il peut \u00eatre facilement int\u00e9gr\u00e9 \u00e0 d'autres plates-formes et outils pour \u00e9tendre ses capacit\u00e9s et rationaliser les flux de travail.</p> <p>Un exemple de diagramme illustrant une strat\u00e9gie de connexion PaaS possible\u00a0:</p> <p></p> <p> </p> <p>Configuration\u00a0: Si vous avez besoin d'aide pour int\u00e9grer une plate-forme en tant qu'offre de service, nous sommes heureux aider!</p>"},{"location":"3-Pipelines/PaaS-Integration/#integration-avec-les-offres-de-plate-forme-externe-en-tant-que-service-paas","title":"Int\u00e9gration avec les offres de plate-forme externe en tant que service (PaaS)","text":"<p>L'int\u00e9gration est la cl\u00e9 du succ\u00e8s.</p> <p></p> <p>Notre plate-forme open source offre une option in\u00e9gal\u00e9e \u00e0 nos utilisateurs. En permettant aux utilisateurs d'utiliser des outils open source, nous leur donnons la possibilit\u00e9 d'utiliser leurs frameworks de science des donn\u00e9es et d'apprentissage automatique pr\u00e9f\u00e9r\u00e9s. Mais la v\u00e9ritable puissance de notre plateforme vient de sa capacit\u00e9 \u00e0 s'int\u00e9grer \u00e0 de nombreuses offres de plateforme en tant que service (PaaS), comme Databricks ou AzureML. Cela signifie que nos utilisateurs peuvent tirer parti de la puissance du cloud pour ex\u00e9cuter des pipelines complexes de traitement de donn\u00e9es et d'apprentissage automatique \u00e0 grande \u00e9chelle. Avec la possibilit\u00e9 de s'int\u00e9grer aux offres PaaS, notre plate-forme permet \u00e0 nos utilisateurs de faire passer leur travail au niveau sup\u00e9rieur, en leur donnant le pouvoir d'adapter facilement leurs charges de travail et de tirer parti des derni\u00e8res innovations dans le domaine de la science des donn\u00e9es et de la machine. apprentissage. En offrant ce niveau d'optionnalit\u00e9, nous nous assurons que nos utilisateurs peuvent toujours choisir le bon outil pour le travail et garder une longueur d'avance dans un domaine en \u00e9volution constante.</p> <p>Nous pouvons nous int\u00e9grer \u00e0 de nombreuses offres de plate-forme en tant que service (PaaS), comme Databricks ou AzureML.</p>"},{"location":"3-Pipelines/PaaS-Integration/#briques-de-donnees","title":"Briques de donn\u00e9es","text":"<ul> <li>Databricks de Microsoft</li> </ul> <p>Databricks est une plate-forme bas\u00e9e sur le cloud qui fournit une plate-forme d'analyse unifi\u00e9e pour le traitement du Big Data et l'apprentissage automatique. Avec son puissant moteur de calcul distribu\u00e9 et ses outils de flux de travail rationalis\u00e9s, Databricks est un choix populaire pour cr\u00e9er et d\u00e9ployer des mod\u00e8les d'apprentissage automatique. En s'int\u00e9grant \u00e0 Databricks, la plate-forme ETAA peut tirer parti de ses capacit\u00e9s informatiques distribu\u00e9es pour former et d\u00e9ployer des mod\u00e8les d'apprentissage automatique \u00e0 grande \u00e9chelle.</p>"},{"location":"3-Pipelines/PaaS-Integration/#azureml","title":"AzureML","text":"<ul> <li>Azure ML de Microsoft</li> </ul> <p>AzureML est une autre plate-forme d'apprentissage automatique populaire qui fournit une large gamme d'outils pour cr\u00e9er, former et d\u00e9ployer des mod\u00e8les d'apprentissage automatique. En s'int\u00e9grant \u00e0 AzureML, la plateforme ETAA peut tirer parti de ses puissants outils de cr\u00e9ation et de formation de mod\u00e8les, ainsi que de sa capacit\u00e9 \u00e0 d\u00e9ployer des mod\u00e8les dans le cloud.</p>"},{"location":"3-Pipelines/PaaS-Integration/#exemples","title":"Exemples","text":"<p>Des exemples d'int\u00e9gration de la plate-forme ETAA avec ces plates-formes et d'autres peuvent \u00eatre trouv\u00e9s sur le r\u00e9f\u00e9rentiel MLOps Github.</p> <ul> <li>D\u00e9p\u00f4t Github MLOps</li> </ul> <p>Ce r\u00e9f\u00e9rentiel contient une gamme d'exemples et de didacticiels pour l'utilisation de la plate-forme ETAA dans divers flux de travail d'apprentissage automatique, y compris la pr\u00e9paration des donn\u00e9es, la formation de mod\u00e8les et le d\u00e9ploiement de mod\u00e8les.</p>"},{"location":"3-Pipelines/PaaS-Integration/#conclusion","title":"Conclusion","text":"<p>En s'int\u00e9grant \u00e0 des plates-formes d'apprentissage automatique populaires telles que Databricks et AzureML, la plate-forme ETAA fournit une solution puissante et flexible pour cr\u00e9er, d\u00e9ployer et g\u00e9rer des workflows d'apprentissage automatique \u00e0 grande \u00e9chelle.</p> <p>En tirant parti des int\u00e9grations et des outils fournis par ces plates-formes, les scientifiques des donn\u00e9es et les ing\u00e9nieurs en apprentissage automatique peuvent acc\u00e9l\u00e9rer leurs flux de travail et obtenir de meilleurs r\u00e9sultats avec moins d'effort.</p>"},{"location":"3-Pipelines/PaaS/","title":"Int\u00e9grer avec des plateformes comme Databricks et AzureML","text":"<p>La plateforme AAW est construite autour de l'id\u00e9e d'int\u00e9grations, et nous pouvons donc s'int\u00e9grer avec de nombreuses offres de \"plateforme en tant que service\" (PaaS), telles que Azure ML et Databricks.</p> <p>Voir quelques exemples dans notre D\u00e9p\u00f4t Github \"MLOps\".</p> <p></p>"},{"location":"3-Pipelines/Serving/","title":"Service de mod\u00e8le avec Seldon Core et KFServing","text":"<p>\u2692 Cette page est en construction \u2692</p> <p>La personne qui \u00e9crit cette entr\u00e9e n'en sait pas assez sur cette fonctionnalit\u00e9 pour \u00e9crire \u00e0 son sujet, mais vous pouvez demander sur notre canal en Slack.</p>"},{"location":"3-Pipelines/Serving/#sans-serveur-avec-knative","title":"Sans-serveur avec KNative","text":"<p>Kubernetes et [KNative] (https://knative.dev/) permettent \u00e0 vos services de monter ou descendre en puissance \u00e0 la demande. Cela vous permet de cr\u00e9er des API pour servir des mod\u00e8les d'apprentissage automatique, sans avoir besoin de g\u00e9rer l'\u00e9quilibrage de charge ou la mont\u00e9e en puissance. La plateforme peut g\u00e9rer toute votre mise \u00e0 l'\u00e9chelle pour vous, afin que vous puissiez vous concentrer sur la logique du programme.</p> <p>\u2692 Cette page est en construction \u2692</p> <p>La personne qui r\u00e9dige cette entr\u00e9e ne conna\u00eet pas suffisamment cette fonctionnalit\u00e9 pour \u00e9crire \u00e0 son sujet, mais vous pouvez demander sur notre canal en Slack.</p>"},{"location":"4-Collaboration/Aper%C3%A7u/","title":"Aper\u00e7u","text":"<p>Il existe de nombreuses fa\u00e7ons de collaborer sur la plateforme. Selon votre situation,ce qu vous voulez partager et le nombre de personnes que vous souhaitez partager avec. Les sc\u00e9narios se d\u00e9composent en gros en ce que vous voulez partager (Donn\u00e9es, Code, ou Environnements de calcul (e.g.: Partager les m\u00eames machines virtuelles)) et avec qui vous voulez le partager (Personne, Mon \u00e9quipe, ou Tout le monde). Cela conduit au tableau d'options suivant:</p> Priv\u00e9e \u00c9quipe StatCan Code GitLab/GitHub ou un dossier personnel GitLab/GitHub or dossier d'\u00e9quipe GitLab/GitHub Donn\u00e9es Dossier personnel ou compartiment Dossier d'\u00e9quipe ou compartiment , ou espace de noms partag\u00e9 Compartiment partag\u00e9 Calcul Espace de nom personnel Espace de noms partag\u00e9 N/A Quelle est la diff\u00e9rence entre un compartiment et un dossier? <p>Les compartiments sont comme le stockage sur r\u00e9seau. Consulter  pr\u00e9sentation du stockage pour plus de d\u00e9tails sur les diff\u00e9rences entre ces deux options.</p> <p>Choisir la meilleure fa\u00e7on de partager le code, les donn\u00e9es et le calcul implique des facteurs, mais vous pouvez g\u00e9n\u00e9ralement m\u00e9langer et assortir (partager le code avec votre \u00e9quipe via github, mais stockez vos donn\u00e9es en priv\u00e9 dans un compartiment personnel). Ces cas sont d\u00e9crits plus en d\u00e9tail dans les sections ci-dessous</p>"},{"location":"4-Collaboration/Aper%C3%A7u/#partager-le-code-entre-les-membres-de-lequipe","title":"Partager le code entre les membres de l'\u00e9quipe","text":"<p>Dans la plupart des cas, il est plus facile de partager du code en utilisant GitHub ou GitLab. L'avantage du partage avec GitHub ou GitLab est que cela fonctionne avec les utilisateurs \u00e0 travers les espaces de noms, et conserver le code dans github est un excellent moyen de g\u00e9rer de grands projets logiciels.</p> N'oubliez pas d'inclure une licence ! <p>Si votre code est public, n'oubliez pas de respecter les directives de l'\u00e9quipe d'innovation et d'utiliser une licence appropri\u00e9e si votre travail est effectu\u00e9 pour Statistique Canada.</p> <p>Si vous devez partager du code sans le publier sur un r\u00e9f\u00e9rentiel, partager un espace de nom) pourrait aussi fonctionner.</p>"},{"location":"4-Collaboration/Aper%C3%A7u/#partager-le-calcul-espace-de-nom-dans-kubeflow","title":"Partager le calcul (espace de nom) dans Kubeflow","text":"<p>!!! danger \"Partager un espace de nom signifie que vous partagez toutes les choses     dans l'espace de nom\".     Kubeflow ne prend pas en charge le partage granulaire d'une ressource (un bloc-notes, un compartiment MinIO, etc.), mais plut\u00f4t le partage de tous ressources. Si vous souhaitez partager un serveur Jupyter Carnet note avec quelqu'un, vous devez partager l'int\u00e9gralit\u00e9 de votre espace de nom et ils auront acc\u00e8s \u00e0 toutes les autres ressources (compartiment MinIO, etc.).</p> <p>Dans Kubeflow, chaque utilisateur dispose d'un espace de nom qui contient son travail (son serveur de Carnets notes, pipelines, disques, etc.). Votre espace de nom vous appartient, mais peut \u00eatre partag\u00e9 si vous voulez collaborer avec d'autres. Vous pouvez aussi  Demander un Espace de nom (soit pour vous-m\u00eame, soit pour partager avec une \u00e9quipe). Une option de collaboration consiste \u00e0 partager des espaces de noms avec les autres.</p> <p>L'avantage de partager un espace de nom Kubeflow est qu'il vous permet, ainsi qu'\u00e0 vos coll\u00e8gues partagent l'environnement de calcul et les compartiments MinIO associ\u00e9s au espace de nom. Cela en fait un moyen tr\u00e8s simple et libre de partager.</p> Demander de l'aide en production <p>Le personnel d'assistance d'Espace de travail d'analyse avanc\u00e9e se fera un plaisir de vous aider avec les cas d'utilisation orient\u00e9s vers la production, et nous pouvons probablement vous faire gagner beaucoup de temps. Ne soyez pas timide Demander pour l'aide!</p>"},{"location":"4-Collaboration/Aper%C3%A7u/#partager-des-donnees","title":"Partager des donn\u00e9es","text":"<p>Une fois que vous avez un espace de nom partag\u00e9, vous avez deux approches de stockage partag\u00e9</p> Possibilit\u00e9 de stockage Avantages Serveurs/espaces de travail Jupyter partag\u00e9s Plus adapt\u00e9 aux petits fichiers, aux cahiers et aux petites exp\u00e9riences. Compartiments partag\u00e9s( consultez Stockage) Mieux adapt\u00e9 pour une utilisation dans les pipelines, les API et pour les fichiers volumineux. <p>Pour en savoir plus sur la technologie qui les sous-tend, consultez le Stockage.</p>"},{"location":"4-Collaboration/Aper%C3%A7u/#partager-avec-statcan","title":"Partager avec StatCan","text":"<p>En plus des compartiments priv\u00e9s ou des compartiments priv\u00e9s partag\u00e9s par l'\u00e9quipe, vous pouvez \u00e9galement placez vos fichiers dans le stockage partag\u00e9. Dans toutes les options de stockage de compartiment (<code>minimal</code>, <code>premium</code>, <code>pachyderm</code>), vous disposez d'un compartiment priv\u00e9, et d'un dossier \u00e0 l'int\u00e9rieur du compartiment \u00ab partag\u00e9 \u00bb. Jetez un \u0153il, par exemple, au lien ci-dessous :</p> <ul> <li><code>shared/blair-drummond/</code></li> </ul> <p>Tout utilisateur connect\u00e9 peut voir ces fichiers et les lire librement.</p>"},{"location":"4-Collaboration/Aper%C3%A7u/#partage-avec-le-monde","title":"Partage avec le monde","text":"<p>Renseignez-vous sur celui-ci dans notre cha\u00eene Slack. L\u00e0 il existe de nombreuses fa\u00e7ons de le faire du c\u00f4t\u00e9 informatique, mais il est important que cela aille par des processus appropri\u00e9s, de sorte que cela ne se fait pas de mani\u00e8re \u00ab libre-service \u00bb que d'autres sont. Cela dit, c'est tout \u00e0 fait possible.</p>"},{"location":"4-Collaboration/Aper%C3%A7u/#recommandation-combinez-les-tous","title":"Recommandation : Combinez-les tous","text":"<p>C'est une excellente id\u00e9e de toujours utiliser github avec des espaces de travail partag\u00e9s est un excellent moyen de combiner le partage ad hoc (via des fichiers) tout en gardant votre code organis\u00e9 et suivi</p>"},{"location":"4-Collaboration/Aper%C3%A7u/#gestion-des-contributeurs","title":"Gestion des contributeurs","text":"<p>Vous pouvez ajouter ou supprimer des personnes d'un espace de nom que vous poss\u00e9dez d\u00e9j\u00e0 via le Menu G\u00e9rer les contributeurs dans Kubeflow.</p> <p></p> <p>Maintenant, vous et vos coll\u00e8gues pouvez partager l'acc\u00e8s \u00e0 un serveur!</p> <p>Essayer le!</p>"},{"location":"4-Collaboration/Demander-EspaceDeNom/","title":"Aper\u00e7u","text":"<p>Par d\u00e9faut, tout le monde obtient son propre espace de nom personnel, <code>pr\u00e9nom-nom</code>. Si vous souhaitez collaborer avec votre \u00e9quipe, vous pouvez demander un Espace de nom \u00e0 partager.</p>"},{"location":"4-Collaboration/Demander-EspaceDeNom/#installation","title":"Installation","text":""},{"location":"4-Collaboration/Demander-EspaceDeNom/#demander-un-espace-de-nom","title":"Demander un espace de nom","text":"<p>Pour cr\u00e9er un espace de noms pour une \u00e9quipe, acc\u00e9dez au portail AAW. Cliquez sur le \u22ee menu sur la section Kubeflow du portail.</p> <p></p> <p>Entrez le nom que vous demandez et soumettez la demande. Veillez \u00e0 n'utiliser que lettres minuscules plus tirets. </p> <p>!!! avertissement \"L'espace de noms ne peut pas avoir de caract\u00e8res sp\u00e9ciaux autres que       des traits d'union\"     Le nom de l'espace de noms ne doit \u00eatre compos\u00e9 que de lettres minuscules \u00ab a-z \u00bb avec des tirets. Sinon,l'espace de noms ne sera pas cr\u00e9\u00e9.</p> <p>Vous recevrez une notification par e-mail lorsque l'espace de noms sera cr\u00e9\u00e9. Une fois l'espace de noms partag\u00e9 est cr\u00e9\u00e9, vous pouvez y acc\u00e9der comme n'importe quel autre l'espace de noms que vous avez via l'interface utilisateur de Kubeflow, comme illustr\u00e9 ci-dessous. Vous serez alors capable de partager et g\u00e9rer \u00e0 votre espace de nom.</p> <p>Pour changer d'espace de noms, jetez un \u0153il en haut de votre fen\u00eatre, juste \u00e0 droite du logo Kubeflow.</p> <p></p>"},{"location":"4-Collaboration/Environnement-Analyse-Geospatiale/","title":"Environnement d'Analyse G\u00e9ospatiale (EAG) - Acc\u00e8s multi-plateforme","text":"Donn\u00e9es non prot\u00e9g\u00e9es uniquement, SSI bient\u00f4t disponible! <p>\u00c0 l'heure actuelle, notre serveur g\u00e9ospatial ne peut h\u00e9berger et donner acc\u00e8s qu'\u00e0 des informations statistiques non sensibles.  </p>"},{"location":"4-Collaboration/Environnement-Analyse-Geospatiale/#demarrage","title":"D\u00e9marrage","text":"<p>Conditions pr\u00e9alables</p> <ol> <li>Un projet int\u00e9gr\u00e9 avec acc\u00e8s au portail DAS EAG ArcGIS</li> <li>Un identifiant client ArcGIS Portal (cl\u00e9 API)</li> </ol> <p>Le portail ArcGIS Enterprise est accessible dans AAW ou CAE \u00e0 l'aide de l'API, \u00e0 partir de n'importe quel service qui exploite le langage de programmation Python. </p> <p>Par exemple, dans AAW et l'utilisation de Jupyter Notebooks dans l'espace, ou pour CAE l'utilisation de Databricks, DataFactory, etc.</p> <p>Le portail DAS GAE ArcGIS Enterprise est accessible directement ici</p> <p>Pour obtenir de l'aide sur l'auto-inscription en tant qu'utilisateur du portail g\u00e9ospatial DAS</p>"},{"location":"4-Collaboration/Environnement-Analyse-Geospatiale/#utilisation-de-lapi-arcgis-pour-python","title":"Utilisation de l'API ArcGIS pour Python","text":""},{"location":"4-Collaboration/Environnement-Analyse-Geospatiale/#connexion-a-arcgis-enterprise-portal-a-laide-de-lapi-arcgis","title":"Connexion \u00e0 ArcGIS Enterprise Portal \u00e0 l'aide de l'API ArcGIS","text":"<ol> <li> <p>Installez les packages\u00a0:</p> <pre><code>conda install -c esri arcgis\n</code></pre> <p>ou utilisez Artifactory</p> <pre><code>conda install -c https://jfrog.aaw.cloud.statcan.ca/artifactory/api/conda/esri-remote arcgis\n</code></pre> </li> <li> <p>Importez les librairies n\u00e9cessaires dont vous aurez besoin dans le Notebook.     <pre><code>from arcgis.gis import GIS\nfrom arcgis.gis import Item\n</code></pre></p> </li> <li> <p>Acc\u00e9der au portail    Votre groupe de projet recevra un identifiant client lors de l'int\u00e9gration. Collez l'ID client entre les guillemets<code>client_id='######'</code>. </p> <pre><code>gis = GIS(\"https://geoanalytics.cloud.statcan.ca/portal\", client_id=' ')\nprint(\"Successfully logged in as: \" + gis.properties.user.username)\n</code></pre> </li> <li> <ul> <li>La sortie vous redirigera vers un portail de connexion.</li> <li>Utilisez l'option de connexion Azure de StatCan et votre identifiant Cloud</li> <li>Apr\u00e8s une connexion r\u00e9ussie, vous recevrez un code pour vous connecter en utilisant SAML.</li> <li> <p>Collez ce code dans la sortie.</p> <p></p> </li> </ul> </li> </ol>"},{"location":"4-Collaboration/Environnement-Analyse-Geospatiale/#afficher-les-informations-utilisateur","title":"Afficher les informations utilisateur","text":"<p>En utilisant la fonction \"me\", nous pouvons afficher diverses informations sur l'utilisateur connect\u00e9. <pre><code>me = gis.users.me\nusername = me.username\ndescription = me.description\ndisplay(me)\n</code></pre></p>"},{"location":"4-Collaboration/Environnement-Analyse-Geospatiale/#rechercher-du-contenu","title":"Rechercher du contenu","text":"<p>Recherchez le contenu que vous avez h\u00e9berg\u00e9 sur le portail g\u00e9o DAaaS. En utilisant la fonction \"me\", nous pouvons rechercher tout le contenu h\u00e9berg\u00e9 sur le compte. Il existe plusieurs fa\u00e7ons de rechercher du contenu. Deux m\u00e9thodes diff\u00e9rentes sont d\u00e9crites ci-dessous.</p> <p>Recherchez tous vos \u00e9l\u00e9ments h\u00e9berg\u00e9s dans le portail g\u00e9ographique DAaaS. <pre><code>my_content = me.items()\nmy_content\n</code></pre> Recherchez du contenu sp\u00e9cifique que vous poss\u00e9dez dans le portail g\u00e9ographique DAaaS.</p> <p>Ceci est similaire \u00e0 l'exemple ci-dessus, mais si vous connaissez le titre de la couche que vous souhaitez utiliser, vous pouvez l'enregistrer en tant que fonction. <pre><code>my_items = me.items()\nfor items in my_items:\nprint(items.title, \" | \", items.type)\nif items.title == \"Flood in Sorel-Tracy\":\nflood_item = items\nelse:\ncontinue\nprint(flood_item)\n</code></pre></p> <p>Recherchez tout le contenu auquel vous avez acc\u00e8s, pas seulement le v\u00f4tre.</p> <pre><code>flood_item = gis.content.search(\"tags: flood\", item_type =\"Feature Service\")\nflood_item\n</code></pre>"},{"location":"4-Collaboration/Environnement-Analyse-Geospatiale/#obtenir-du-contenu","title":"Obtenir du contenu","text":"<p>Nous devons obtenir l'\u00e9l\u00e9ment du portail g\u00e9ographique DAaaS afin de l'utiliser dans le bloc-notes Jupyter. Cela se fait en fournissant le num\u00e9ro d'identification unique de l'article que vous souhaitez utiliser. Trois exemples sont d\u00e9crits ci-dessous, tous acc\u00e9dant \u00e0 la m\u00eame couche. <pre><code>item1 = gis.content.get(my_content[5].id) #from searching your content above\ndisplay(item1)\nitem2 = gis.content.get(flood_item.id) #from example above -searching for specific content\ndisplay(item2)\nitem3 = gis.content.get('edebfe03764b497f90cda5f0bfe727e2') #the actual content id number\ndisplay(item3)\n</code></pre></p>"},{"location":"4-Collaboration/Environnement-Analyse-Geospatiale/#effectuer-une-analyse","title":"Effectuer une analyse","text":"<p>Une fois les couches import\u00e9es dans le bloc-notes Jupyter, nous sommes en mesure d'effectuer des types d'analyse similaires \u00e0 ceux que vous vous attendriez \u00e0 trouver dans un logiciel SIG tel qu'ArcGIS. Il existe de nombreux modules contenant de nombreux sous-modules qui peuvent effectuer plusieurs types d'analyses. </p> <p>\u00c0 l'aide du module arcgis.features, importez le sous-module use_proximity <code>from arcgis.features import use_proximity</code>. Ce sous-module nous permet de <code>.create_buffers</code> - des zones \u00e0 \u00e9gale distance des entit\u00e9s. Ici, nous sp\u00e9cifions la couche que nous voulons utiliser, la distance, les unit\u00e9s et le nom de sortie (vous pouvez \u00e9galement sp\u00e9cifier d'autres caract\u00e9ristiques telles que le champ, le type d'anneau, le type de fin et autres). En sp\u00e9cifiant un nom de sortie, apr\u00e8s avoir ex\u00e9cut\u00e9 la commande buffer, une nouvelle couche sera automatiquement t\u00e9l\u00e9charg\u00e9e dans le portail DAaaS GEO contenant la nouvelle fonctionnalit\u00e9 que vous venez de cr\u00e9er. </p> <pre><code>buffer_lyr = use_proximity.create_buffers(item1, distances=[1], \nunits = \"Kilometers\", \noutput_name='item1_buffer')\ndisplay(item1_buffer)\n</code></pre> <p>Certains utilisateurs pr\u00e9f\u00e8rent travailler avec des packages Open Source. La traduction d'ArcGIS vers Spatial Dataframes est simple. <pre><code># create a Spatially Enabled DataFrame object\nsdf = pd.DataFrame.spatial.from_layer(feature_layer)\n</code></pre></p>"},{"location":"4-Collaboration/Environnement-Analyse-Geospatiale/#mettre-a-jour-les-elements","title":"Mettre \u00e0 jour les \u00e9l\u00e9ments","text":"<p>En obtenant l'\u00e9l\u00e9ment comme nous l'avons fait similaire \u00e0 l'exemple ci-dessus, nous pouvons utiliser la fonction <code>.update</code> pour mettre \u00e0 jour l'\u00e9l\u00e9ment existant dans le portail DAaaS GEO. Nous pouvons mettre \u00e0 jour les propri\u00e9t\u00e9s, les donn\u00e9es, les vignettes et les m\u00e9tadonn\u00e9es des \u00e9l\u00e9ments. <pre><code>item1_buffer = gis.content.get('c60c7e57bdb846dnbd7c8226c80414d2')\nitem1_buffer.update(item_properties={'title': 'Enter Title'\n'tags': 'tag1, tag2, tag3, tag4',\n'description': 'Enter description of item'}\n</code></pre></p>"},{"location":"4-Collaboration/Environnement-Analyse-Geospatiale/#visualisez-vos-donnees-sur-une-carte-interactive","title":"Visualisez vos donn\u00e9es sur une carte interactive","text":"<p>Exemple\u00a0: Librairie MatplotLib Dans le code ci-dessous, nous cr\u00e9ons un objet ax, qui est un trac\u00e9 de style carte. Nous tra\u00e7ons ensuite notre colonne de changement de donn\u00e9es (\"Population Change\") sur les axes <pre><code>import matplotlib.pyplot as plt\nax = sdf.boundary.plot(figsize=(10, 5))\nshape.plot(ax=ax, column='Population Change', legend=True)\nplt.show()\n</code></pre></p> <p>Exemple\u00a0: librairie ipyleaflet Dans cet exemple, nous utiliserons la librairie 'ipyleaflet' pour cr\u00e9er une carte interactive. Cette carte sera centr\u00e9e autour de Toronto, ON. Les donn\u00e9es utilis\u00e9es seront d\u00e9crites ci-dessous. Commencez par coller <code>conda install -c conda-forge ipyleaflet</code> vous permettant d'installer les librairies ipyleaflet dans l'environnement Python.  Import the necessary libraries. <pre><code>import ipyleaflet \nfrom ipyleaflet import *\n</code></pre> Maintenant que nous avons import\u00e9 le module ipyleaflet, nous pouvons cr\u00e9er une carte simple en sp\u00e9cifiant la latitude et la longitude de l'emplacement que nous voulons, le niveau de zoom et le fond de carte (plus de fonds de carte). Des contr\u00f4les suppl\u00e9mentaires ont \u00e9t\u00e9 ajout\u00e9s tels que les calques et l'\u00e9chelle. <pre><code>toronto_map = Map(center=[43.69, -79.35], zoom=11, basemap=basemaps.Esri.WorldStreetMap)\ntoronto_map.add_control(LayersControl(position='topright'))\ntoronto_map.add_control(ScaleControl(position='bottomleft'))\ntoronto_map\n</code></pre> </p>"},{"location":"4-Collaboration/Environnement-Analyse-Geospatiale/#en-savoir-plus-sur-lapi-arcgis-pour-python","title":"En savoir plus sur l'API ArcGIS pour Python","text":"<p>La documentation compl\u00e8te de l'API ArcGIS peut \u00eatre trouv\u00e9e ici</p>"},{"location":"4-Collaboration/Environnement-Analyse-Geospatiale/#en-savoir-plus-sur-lenvironnement-analytique-geospatial-gae-et-les-services-das","title":"En savoir plus sur l'environnement analytique g\u00e9ospatial (GAE) et les services DAS","text":"<p>Guide d'aide GAE</p>"},{"location":"5-Stockage/Aper%C3%A7u/","title":"Stockage","text":"<p>La plateforme propose plusieurs types de stockage :</p> <ul> <li>Disque (\u00e9galement appel\u00e9 Volumes sur l'\u00e9cran de cr\u00e9ation de serveur de   blocs-note)</li> <li>Compartiment (stockage \"Blob\" ou S3, fourni via MinIO)</li> <li>Data Lakes (\u00e0 venir)</li> </ul> <p>Selon votre cas d'utilisation, le disque ou le compartiment peut \u00eatre le plus appropri\u00e9 :</p> Type Utilisateurs simultan\u00e9s Vitesse Taille totale Partageable avec d'autres utilisateurs Disque Une machine/serveur de bloc-notes \u00e0 la fois Le plus rapide (d\u00e9bit et latence) &lt;=512GB total par stockage Non Compartiment (via MinIO) Acc\u00e8s simultan\u00e9 depuis plusieurs machines/serveurs d'ordinateurs portables en m\u00eame temps Fast-ish ((T\u00e9l\u00e9chargement rapide, t\u00e9l\u00e9chargement modeste, latence modeste) Infini (dans la limite du raisonnable) [Oui] Si vous ne savez pas lequel choisir, ne vous en faites pas <p>Ce sont des lignes directrices, pas une science exacte - choisissez ce qui sonne le mieux maintenant et ex\u00e9cutez-le. Le meilleur choix pour une utilisation compliqu\u00e9e n'est pas \u00e9vident et n\u00e9cessite souvent une exp\u00e9rience pratique, donc essayer quelque chose vous aidera. Dans la plupart des situations, les deux options fonctionnent bien m\u00eame si elles ne sont pas parfaites, et n'oubliez pas que les donn\u00e9es peuvent toujours \u00eatre copi\u00e9es plus tard si vous changez d'avis.</p>"},{"location":"5-Stockage/Disque/","title":"Aper\u00e7u","text":"<p>Les disques sont les syst\u00e8mes de fichiers familiers de type disque dur auxquels vous \u00eates habitu\u00e9, fournis \u00e0 vous des disques SSD rapides !</p>"},{"location":"5-Stockage/Disque/#installation","title":"Installation","text":"<p>Lors de la cr\u00e9ation de votre serveur bloc-notes, vous demandez des disques en ajoutant des volumes de donn\u00e9es \u00e0 votre serveur de bloc-notes (illustr\u00e9 ci-dessous, avec <code>Type = New</code>). Ils sont automatiquement mont\u00e9 dans le r\u00e9pertoire (<code>Mount Point</code>) que vous choisissez, et sert de simple et moyen fiable de conserver les donn\u00e9es attach\u00e9es \u00e0 un serveur de bloc-notes .</p> <p></p> Vous payez pour tous les disques que vous poss\u00e9dez, qu'ils soient connect\u00e9s \u00e0 un serveur bloc-note ou non <p>D\u00e8s que vous cr\u00e9ez un disque, vous le payez jusqu'\u00e0 ce qu'il soit supprim\u00e9, m\u00eame si le Serveur de blocs-note  d'origine est supprim\u00e9. Voir Suppression du stockage sur disque pour plus d'informations</p>"},{"location":"5-Stockage/Disque/#une-fois-que-vous-avez-les-bases","title":"Une fois que vous avez les bases...","text":"<p>Lorsque vous supprimez votre Serveur blocs-note , vos disques ne sont pas supprim\u00e9s. Cela laisse vous r\u00e9utilisez ce m\u00eame disque (avec tout son contenu) sur un nouveau Serveur blocs-note plus tard (comme indiqu\u00e9 ci-dessus avec \u00ab Type = existant \u00bb et le \u00ab Nom \u00bb d\u00e9fini sur le volume que vous voulez r\u00e9utiliser). Si vous avez termin\u00e9 avec le disque et son contenu,  supprimez-le.</p>"},{"location":"5-Stockage/Disque/#suppression-du-stockage-sur-disque","title":"Suppression du stockage sur disque","text":"<p>Pour voir vos disques, consultez la section Volumes blocs-note de Serveur blocs-note page (ci-dessous). Vous pouvez supprimer n'importe quel disque non connect\u00e9 (ic\u00f4ne orange \u00e0 gauche) en cliquant sur l'ic\u00f4ne de la corbeille.</p> <p></p>"},{"location":"5-Stockage/Disque/#prix","title":"Prix","text":"Les mod\u00e8les de tarification sont provisoires et peuvent changer <p>Au moment de la r\u00e9daction, la tarification est couverte par la plate-forme pour les utilisateurs initiaux. Ce guide explique comment les choses devraient \u00eatre tarif\u00e9es \u00e0 l'avenir, mais cela peut changer.</p> <p>Lors du montage d'un disque, vous obtenez un Disque manag\u00e9 Azure. La tarification Disques g\u00e9r\u00e9s SSD Premium indique le co\u00fbt par disque en fonction de la taille. Notez que vous payez pour la taille de disque demand\u00e9e, et non pour la quantit\u00e9 d'espace que vous utilisent actuellement.</p> Conseils pour minimiser les co\u00fbts <p>Comme les disques peuvent \u00eatre attach\u00e9s \u00e0 un Serveur blocs-note  et r\u00e9utilis\u00e9s, un mod\u00e8le d'utilisation typique pourrait \u00eatre :</p> <ul> <li>\u00c0 9h, cr\u00e9ez un Serveur blocs-note  (demandez 2CPU/8GB RAM et un 32GB attach\u00e9   disque)</li> <li>Travaillez tout au long de la journ\u00e9e, en enregistrant les r\u00e9sultats sur le disque attach\u00e9</li> <li>\u00c0 17h, \u00e9teignez votre Serveur blocs-note  pour \u00e9viter de le payer du jour au lendemain</li> <li>REMARQUE : Le disque attach\u00e9  n'est pas d\u00e9truit  par cette action</li> <li>\u00c0 9 heures du matin le lendemain, cr\u00e9ez un nouveau serveur blocs-note et  joignez     votre disque</li> <li>Continuez votre travail...</li> </ul> <p>Cela prot\u00e8ge tout votre travail sans payer pour l'ordinateur lorsque vous ne l'utilisez pas</p>"},{"location":"5-Stockage/MinIo/","title":"Stockage","text":"<p>La plateforme propose diff\u00e9rents types de stockage, con\u00e7us pour diff\u00e9rents types de cas d'utilisation. Par cons\u00e9quent, cette section vous concerne, que vous soyez en train d'exp\u00e9rimenter, de cr\u00e9er des pipelines, ou d'\u00e9diter.</p> <p>En surface, il existe quelques types de stockage :</p> <ul> <li>des disques (aussi appel\u00e9s volumes)</li> <li>des compartiments (stockage S3 ou \u00ab blob \u00bb via MinIO)</li> <li>Data Lakes (\u00e0 venir)</li> </ul> <p>En fonction de votre cas d'utilisation, un disque ou compartiments peut \u00eatre le plus appropri\u00e9:</p> Type Usagers Simultan\u00e9s Vitesse Capacit\u00e9 Totale Peut \u00eatre partag\u00e9 avec d'autres usagers Disque Une machine/serveur bloc-notes \u00e0 la fois Plus rapide (d\u00e9bit et latence) &lt;=512GB total par disque Non Compartiment Acc\u00e8s simultan\u00e9 \u00e0 partir de plusieurs machines/serveurs bloc-notes en m\u00eame temps Assez rapide (t\u00e9l\u00e9chargement rapide, envoi modeste, latence modeste) Quantit\u00e9 infinie (dans une limite raisonnable) Oui <p>??? info \"Si vous n'\u00eates pas s\u00fbr de votre choix, ne vous inqui\u00e9tez pas\".     Il s'agit de lignes directrices, pas d'une science exacte - choisissez ce qui vous semble le mieux maintenant et faites-le.  Le meilleur choix pour une utilisation compliqu\u00e9e n'est pas \u00e9vident et n\u00e9cessite souvent une exp\u00e9rience pratique, donc le simple fait d'essayer quelque chose vous aidera.  Dans la plupart des situations, les deux options fonctionnent bien, m\u00eame si elles ne sont pas parfaites, et n'oubliez pas que les donn\u00e9es peuvent toujours \u00eatre copi\u00e9es ult\u00e9rieurement si vous changez d'avis.</p>"},{"location":"5-Stockage/MinIo/#disques","title":"Disques","text":"<p>Les disques sont les syst\u00e8mes de fichiers courants de type disque dur ou SSD. Vous pouvez monter les disques dans votre serveur Kubeflow, et m\u00eame si vous supprimez votre serveur, vous pouvez remonter les disques, car ils ne sont jamais d\u00e9truits par d\u00e9faut. C'est un moyen tr\u00e8s simple de stocker vos donn\u00e9es, et si vous partagez un espace de travail avec une \u00e9quipe, tous les membres peuvent utiliser le disque du m\u00eame serveur (comme un lecteur partag\u00e9).</p> <p></p> <p>C'est un moyen tr\u00e8s simple de stocker vos donn\u00e9es. Si vous partagez un espace de travail avec une \u00e9quipe, tous les membres peuvent utiliser le disque du m\u00eame serveur comme un lecteur partag\u00e9.</p>"},{"location":"5-Stockage/MinIo/#compartiments","title":"Compartiments","text":"<p>Les compartiments sont un peu plus compliqu\u00e9s, mais ils pr\u00e9sentent trois avantages :</p> <ul> <li> <p>Le stockage de grandes quantit\u00e9s de donn\u00e9es</p> <ul> <li>Les compartiments peuvent \u00eatre \u00e9normes (bien plus grands que les disques   durs), et ils sont rapides.</li> </ul> </li> </ul> <ul> <li> <p>Le partage de donn\u00e9es</p> <ul> <li>Vous pouvez partager des fichiers \u00e0 partir d'un compartiment en partageant   une URL que vous pouvez obtenir par l'interm\u00e9diaire d'une interface Web   simple. C'est une excellente fa\u00e7on de partager des donn\u00e9es avec des   personnes \u00e0 l'ext\u00e9rieur de votre espace de travail.</li> </ul> </li> </ul> <ul> <li> <p>L'acc\u00e8s \u00e0 la programmation</p> <ul> <li>Plus important encore, il est beaucoup plus facile pour les pipelines et les   navigateurs Web d'acc\u00e9der aux donn\u00e9es provenant de compartiments que d'un   disque dur. Donc, si vous voulez utiliser des pipelines, il faut d'abord les   configurer pour qu'ils fonctionnent avec un compartiment.</li> </ul> </li> </ul>"},{"location":"5-Stockage/MinIo/#stockage-en-compartiment","title":"Stockage en compartiment","text":"<p>Nous avons trois types de stockage en compartiment.</p> <p>Libre-service :</p> <ul> <li>Standard:   Stockage soutenu par des HDD. Par d\u00e9faut, utilisez cette option.</li> <li>Premium:   Utilisez cette option si vous avez besoin de vitesses de lecture / \u00e9criture   tr\u00e8s \u00e9lev\u00e9es, comme pour l'entra\u00eenement de mod\u00e8les sur de tr\u00e8s grands   ensembles de donn\u00e9es.</li> </ul> <p>Accessible au grand public :</p> <p>Public (en lecture seule)</p>"},{"location":"5-Stockage/MinIo/#libre-service","title":"Libre-service","text":"<p>Dans chacune des trois options de libre-service, vous pouvez cr\u00e9er un compartiment personnel. Pour vous connecter, il vous suffit d'utiliser votre OpenID.</p> <p></p> <p>Une fois que vous \u00eates connect\u00e9, vous pouvez cr\u00e9er un compartiment personnel selon le format <code>prenom-nom</code>.</p> <p></p> <p>Impossible de partager des fichiers avec votre OpenID</p> <p>\u00c0 cause d'un bogue dans MinIO, vous ne pouvez pas encore partager des fichiers. Nous esp\u00e9rons que cela sera bient\u00f4t r\u00e9solu. En attendant, \u00e7a fonctionnera bien si vous utilisez votre cl\u00e9 d'acc\u00e8s et votre cl\u00e9 secr\u00e8te, que vous pouvez obtenir aupr\u00e8s de Kubeflow.</p>"},{"location":"5-Stockage/MinIo/#partager","title":"Partager \u00e0 partir d'un compartiment priv\u00e9","text":"<p>Vous pouvez facilement partager des fichiers individuels. Utilisez simplement l'option \u00ab partager \u00bb pour un fichier particulier, et vous recevrez un lien que vous pourrez envoyer \u00e0 un  collaborateur.</p> <p></p>"},{"location":"5-Stockage/MinIo/#acces-a-la-programmation","title":"Acc\u00e8s \u00e0 la programmation","text":"<p>Nous travaillons actuellement \u00e0 un moyen de vous permettre d'acc\u00e9der \u00e0 votre stockage de compartiment par l'interm\u00e9diaire d'un dossier dans votre bloc-notes, mais en attendant, vous pouvez y acc\u00e9der par programme en utilisant l'outil de ligne de commande <code>mc</code>, ou par l'interm\u00e9diaire des appels d'API S3 dans R ou Python.</p> <p>Voir les exemples de blocs-notes!</p> <p>Un mod\u00e8le est fourni pour se connecter dans <code>R</code>, <code>python</code>, ou par la ligne de commande fournie dans <code>jupyter-notebooks/self-serve-storage</code>. Vous pouvez copier-coller et modifier ces exemples. Ils devraient r\u00e9pondre \u00e0 la plupart de vos besoins.</p>"},{"location":"5-Stockage/MinIo/#connexion-a-laide-de-mc","title":"Connexion \u00e0 l'aide de <code>mc</code>","text":"<p>Pour vous connecter, ex\u00e9cutez la commande suivante (remplacer <code>NOMCOMPLET=blair-drummond</code> par votre <code>nom-pr\u00e9nom</code> r\u00e9el) :</p> <pre><code>#!/bin/sh\nNOMCOMPLET=blair-drummond\n# Obtenir les justificatifs d'identit\u00e9\nsource /vault/secrets/minio-standard-tenant-1\n# Ajouter le stockage sous le pseudonyme \u00ab standard \u00bb\nmc config host add standard $MINIO_URL $MINIO_ACCESS_KEY $MINIO_SECRET_KEY\n# Cr\u00e9er un compartiment \u00e0 votre nom\n# NOTE : Vous pouvez *uniquement* cr\u00e9er des compartiments nomm\u00e9s avec votre PR\u00c9NOM-NOM.\n# Tout autre nom sera rejet\u00e9.\n# Compartiment priv\u00e9 (\"mb\" = \"cr\u00e9er compartiment\")\nmc mb standard/${NOMCOMPLET}\n# Compartiment partag\u00e9\nmc mb standard/shared/${NOMCOMPLET}\n# Voil\u00e0! Vous pouvez maintenant copier des fichiers ou des dossiers!\n[ -f test.txt ] || echo \"Ceci est un test\" &gt; test.txt\nmc cp test.txt standard/${NOMCOMPLET}/test.txt\n</code></pre> <p>Maintenant, ouvrez le document dans le navigateur MinIO. Vous y verrez votre fichier de test.</p> <p>Vous pouvez utiliser <code>mc</code> pour copier des fichiers vers/depuis le compartiment. Cette op\u00e9ration est tr\u00e8s rapide. Vous pouvez \u00e9galement utiliser <code>mc --help</code> pour voir les autres options qui s'offrent \u00e0 vous, comme <code>mc ls standard/PR\u00c9NOM-NOM/</code> pour afficher le contenu de votre compartiment.</p> Autres options de stockage <p>Pour utiliser une de nos autres options de stockage, <code>premium</code>, remplacez simplement  la valeur <code>standard</code> dans le programme ci-dessus par le type dont vous avez besoin.</p>"},{"location":"5-Stockage/MinIo/#obtenir-linformation-didentification-de-minio","title":"Obtenir l'information d'identification de MinIO","text":"<p>Pour acc\u00e9der \u00e0 vos stockage en compartiment en MinIO par programme (par exemple via l'outil de l'invite de commande <code>mc</code>, ou via Python ou R), vous avez besoin d'informations d'identification personnelles de MinIO. Les m\u00e9thodes pour obtenir celles-ci sont expliqu\u00e9es ci-dessous.</p>"},{"location":"5-Stockage/MinIo/#methode-1-utiliser-le-vault","title":"M\u00e9thode 1: Utiliser le Vault","text":"<p>Pour obtenir vos informations d'identification MinIO, vous pouvez utiliser le Vault. S\u00e9lectionnez la m\u00e9thode OIDC, laissez le R\u00f4le vide et cliquez \u00ab Sign In with OIDC Provider \u00bb.</p> <p></p> <p>Ex\u00e9cutez la commande suivante dans le terminal situ\u00e9 dans le coin \u00e0 droit:</p> <pre><code># Replacez standard avec premium pour changer le type de compartiment\nread minio_standard_tenant_1/keys/profile-votrepr\u00e9nom-votrenom\n</code></pre> <p></p>"},{"location":"5-Stockage/MinIo/#methode-2-utiliser-le-serveur","title":"M\u00e9thode 2: Utiliser le Serveur","text":"<p>D\u00e9marrez votre serveur et ex\u00e9cutez la commande suivante:</p> <pre><code>cat /vault/secrets/minio-standard-tenant-1\n\n# Output:\n# export MINIO_URL=\"http://minio.minio-standard-tenant-1 ...\"\n# export MINIO_ACCESS_KEY=\"...\"\n# export MINIO_SECRET_KEY=\"...\"\n</code></pre>"}]}